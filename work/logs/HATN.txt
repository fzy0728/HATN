loading data...
source domain:  books target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 11843
vocab-size:  100530
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'favorite', 'fantastic', 'enjoyed', 'funny', 'love', 'brilliant', 'classic', 'awesome', 'well', 'entertaining', 'interesting', 'useful', 'incredible', 'fascinating', 'nice', 'loved', 'refreshing', 'inspiring', 'amazing', 'superb', 'fun', 'valuable', 'helps', 'compelling', 'wonderful', 'solid', 'inspirational', 'helpful', 'important', 'wonderfully', 'hilarious', 'invaluable', 'essential', 'riveting', 'emotional', 'impressive', 'readable', 'authentic', 'honest', 'gives', 'sad', 'enlightening', 'pleasant', 'powerful', 'loves', 'liked', 'read', 'humorous', 'deserves', 'heartwarming', 'worthwhile', 'believable']
['disappointing', 'poorly', 'boring', 'disappointed', 'worst', 'horrible', 'useless', 'annoying', 'repetitive', 'confusing', 'lacks', 'hard', 'tedious', 'misleading', 'awful', 'ridiculous', 'difficult', 'pathetic', 'simplistic', 'unnecessary', 'flawed', 'terrible', 'shallow', 'slow', 'mediocre', 'better', 'laughable', 'frustrating', 'tired', 'wasted', 'bad', 'outdated', 'uninspired', 'contrived', 'waste', 'amusing', 'trite', 'uninteresting', 'disjointed', 'sloppy', 'predictable', 'silly', 'dissapointed', 'absurd', 'sorry', 'wrong', 'tiresome', 'impossible', 'insulting', 'worthless', 'weakest', 'inaccurate', 'ok', 'unreadable', 'lame', 'unbelievable', 'unfortunate', 'wastes', 'sophomoric', 'deceived', 'fails', 'weak', 'unlikeable', 'terribly', 'repetitious', 'overrated', 'hackneyed', 'trying', 'dangerous']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
5600 400 6000 15750 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 392.64447570, sen-loss: 76.86802083, dom-loss: 79.25041121, src-aux-loss: 122.87815541, tar-aux-loss: 113.64788836
Epoch: [1  ] train-acc: 0.67660714, dom-acc: 0.67116071, val-acc: 0.67750000, val_loss: 0.65289891
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 365.20356846, sen-loss: 70.31201476, dom-loss: 76.50024581, src-aux-loss: 114.92448002, tar-aux-loss: 103.46682864
Epoch: [2  ] train-acc: 0.75178571, dom-acc: 0.72392857, val-acc: 0.75750000, val_loss: 0.58705676
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 349.69688082, sen-loss: 62.86016980, dom-loss: 75.74408245, src-aux-loss: 110.41742474, tar-aux-loss: 100.67520231
Epoch: [3  ] train-acc: 0.79982143, dom-acc: 0.71000000, val-acc: 0.81500000, val_loss: 0.50924724
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 335.45945287, sen-loss: 54.81946769, dom-loss: 75.40760678, src-aux-loss: 107.21053791, tar-aux-loss: 98.02184254
Epoch: [4  ] train-acc: 0.83035714, dom-acc: 0.68580357, val-acc: 0.82750000, val_loss: 0.43385270
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 324.74610257, sen-loss: 47.77352390, dom-loss: 75.36138135, src-aux-loss: 104.66494983, tar-aux-loss: 96.94624764
Epoch: [5  ] train-acc: 0.84500000, dom-acc: 0.67616071, val-acc: 0.84500000, val_loss: 0.37871873
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 315.04613638, sen-loss: 42.29169697, dom-loss: 75.40706897, src-aux-loss: 102.74105793, tar-aux-loss: 94.60631311
Epoch: [6  ] train-acc: 0.85517857, dom-acc: 0.71196429, val-acc: 0.86000000, val_loss: 0.35618311
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 310.06212330, sen-loss: 39.23055144, dom-loss: 75.30012053, src-aux-loss: 101.52661496, tar-aux-loss: 94.00483513
Epoch: [7  ] train-acc: 0.86892857, dom-acc: 0.74580357, val-acc: 0.85750000, val_loss: 0.32813293
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 305.85202503, sen-loss: 36.67870724, dom-loss: 75.34418941, src-aux-loss: 100.33670157, tar-aux-loss: 93.49242699
Epoch: [8  ] train-acc: 0.87678571, dom-acc: 0.74392857, val-acc: 0.87000000, val_loss: 0.31892562
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 302.67563009, sen-loss: 35.14108400, dom-loss: 75.50985926, src-aux-loss: 99.09538400, tar-aux-loss: 92.92930448
Epoch: [9  ] train-acc: 0.88303571, dom-acc: 0.73026786, val-acc: 0.87250000, val_loss: 0.31747267
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 300.20505714, sen-loss: 33.56984374, dom-loss: 75.62812400, src-aux-loss: 98.40732753, tar-aux-loss: 92.59976453
Epoch: [10 ] train-acc: 0.88303571, dom-acc: 0.72008929, val-acc: 0.86250000, val_loss: 0.31359842
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 298.15914702, sen-loss: 32.83644764, dom-loss: 75.76719749, src-aux-loss: 97.96653718, tar-aux-loss: 91.58896399
Epoch: [11 ] train-acc: 0.89035714, dom-acc: 0.71151786, val-acc: 0.88250000, val_loss: 0.31513894
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 297.35196304, sen-loss: 31.97150369, dom-loss: 75.82183552, src-aux-loss: 96.86201638, tar-aux-loss: 92.69660735
Epoch: [12 ] train-acc: 0.89321429, dom-acc: 0.71901786, val-acc: 0.88500000, val_loss: 0.30857715
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 295.44029713, sen-loss: 31.42694026, dom-loss: 76.06901914, src-aux-loss: 96.39830470, tar-aux-loss: 91.54603356
Epoch: [13 ] train-acc: 0.89285714, dom-acc: 0.70678571, val-acc: 0.87000000, val_loss: 0.31056195
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 293.21204829, sen-loss: 30.70320117, dom-loss: 76.26043177, src-aux-loss: 95.90252632, tar-aux-loss: 90.34589034
Epoch: [14 ] train-acc: 0.89767857, dom-acc: 0.70785714, val-acc: 0.87500000, val_loss: 0.30643854
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 292.12090921, sen-loss: 30.32234441, dom-loss: 76.17801493, src-aux-loss: 95.40844053, tar-aux-loss: 90.21210808
Epoch: [15 ] train-acc: 0.89857143, dom-acc: 0.70625000, val-acc: 0.88250000, val_loss: 0.30385667
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 291.86312771, sen-loss: 29.73324004, dom-loss: 76.36384320, src-aux-loss: 94.91791600, tar-aux-loss: 90.84812605
Epoch: [16 ] train-acc: 0.90232143, dom-acc: 0.70294643, val-acc: 0.89500000, val_loss: 0.30659276
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 291.27866983, sen-loss: 29.49423937, dom-loss: 76.69019705, src-aux-loss: 94.31964904, tar-aux-loss: 90.77458435
Epoch: [17 ] train-acc: 0.90446429, dom-acc: 0.70062500, val-acc: 0.88500000, val_loss: 0.30105442
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 289.61727905, sen-loss: 28.84227601, dom-loss: 76.62629557, src-aux-loss: 93.98466861, tar-aux-loss: 90.16403741
Epoch: [18 ] train-acc: 0.90607143, dom-acc: 0.69169643, val-acc: 0.88750000, val_loss: 0.30072266
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 288.77602983, sen-loss: 28.45657235, dom-loss: 76.90683430, src-aux-loss: 93.41044527, tar-aux-loss: 90.00217527
Epoch: [19 ] train-acc: 0.90678571, dom-acc: 0.68276786, val-acc: 0.88000000, val_loss: 0.30089697
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 289.26867700, sen-loss: 28.23137576, dom-loss: 77.12306559, src-aux-loss: 93.16030103, tar-aux-loss: 90.75393468
Epoch: [20 ] train-acc: 0.90946429, dom-acc: 0.68080357, val-acc: 0.89000000, val_loss: 0.29873630
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 286.38849878, sen-loss: 27.71124094, dom-loss: 77.13523674, src-aux-loss: 92.57374346, tar-aux-loss: 88.96827775
Epoch: [21 ] train-acc: 0.90785714, dom-acc: 0.67035714, val-acc: 0.88000000, val_loss: 0.29937926
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 287.70777583, sen-loss: 27.53231567, dom-loss: 77.24395770, src-aux-loss: 92.37568969, tar-aux-loss: 90.55581254
Epoch: [22 ] train-acc: 0.91375000, dom-acc: 0.67008929, val-acc: 0.89750000, val_loss: 0.29934996
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 285.14222741, sen-loss: 27.26028327, dom-loss: 77.28453434, src-aux-loss: 91.83977407, tar-aux-loss: 88.75763696
Epoch: [23 ] train-acc: 0.91303571, dom-acc: 0.66160714, val-acc: 0.88500000, val_loss: 0.29775628
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 285.63218045, sen-loss: 26.87915391, dom-loss: 77.47785574, src-aux-loss: 91.43775535, tar-aux-loss: 89.83741695
Epoch: [24 ] train-acc: 0.91410714, dom-acc: 0.66348214, val-acc: 0.88750000, val_loss: 0.29691893
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 285.64271522, sen-loss: 26.63510048, dom-loss: 77.53053856, src-aux-loss: 91.32608980, tar-aux-loss: 90.15098590
Epoch: [25 ] train-acc: 0.91375000, dom-acc: 0.64758929, val-acc: 0.88750000, val_loss: 0.29826805
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 282.98125029, sen-loss: 26.21978919, dom-loss: 77.54650074, src-aux-loss: 90.64487088, tar-aux-loss: 88.57008988
Epoch: [26 ] train-acc: 0.91750000, dom-acc: 0.64776786, val-acc: 0.89000000, val_loss: 0.29793116
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 282.59057379, sen-loss: 26.06393100, dom-loss: 77.63942808, src-aux-loss: 90.40628964, tar-aux-loss: 88.48092562
Epoch: [27 ] train-acc: 0.91678571, dom-acc: 0.64758929, val-acc: 0.89250000, val_loss: 0.29637420
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 282.49487686, sen-loss: 25.75001772, dom-loss: 77.65408927, src-aux-loss: 89.80477297, tar-aux-loss: 89.28599674
Epoch: [28 ] train-acc: 0.91678571, dom-acc: 0.64758929, val-acc: 0.89000000, val_loss: 0.30207068
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 280.65411639, sen-loss: 25.42443407, dom-loss: 77.61935395, src-aux-loss: 89.45257187, tar-aux-loss: 88.15775436
Epoch: [29 ] train-acc: 0.91892857, dom-acc: 0.63946429, val-acc: 0.89500000, val_loss: 0.29604408
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 280.75475883, sen-loss: 25.27235761, dom-loss: 77.83290178, src-aux-loss: 89.15816313, tar-aux-loss: 88.49133700
Epoch: [30 ] train-acc: 0.92017857, dom-acc: 0.63821429, val-acc: 0.89500000, val_loss: 0.29701760
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 281.10515642, sen-loss: 24.91225534, dom-loss: 77.74084717, src-aux-loss: 88.91234171, tar-aux-loss: 89.53971213
Epoch: [31 ] train-acc: 0.91928571, dom-acc: 0.64017857, val-acc: 0.89500000, val_loss: 0.30479655
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 279.00280309, sen-loss: 24.59930433, dom-loss: 77.85771960, src-aux-loss: 88.68390435, tar-aux-loss: 87.86187506
Epoch: [32 ] train-acc: 0.92053571, dom-acc: 0.63589286, val-acc: 0.89250000, val_loss: 0.30353719
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 278.92756820, sen-loss: 24.45075216, dom-loss: 77.74425185, src-aux-loss: 88.03232610, tar-aux-loss: 88.70023763
Epoch: [33 ] train-acc: 0.92482143, dom-acc: 0.64125000, val-acc: 0.89500000, val_loss: 0.29523385
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 278.45342278, sen-loss: 23.99629885, dom-loss: 77.72774571, src-aux-loss: 87.95206290, tar-aux-loss: 88.77731472
Epoch: [34 ] train-acc: 0.92446429, dom-acc: 0.62482143, val-acc: 0.89750000, val_loss: 0.29654053
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 277.34618068, sen-loss: 23.95409682, dom-loss: 77.75787157, src-aux-loss: 87.36426491, tar-aux-loss: 88.26994854
Epoch: [35 ] train-acc: 0.92303571, dom-acc: 0.62348214, val-acc: 0.89250000, val_loss: 0.29754642
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 276.27479076, sen-loss: 23.48929805, dom-loss: 77.91505182, src-aux-loss: 86.97639722, tar-aux-loss: 87.89404273
Epoch: [36 ] train-acc: 0.92625000, dom-acc: 0.63017857, val-acc: 0.89750000, val_loss: 0.29887879
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 274.91537118, sen-loss: 23.31063720, dom-loss: 77.82018566, src-aux-loss: 86.51092982, tar-aux-loss: 87.27362007
Epoch: [37 ] train-acc: 0.93053571, dom-acc: 0.62839286, val-acc: 0.90000000, val_loss: 0.29616469
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 275.15222645, sen-loss: 22.88580396, dom-loss: 77.91230994, src-aux-loss: 86.22346377, tar-aux-loss: 88.13064915
Epoch: [38 ] train-acc: 0.92767857, dom-acc: 0.63151786, val-acc: 0.89500000, val_loss: 0.30062342
---------------------------------------------------

Successfully load model from save path: ./work/models/books_dvd_HATN.ckpt
Best Epoch: [ 33] best val accuracy: 0.00000000 best val loss: 0.29523385
Testing accuracy: 0.87700000
./work/attentions/books_dvd_train_HATN.txt
./work/attentions/books_dvd_test_HATN.txt
loading data...
source domain:  books target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 17009
vocab-size:  83050
['great', 'excellent', 'good', 'best', 'easy', 'highly', 'enjoyable', 'favorite', 'fantastic', 'love', 'brilliant', 'awesome', 'funny', 'well', 'classic', 'entertaining', 'enjoyed', 'interesting', 'useful', 'loved', 'incredible', 'nice', 'solid', 'amazing', 'fascinating', 'superb', 'refreshing', 'amusing', 'essential', 'important', 'readable', 'honest', 'fun', 'valuable', 'invaluable', 'wonderfully', 'compelling', 'sad', 'beautifully', 'inspirational', 'helpful', 'impressive', 'hilarious', 'wonderful', 'inspiring', 'enlightening', 'riveting', 'worthwhile', 'finest', 'pleasant', 'authentic', 'liked', 'simple', 'humorous', 'exceptional', 'emotional']
['disappointing', 'disappointed', 'boring', 'poorly', 'horrible', 'worst', 'useless', 'repetitive', 'hard', 'annoying', 'confusing', 'misleading', 'better', 'difficult', 'tedious', 'terrible', 'awful', 'laughable', 'unnecessary', 'flawed', 'lacks', 'pathetic', 'shallow', 'ridiculous', 'mediocre', 'wasted', 'slow', 'simplistic', 'bad', 'frustrating', 'waste', 'outdated', 'uninspired', 'uninteresting', 'sloppy', 'trite', 'tired', 'wrong', 'predictable', 'silly', 'contrived', 'dissapointed', 'disjointed', 'trying', 'dangerous', 'unbelievable', 'tiresome', 'expensive', 'dull', 'choppy', 'unreadable', 'terribly', 'impossible', 'wastes', 'false', 'sophomoric', 'deceived', 'insulting', 'weak', 'worthless', 'unfortunate', 'incorrect', 'weakest', 'overrated', 'biased', 'inaccurate', 'hackneyed', 'pointless']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
5600 400 6000 15750 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 410.95931268, sen-loss: 76.73321694, dom-loss: 78.04397219, src-aux-loss: 131.98481476, tar-aux-loss: 124.19730902
Epoch: [1  ] train-acc: 0.68375000, dom-acc: 0.85776786, val-acc: 0.69750000, val_loss: 0.65001917
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 382.38969398, sen-loss: 69.87574345, dom-loss: 73.59005064, src-aux-loss: 123.51073486, tar-aux-loss: 115.41316348
Epoch: [2  ] train-acc: 0.75750000, dom-acc: 0.91294643, val-acc: 0.76750000, val_loss: 0.58134770
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 363.81699157, sen-loss: 62.18749419, dom-loss: 72.31644559, src-aux-loss: 118.59146661, tar-aux-loss: 110.72158521
Epoch: [3  ] train-acc: 0.79732143, dom-acc: 0.90419643, val-acc: 0.80750000, val_loss: 0.50079775
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 351.68668127, sen-loss: 54.29888639, dom-loss: 72.76796246, src-aux-loss: 115.63468146, tar-aux-loss: 108.98515087
Epoch: [4  ] train-acc: 0.82303571, dom-acc: 0.87937500, val-acc: 0.84250000, val_loss: 0.43172556
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 341.75056028, sen-loss: 47.48361993, dom-loss: 73.61816311, src-aux-loss: 113.11978340, tar-aux-loss: 107.52899295
Epoch: [5  ] train-acc: 0.85089286, dom-acc: 0.85276786, val-acc: 0.85500000, val_loss: 0.37371391
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 334.19641566, sen-loss: 42.29137263, dom-loss: 74.72594815, src-aux-loss: 111.11651540, tar-aux-loss: 106.06257987
Epoch: [6  ] train-acc: 0.86178571, dom-acc: 0.80839286, val-acc: 0.86500000, val_loss: 0.34732422
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 330.08789039, sen-loss: 38.91688049, dom-loss: 75.85814267, src-aux-loss: 109.88384867, tar-aux-loss: 105.42901808
Epoch: [7  ] train-acc: 0.87142857, dom-acc: 0.73357143, val-acc: 0.87500000, val_loss: 0.32981640
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 325.54543996, sen-loss: 36.49956429, dom-loss: 76.79391491, src-aux-loss: 108.51126283, tar-aux-loss: 103.74069619
Epoch: [8  ] train-acc: 0.87553571, dom-acc: 0.68857143, val-acc: 0.86250000, val_loss: 0.31621149
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 322.63886428, sen-loss: 35.02274266, dom-loss: 77.40898860, src-aux-loss: 107.35792679, tar-aux-loss: 102.84920758
Epoch: [9  ] train-acc: 0.88250000, dom-acc: 0.65366071, val-acc: 0.88250000, val_loss: 0.31943527
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 322.47405839, sen-loss: 33.48217975, dom-loss: 78.06369942, src-aux-loss: 106.66586751, tar-aux-loss: 104.26231009
Epoch: [10 ] train-acc: 0.88482143, dom-acc: 0.63214286, val-acc: 0.86000000, val_loss: 0.30849409
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 319.51805186, sen-loss: 32.99192484, dom-loss: 78.57845449, src-aux-loss: 106.34169716, tar-aux-loss: 101.60597378
Epoch: [11 ] train-acc: 0.89339286, dom-acc: 0.60937500, val-acc: 0.89250000, val_loss: 0.30873582
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 317.38161445, sen-loss: 31.92875856, dom-loss: 78.41810364, src-aux-loss: 105.10220408, tar-aux-loss: 101.93254733
Epoch: [12 ] train-acc: 0.89321429, dom-acc: 0.61928571, val-acc: 0.88750000, val_loss: 0.31015143
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 316.39677525, sen-loss: 31.33803172, dom-loss: 78.80185330, src-aux-loss: 104.74889761, tar-aux-loss: 101.50799167
Epoch: [13 ] train-acc: 0.89035714, dom-acc: 0.59553571, val-acc: 0.86250000, val_loss: 0.30724940
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 314.47394753, sen-loss: 30.68746272, dom-loss: 79.01286745, src-aux-loss: 104.08217937, tar-aux-loss: 100.69143927
Epoch: [14 ] train-acc: 0.89839286, dom-acc: 0.59205357, val-acc: 0.88250000, val_loss: 0.30206692
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 313.53573966, sen-loss: 30.29884154, dom-loss: 78.97139353, src-aux-loss: 103.60811454, tar-aux-loss: 100.65738839
Epoch: [15 ] train-acc: 0.90303571, dom-acc: 0.58892857, val-acc: 0.88500000, val_loss: 0.29987115
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 312.32393217, sen-loss: 29.64014928, dom-loss: 78.59877026, src-aux-loss: 102.99631840, tar-aux-loss: 101.08869535
Epoch: [16 ] train-acc: 0.90553571, dom-acc: 0.59723214, val-acc: 0.89000000, val_loss: 0.30234906
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 310.89881730, sen-loss: 29.41554752, dom-loss: 78.38043970, src-aux-loss: 102.61622232, tar-aux-loss: 100.48660606
Epoch: [17 ] train-acc: 0.90589286, dom-acc: 0.60330357, val-acc: 0.89250000, val_loss: 0.29956836
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 308.76842809, sen-loss: 28.76407526, dom-loss: 77.90936852, src-aux-loss: 102.24758095, tar-aux-loss: 99.84740323
Epoch: [18 ] train-acc: 0.90714286, dom-acc: 0.61946429, val-acc: 0.88750000, val_loss: 0.30149436
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 307.27768302, sen-loss: 28.46352796, dom-loss: 77.79520082, src-aux-loss: 101.56894630, tar-aux-loss: 99.45000720
Epoch: [19 ] train-acc: 0.90428571, dom-acc: 0.62071429, val-acc: 0.88500000, val_loss: 0.29835811
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 307.54952836, sen-loss: 28.18194491, dom-loss: 77.63643044, src-aux-loss: 101.09784257, tar-aux-loss: 100.63330978
Epoch: [20 ] train-acc: 0.91000000, dom-acc: 0.62366071, val-acc: 0.88750000, val_loss: 0.29977873
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 304.92248392, sen-loss: 27.68580554, dom-loss: 77.31933248, src-aux-loss: 100.70594972, tar-aux-loss: 99.21139467
Epoch: [21 ] train-acc: 0.90875000, dom-acc: 0.63366071, val-acc: 0.88500000, val_loss: 0.29595748
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 305.07177067, sen-loss: 27.45780430, dom-loss: 77.35479772, src-aux-loss: 100.42859334, tar-aux-loss: 99.83057529
Epoch: [22 ] train-acc: 0.91303571, dom-acc: 0.63419643, val-acc: 0.88500000, val_loss: 0.30028525
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 302.97870278, sen-loss: 27.32808257, dom-loss: 76.88394654, src-aux-loss: 100.10096204, tar-aux-loss: 98.66571182
Epoch: [23 ] train-acc: 0.91267857, dom-acc: 0.64794643, val-acc: 0.88750000, val_loss: 0.29507479
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 302.90484118, sen-loss: 26.80634899, dom-loss: 76.97768629, src-aux-loss: 99.72473431, tar-aux-loss: 99.39607090
Epoch: [24 ] train-acc: 0.91535714, dom-acc: 0.64785714, val-acc: 0.88750000, val_loss: 0.29772672
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 302.11314273, sen-loss: 26.59760557, dom-loss: 77.01792282, src-aux-loss: 99.33922857, tar-aux-loss: 99.15838701
Epoch: [25 ] train-acc: 0.91571429, dom-acc: 0.63910714, val-acc: 0.89250000, val_loss: 0.29486865
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 300.90742540, sen-loss: 26.22785215, dom-loss: 77.15890670, src-aux-loss: 98.94705069, tar-aux-loss: 98.57361561
Epoch: [26 ] train-acc: 0.91589286, dom-acc: 0.63687500, val-acc: 0.88500000, val_loss: 0.29667145
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 300.48541760, sen-loss: 26.11627748, dom-loss: 77.30208862, src-aux-loss: 98.78402102, tar-aux-loss: 98.28303027
Epoch: [27 ] train-acc: 0.91607143, dom-acc: 0.62839286, val-acc: 0.88500000, val_loss: 0.29467201
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 299.16834283, sen-loss: 25.75212291, dom-loss: 77.51699352, src-aux-loss: 98.18367290, tar-aux-loss: 97.71555400
Epoch: [28 ] train-acc: 0.91821429, dom-acc: 0.62098214, val-acc: 0.88000000, val_loss: 0.30279160
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 299.06465101, sen-loss: 25.48991659, dom-loss: 77.51051265, src-aux-loss: 97.91817105, tar-aux-loss: 98.14605033
Epoch: [29 ] train-acc: 0.92017857, dom-acc: 0.61544643, val-acc: 0.89000000, val_loss: 0.29600483
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 298.84404159, sen-loss: 25.35666214, dom-loss: 78.00652683, src-aux-loss: 97.30859572, tar-aux-loss: 98.17225647
Epoch: [30 ] train-acc: 0.92232143, dom-acc: 0.59696429, val-acc: 0.89000000, val_loss: 0.29606566
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 298.25748205, sen-loss: 24.95187637, dom-loss: 78.15868688, src-aux-loss: 97.27131873, tar-aux-loss: 97.87560177
Epoch: [31 ] train-acc: 0.92142857, dom-acc: 0.58839286, val-acc: 0.88250000, val_loss: 0.30207756
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 298.03962851, sen-loss: 24.63702426, dom-loss: 78.37571996, src-aux-loss: 96.91107929, tar-aux-loss: 98.11580646
Epoch: [32 ] train-acc: 0.92214286, dom-acc: 0.56616071, val-acc: 0.88500000, val_loss: 0.30217999
---------------------------------------------------

Successfully load model from save path: ./work/models/books_electronics_HATN.ckpt
Best Epoch: [ 27] best val accuracy: 0.00000000 best val loss: 0.29467201
Testing accuracy: 0.86200000
./work/attentions/books_electronics_train_HATN.txt
./work/attentions/books_electronics_test_HATN.txt
loading data...
source domain:  books target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 13856
vocab-size:  78006
['great', 'good', 'excellent', 'best', 'easy', 'highly', 'enjoyable', 'love', 'favorite', 'fantastic', 'enjoyed', 'brilliant', 'entertaining', 'well', 'awesome', 'classic', 'funny', 'loved', 'interesting', 'nice', 'incredible', 'useful', 'amazing', 'fascinating', 'superb', 'solid', 'valuable', 'amusing', 'readable', 'compelling', 'wonderful', 'refreshing', 'fun', 'important', 'wonderfully', 'sad', 'inspiring', 'inspirational', 'invaluable', 'essential', 'riveting', 'impressive', 'finest', 'hilarious', 'enlightening', 'liked', 'fabulous', 'authentic', 'pleasant', 'honest', 'outstanding', 'real', 'worthwhile']
['disappointing', 'boring', 'disappointed', 'poorly', 'worst', 'horrible', 'useless', 'annoying', 'repetitive', 'hard', 'confusing', 'misleading', 'better', 'difficult', 'awful', 'ridiculous', 'pathetic', 'laughable', 'unnecessary', 'mediocre', 'tedious', 'lacks', 'shallow', 'terrible', 'slow', 'simplistic', 'wasted', 'frustrating', 'flawed', 'silly', 'bad', 'waste', 'trite', 'outdated', 'uninspired', 'contrived', 'uninteresting', 'tired', 'predictable', 'sloppy', 'dissapointed', 'disjointed', 'weak', 'dangerous', 'unreadable', 'tiresome', 'insulting', 'worthless', 'trying', 'dull', 'sorry', 'choppy', 'unbelievable', 'terribly', 'impossible', 'wastes', 'sophomoric', 'absurd', 'deceived', 'expensive', 'incorrect', 'weakest', 'superficial', 'overrated', 'biased', 'inaccurate', 'hackneyed']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
5600 400 6000 15750 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 409.05693364, sen-loss: 76.85290730, dom-loss: 78.35469878, src-aux-loss: 133.44113141, tar-aux-loss: 120.40819710
Epoch: [1  ] train-acc: 0.66553571, dom-acc: 0.83625000, val-acc: 0.68500000, val_loss: 0.65277851
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 380.66014814, sen-loss: 70.19480938, dom-loss: 73.80044436, src-aux-loss: 124.99870861, tar-aux-loss: 111.66618282
Epoch: [2  ] train-acc: 0.75910714, dom-acc: 0.91116071, val-acc: 0.75750000, val_loss: 0.58464283
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 363.23026252, sen-loss: 62.58652833, dom-loss: 72.61966932, src-aux-loss: 120.16971523, tar-aux-loss: 107.85435027
Epoch: [3  ] train-acc: 0.79714286, dom-acc: 0.91196429, val-acc: 0.80500000, val_loss: 0.50503361
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 349.39539623, sen-loss: 54.46641156, dom-loss: 72.68359917, src-aux-loss: 116.68364257, tar-aux-loss: 105.56174487
Epoch: [4  ] train-acc: 0.82875000, dom-acc: 0.89937500, val-acc: 0.83750000, val_loss: 0.43023112
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 339.58564115, sen-loss: 47.75326946, dom-loss: 73.32020813, src-aux-loss: 114.22205502, tar-aux-loss: 104.29010868
Epoch: [5  ] train-acc: 0.85017857, dom-acc: 0.86017857, val-acc: 0.85250000, val_loss: 0.37589303
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 331.77390742, sen-loss: 42.54221109, dom-loss: 74.13798368, src-aux-loss: 112.17998230, tar-aux-loss: 102.91372967
Epoch: [6  ] train-acc: 0.86017857, dom-acc: 0.78937500, val-acc: 0.86250000, val_loss: 0.34817320
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 326.98544812, sen-loss: 39.19902536, dom-loss: 74.82243019, src-aux-loss: 110.98193645, tar-aux-loss: 101.98205525
Epoch: [7  ] train-acc: 0.86875000, dom-acc: 0.73089286, val-acc: 0.87000000, val_loss: 0.33522782
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 322.51944804, sen-loss: 36.75751056, dom-loss: 75.78448808, src-aux-loss: 109.51530427, tar-aux-loss: 100.46214485
Epoch: [8  ] train-acc: 0.87678571, dom-acc: 0.68785714, val-acc: 0.86500000, val_loss: 0.31929773
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 321.09698987, sen-loss: 35.33278738, dom-loss: 76.69059587, src-aux-loss: 108.44053417, tar-aux-loss: 100.63307321
Epoch: [9  ] train-acc: 0.88571429, dom-acc: 0.66705357, val-acc: 0.88250000, val_loss: 0.32037473
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 318.87493229, sen-loss: 33.59090702, dom-loss: 77.02632195, src-aux-loss: 107.84891856, tar-aux-loss: 100.40878421
Epoch: [10 ] train-acc: 0.88464286, dom-acc: 0.63803571, val-acc: 0.87000000, val_loss: 0.31282055
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 317.22986531, sen-loss: 33.01429224, dom-loss: 78.08726484, src-aux-loss: 107.18924725, tar-aux-loss: 98.93906087
Epoch: [11 ] train-acc: 0.89285714, dom-acc: 0.62330357, val-acc: 0.88250000, val_loss: 0.31417790
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 316.05474782, sen-loss: 32.02986524, dom-loss: 77.83057266, src-aux-loss: 106.18769282, tar-aux-loss: 100.00661647
Epoch: [12 ] train-acc: 0.89625000, dom-acc: 0.62107143, val-acc: 0.88000000, val_loss: 0.30871254
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 315.97815275, sen-loss: 31.41787867, dom-loss: 78.92689466, src-aux-loss: 105.67424750, tar-aux-loss: 99.95913166
Epoch: [13 ] train-acc: 0.88982143, dom-acc: 0.59517857, val-acc: 0.87000000, val_loss: 0.31179807
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 312.01463795, sen-loss: 30.76301096, dom-loss: 78.88223141, src-aux-loss: 105.13050383, tar-aux-loss: 97.23889035
Epoch: [14 ] train-acc: 0.89803571, dom-acc: 0.59758929, val-acc: 0.87500000, val_loss: 0.30561811
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 311.56101894, sen-loss: 30.33572482, dom-loss: 79.07153571, src-aux-loss: 104.65749007, tar-aux-loss: 97.49626750
Epoch: [15 ] train-acc: 0.90071429, dom-acc: 0.59616071, val-acc: 0.88500000, val_loss: 0.30209467
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 310.17193818, sen-loss: 29.66637121, dom-loss: 78.92033756, src-aux-loss: 104.07741421, tar-aux-loss: 97.50781548
Epoch: [16 ] train-acc: 0.90410714, dom-acc: 0.60330357, val-acc: 0.89000000, val_loss: 0.30472454
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 309.31423664, sen-loss: 29.46780878, dom-loss: 78.51735562, src-aux-loss: 103.64443278, tar-aux-loss: 97.68463981
Epoch: [17 ] train-acc: 0.90553571, dom-acc: 0.60857143, val-acc: 0.88750000, val_loss: 0.30274266
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 307.40490437, sen-loss: 28.82688604, dom-loss: 78.34995669, src-aux-loss: 103.12615716, tar-aux-loss: 97.10190570
Epoch: [18 ] train-acc: 0.90642857, dom-acc: 0.61866071, val-acc: 0.89000000, val_loss: 0.30268648
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 307.10254407, sen-loss: 28.50300603, dom-loss: 78.20361954, src-aux-loss: 102.65273649, tar-aux-loss: 97.74318111
Epoch: [19 ] train-acc: 0.90607143, dom-acc: 0.61946429, val-acc: 0.88250000, val_loss: 0.30052412
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 305.13399887, sen-loss: 28.21620762, dom-loss: 77.89791334, src-aux-loss: 102.21714664, tar-aux-loss: 96.80273104
Epoch: [20 ] train-acc: 0.90839286, dom-acc: 0.63178571, val-acc: 0.89000000, val_loss: 0.30091116
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 303.19808578, sen-loss: 27.77963388, dom-loss: 77.37423527, src-aux-loss: 101.85021263, tar-aux-loss: 96.19400340
Epoch: [21 ] train-acc: 0.90767857, dom-acc: 0.63678571, val-acc: 0.88500000, val_loss: 0.30027753
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 303.79582119, sen-loss: 27.52008943, dom-loss: 77.33845252, src-aux-loss: 101.54695612, tar-aux-loss: 97.39032507
Epoch: [22 ] train-acc: 0.91160714, dom-acc: 0.64839286, val-acc: 0.89000000, val_loss: 0.30100247
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 301.71152711, sen-loss: 27.32846070, dom-loss: 76.75922787, src-aux-loss: 101.20999116, tar-aux-loss: 96.41384572
Epoch: [23 ] train-acc: 0.91232143, dom-acc: 0.65178571, val-acc: 0.89000000, val_loss: 0.29813635
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 300.02491879, sen-loss: 26.85411759, dom-loss: 76.61438203, src-aux-loss: 100.75084668, tar-aux-loss: 95.80557281
Epoch: [24 ] train-acc: 0.91375000, dom-acc: 0.65732143, val-acc: 0.89250000, val_loss: 0.29732165
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 301.05577111, sen-loss: 26.67652227, dom-loss: 76.74725676, src-aux-loss: 100.30399024, tar-aux-loss: 97.32800144
Epoch: [25 ] train-acc: 0.91339286, dom-acc: 0.65508929, val-acc: 0.88750000, val_loss: 0.29860094
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 299.13977456, sen-loss: 26.31126708, dom-loss: 76.78295559, src-aux-loss: 99.99454498, tar-aux-loss: 96.05100751
Epoch: [26 ] train-acc: 0.91446429, dom-acc: 0.65866071, val-acc: 0.89000000, val_loss: 0.29866180
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 298.20882773, sen-loss: 26.13357400, dom-loss: 76.52436370, src-aux-loss: 99.92767960, tar-aux-loss: 95.62320822
Epoch: [27 ] train-acc: 0.91464286, dom-acc: 0.65330357, val-acc: 0.89250000, val_loss: 0.29701507
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 297.87601042, sen-loss: 25.82244360, dom-loss: 77.00917011, src-aux-loss: 99.29953712, tar-aux-loss: 95.74485934
Epoch: [28 ] train-acc: 0.91821429, dom-acc: 0.65526786, val-acc: 0.89250000, val_loss: 0.30134866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 296.74614882, sen-loss: 25.37991535, dom-loss: 76.84169954, src-aux-loss: 98.86526889, tar-aux-loss: 95.65926558
Epoch: [29 ] train-acc: 0.91803571, dom-acc: 0.64517857, val-acc: 0.89250000, val_loss: 0.29868212
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 295.95060396, sen-loss: 25.33467977, dom-loss: 77.19859236, src-aux-loss: 98.56164336, tar-aux-loss: 94.85568887
Epoch: [30 ] train-acc: 0.91928571, dom-acc: 0.64133929, val-acc: 0.89000000, val_loss: 0.29699859
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 295.87506914, sen-loss: 24.88047225, dom-loss: 77.48158944, src-aux-loss: 98.24345380, tar-aux-loss: 95.26955420
Epoch: [31 ] train-acc: 0.92160714, dom-acc: 0.63330357, val-acc: 0.89500000, val_loss: 0.30249447
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 295.70912981, sen-loss: 24.64732555, dom-loss: 77.59702766, src-aux-loss: 97.90644640, tar-aux-loss: 95.55833054
Epoch: [32 ] train-acc: 0.92160714, dom-acc: 0.62071429, val-acc: 0.88500000, val_loss: 0.30748436
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 294.43154383, sen-loss: 24.57259931, dom-loss: 77.76887476, src-aux-loss: 97.53872210, tar-aux-loss: 94.55134761
Epoch: [33 ] train-acc: 0.92214286, dom-acc: 0.61401786, val-acc: 0.89250000, val_loss: 0.29598153
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 295.30680585, sen-loss: 24.14278513, dom-loss: 78.05586761, src-aux-loss: 97.40358502, tar-aux-loss: 95.70456827
Epoch: [34 ] train-acc: 0.92267857, dom-acc: 0.60473214, val-acc: 0.89500000, val_loss: 0.29756460
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 292.96872306, sen-loss: 24.03188755, dom-loss: 78.13196027, src-aux-loss: 96.84262568, tar-aux-loss: 93.96224779
Epoch: [35 ] train-acc: 0.92517857, dom-acc: 0.59508929, val-acc: 0.89250000, val_loss: 0.29628181
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 292.26871467, sen-loss: 23.57571766, dom-loss: 78.33713174, src-aux-loss: 96.16150588, tar-aux-loss: 94.19436008
Epoch: [36 ] train-acc: 0.92750000, dom-acc: 0.58964286, val-acc: 0.90000000, val_loss: 0.30165389
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 292.87460542, sen-loss: 23.45788576, dom-loss: 78.38421631, src-aux-loss: 96.07702243, tar-aux-loss: 94.95548087
Epoch: [37 ] train-acc: 0.92732143, dom-acc: 0.58455357, val-acc: 0.89500000, val_loss: 0.29741681
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 291.21414495, sen-loss: 23.09926932, dom-loss: 78.43950772, src-aux-loss: 95.84707648, tar-aux-loss: 93.82829171
Epoch: [38 ] train-acc: 0.92875000, dom-acc: 0.58321429, val-acc: 0.89250000, val_loss: 0.30254641
---------------------------------------------------

Successfully load model from save path: ./work/models/books_kitchen_HATN.ckpt
Best Epoch: [ 33] best val accuracy: 0.00000000 best val loss: 0.29598153
Testing accuracy: 0.87083333
./work/attentions/books_kitchen_train_HATN.txt
./work/attentions/books_kitchen_test_HATN.txt
loading data...
source domain:  books target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 30180
vocab-size:  98084
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'favorite', 'love', 'funny', 'fantastic', 'brilliant', 'awesome', 'enjoyed', 'classic', 'entertaining', 'well', 'loved', 'interesting', 'inspiring', 'fascinating', 'incredible', 'amazing', 'refreshing', 'solid', 'valuable', 'nice', 'superb', 'compelling', 'hilarious', 'inspirational', 'useful', 'important', 'readable', 'fun', 'invaluable', 'essential', 'impressive', 'wonderfully', 'authentic', 'honest', 'sad', 'enlightening', 'liked', 'simple', 'riveting', 'pleasant', 'loves', 'beautifully', 'greatest', 'real', 'provoking', 'humorous', 'deserves', 'heartwarming', 'emotional', 'believable']
['disappointing', 'boring', 'poorly', 'disappointed', 'horrible', 'worst', 'useless', 'annoying', 'repetitive', 'hard', 'confusing', 'difficult', 'lacks', 'better', 'misleading', 'tedious', 'awful', 'pathetic', 'laughable', 'unnecessary', 'ridiculous', 'simplistic', 'flawed', 'mediocre', 'tired', 'shallow', 'slow', 'frustrating', 'amusing', 'uninteresting', 'terrible', 'wasted', 'outdated', 'uninspired', 'silly', 'waste', 'trite', 'sloppy', 'contrived', 'predictable', 'dissapointed', 'bad', 'disjointed', 'choppy', 'unreadable', 'wrong', 'lame', 'tiresome', 'impossible', 'wastes', 'false', 'insulting', 'worthless', 'trying', 'dangerous', 'sorry', 'clumsy', 'terribly', 'sophomoric', 'absurd', 'rambling', 'deceived', 'weak', 'expensive', 'unlikeable', 'unfortunate', 'weakest', 'superficial', 'repetitious', 'overrated', 'biased', 'inaccurate', 'hackneyed', 'wasting']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 15750 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 405.90112567, sen-loss: 76.73069715, dom-loss: 79.41594672, src-aux-loss: 131.16805226, tar-aux-loss: 118.58642864
Epoch: [1  ] train-acc: 0.69142857, dom-acc: 0.65553571, val-acc: 0.69500000, val_loss: 0.64996397
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 378.39426088, sen-loss: 69.82193625, dom-loss: 76.65946662, src-aux-loss: 122.72987270, tar-aux-loss: 109.18298656
Epoch: [2  ] train-acc: 0.75000000, dom-acc: 0.70946429, val-acc: 0.76250000, val_loss: 0.58127093
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 361.49222946, sen-loss: 62.02983010, dom-loss: 75.87968588, src-aux-loss: 117.79486424, tar-aux-loss: 105.78784728
Epoch: [3  ] train-acc: 0.80089286, dom-acc: 0.70241071, val-acc: 0.81250000, val_loss: 0.49884915
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 347.18865037, sen-loss: 54.04012504, dom-loss: 75.45760047, src-aux-loss: 114.41184956, tar-aux-loss: 103.27907497
Epoch: [4  ] train-acc: 0.83017857, dom-acc: 0.69410714, val-acc: 0.84250000, val_loss: 0.42586046
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 335.11676002, sen-loss: 47.12918085, dom-loss: 75.41269386, src-aux-loss: 112.13213009, tar-aux-loss: 100.44275397
Epoch: [5  ] train-acc: 0.85125000, dom-acc: 0.71500000, val-acc: 0.84500000, val_loss: 0.37084982
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 326.83093190, sen-loss: 42.10478723, dom-loss: 75.34919649, src-aux-loss: 109.81497037, tar-aux-loss: 99.56197530
Epoch: [6  ] train-acc: 0.85857143, dom-acc: 0.75964286, val-acc: 0.86000000, val_loss: 0.35030520
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 320.98313689, sen-loss: 39.14465322, dom-loss: 75.18722528, src-aux-loss: 108.63437039, tar-aux-loss: 98.01689011
Epoch: [7  ] train-acc: 0.86946429, dom-acc: 0.76116071, val-acc: 0.87500000, val_loss: 0.32998177
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 316.65677571, sen-loss: 36.83207814, dom-loss: 75.22840035, src-aux-loss: 107.05727720, tar-aux-loss: 97.53902125
Epoch: [8  ] train-acc: 0.87892857, dom-acc: 0.73991071, val-acc: 0.86750000, val_loss: 0.31534874
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 313.70842409, sen-loss: 35.46112095, dom-loss: 75.33817399, src-aux-loss: 106.02297080, tar-aux-loss: 96.88615680
Epoch: [9  ] train-acc: 0.87875000, dom-acc: 0.72669643, val-acc: 0.88250000, val_loss: 0.32227394
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 311.96177435, sen-loss: 33.76032883, dom-loss: 75.42742991, src-aux-loss: 105.24173844, tar-aux-loss: 97.53227603
Epoch: [10 ] train-acc: 0.88250000, dom-acc: 0.70589286, val-acc: 0.86250000, val_loss: 0.30850303
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 308.07755375, sen-loss: 33.05458674, dom-loss: 75.69376600, src-aux-loss: 104.82190967, tar-aux-loss: 94.50729138
Epoch: [11 ] train-acc: 0.89071429, dom-acc: 0.70839286, val-acc: 0.88250000, val_loss: 0.31012022
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 308.19242406, sen-loss: 32.05016999, dom-loss: 75.63464099, src-aux-loss: 103.70978546, tar-aux-loss: 96.79782742
Epoch: [12 ] train-acc: 0.89660714, dom-acc: 0.70482143, val-acc: 0.89000000, val_loss: 0.30247292
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 305.96221709, sen-loss: 31.50539917, dom-loss: 75.98958403, src-aux-loss: 103.18400842, tar-aux-loss: 95.28322619
Epoch: [13 ] train-acc: 0.89303571, dom-acc: 0.68946429, val-acc: 0.86500000, val_loss: 0.30451098
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 303.22564363, sen-loss: 30.81596965, dom-loss: 76.02326423, src-aux-loss: 102.70062220, tar-aux-loss: 93.68578839
Epoch: [14 ] train-acc: 0.89750000, dom-acc: 0.69348214, val-acc: 0.87500000, val_loss: 0.30131111
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 303.16492176, sen-loss: 30.38343708, dom-loss: 76.09329611, src-aux-loss: 102.16381830, tar-aux-loss: 94.52437145
Epoch: [15 ] train-acc: 0.90107143, dom-acc: 0.69214286, val-acc: 0.88500000, val_loss: 0.29907116
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 301.04926538, sen-loss: 29.73318103, dom-loss: 76.09080511, src-aux-loss: 101.70804310, tar-aux-loss: 93.51723599
Epoch: [16 ] train-acc: 0.90446429, dom-acc: 0.68473214, val-acc: 0.88250000, val_loss: 0.29972497
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 301.80141401, sen-loss: 29.57452722, dom-loss: 76.42011911, src-aux-loss: 101.19250160, tar-aux-loss: 94.61426759
Epoch: [17 ] train-acc: 0.90535714, dom-acc: 0.68303571, val-acc: 0.88750000, val_loss: 0.30034438
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 300.21622157, sen-loss: 28.86850959, dom-loss: 76.40900505, src-aux-loss: 100.70332038, tar-aux-loss: 94.23538536
Epoch: [18 ] train-acc: 0.90678571, dom-acc: 0.68366071, val-acc: 0.89000000, val_loss: 0.29901606
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 298.29582548, sen-loss: 28.59389028, dom-loss: 76.69070363, src-aux-loss: 100.08804142, tar-aux-loss: 92.92319161
Epoch: [19 ] train-acc: 0.90785714, dom-acc: 0.67196429, val-acc: 0.89250000, val_loss: 0.29616007
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 298.69574881, sen-loss: 28.25075468, dom-loss: 76.90703762, src-aux-loss: 99.65147656, tar-aux-loss: 93.88648069
Epoch: [20 ] train-acc: 0.91053571, dom-acc: 0.67437500, val-acc: 0.88750000, val_loss: 0.29641920
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 296.49645925, sen-loss: 27.78968388, dom-loss: 76.80379719, src-aux-loss: 99.34724945, tar-aux-loss: 92.55572647
Epoch: [21 ] train-acc: 0.90678571, dom-acc: 0.66151786, val-acc: 0.87750000, val_loss: 0.29539728
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 296.28835607, sen-loss: 27.54640129, dom-loss: 77.04183781, src-aux-loss: 99.07901978, tar-aux-loss: 92.62109822
Epoch: [22 ] train-acc: 0.91250000, dom-acc: 0.66339286, val-acc: 0.89000000, val_loss: 0.29674500
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 295.31527352, sen-loss: 27.33204557, dom-loss: 76.78876346, src-aux-loss: 98.62286323, tar-aux-loss: 92.57160133
Epoch: [23 ] train-acc: 0.91267857, dom-acc: 0.65491071, val-acc: 0.88750000, val_loss: 0.29411107
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 294.64339089, sen-loss: 26.91013811, dom-loss: 77.15152419, src-aux-loss: 98.22748661, tar-aux-loss: 92.35424107
Epoch: [24 ] train-acc: 0.91482143, dom-acc: 0.66401786, val-acc: 0.89000000, val_loss: 0.29605699
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.43399811, sen-loss: 26.75253086, dom-loss: 77.27475899, src-aux-loss: 97.96119857, tar-aux-loss: 93.44551080
Epoch: [25 ] train-acc: 0.91232143, dom-acc: 0.63580357, val-acc: 0.89250000, val_loss: 0.29439038
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.32492352, sen-loss: 26.30814003, dom-loss: 77.25105107, src-aux-loss: 97.32521731, tar-aux-loss: 92.44051379
Epoch: [26 ] train-acc: 0.91500000, dom-acc: 0.64535714, val-acc: 0.89250000, val_loss: 0.29541969
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 292.17379928, sen-loss: 26.08412344, dom-loss: 77.35506141, src-aux-loss: 97.23844552, tar-aux-loss: 91.49616599
Epoch: [27 ] train-acc: 0.91589286, dom-acc: 0.64616071, val-acc: 0.89000000, val_loss: 0.29356655
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 290.05870318, sen-loss: 25.77654666, dom-loss: 77.35428190, src-aux-loss: 96.72337943, tar-aux-loss: 90.20449507
Epoch: [28 ] train-acc: 0.91803571, dom-acc: 0.65321429, val-acc: 0.88750000, val_loss: 0.29660267
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 290.71385980, sen-loss: 25.47034798, dom-loss: 77.14527643, src-aux-loss: 96.19853020, tar-aux-loss: 91.89970481
Epoch: [29 ] train-acc: 0.91910714, dom-acc: 0.63955357, val-acc: 0.89250000, val_loss: 0.29420215
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.00047851, sen-loss: 25.35334964, dom-loss: 77.44760841, src-aux-loss: 95.77853554, tar-aux-loss: 91.42098510
Epoch: [30 ] train-acc: 0.92000000, dom-acc: 0.64571429, val-acc: 0.88750000, val_loss: 0.29521531
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 290.17010665, sen-loss: 24.93570482, dom-loss: 77.54325181, src-aux-loss: 95.63017201, tar-aux-loss: 92.06097949
Epoch: [31 ] train-acc: 0.92071429, dom-acc: 0.65232143, val-acc: 0.88750000, val_loss: 0.29818004
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.90930200, sen-loss: 24.69125596, dom-loss: 77.53393006, src-aux-loss: 95.26546842, tar-aux-loss: 91.41864771
Epoch: [32 ] train-acc: 0.92285714, dom-acc: 0.65580357, val-acc: 0.88000000, val_loss: 0.30205691
---------------------------------------------------

Successfully load model from save path: ./work/models/books_video_HATN.ckpt
Best Epoch: [ 27] best val accuracy: 0.00000000 best val loss: 0.29356655
Testing accuracy: 0.87350000
./work/attentions/books_video_train_HATN.txt
./work/attentions/books_video_test_HATN.txt
loading data...
source domain:  dvd target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 9750
vocab-size:  100530
['best', 'good', 'excellent', 'great', 'enjoyable', 'funny', 'entertaining', 'nice', 'fantastic', 'awesome', 'classic', 'love', 'funniest', 'hilarious', 'wonderful', 'loved', 'amazing', 'underrated', 'favorite', 'perfect', 'sad', 'brilliant', 'interesting', 'finest', 'superb', 'easy', 'real', 'solid', 'greatest', 'recommend', 'outstanding', 'beautifully', 'amusing', 'terrific', 'beautiful', 'fine', 'enjoy', 'pleasant', 'fascinating', 'liked', 'worthy', 'memorable', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'disappointing', 'poor', 'terrible', 'awful', 'disappointed', 'dull', 'wasted', 'waste', 'annoying', 'better', 'laughable', 'unfunny', 'forgettable', 'sucks', 'lousy', 'predictable', 'stupid', 'unwatchable', 'uninspired', 'pathetic', 'poorly', 'contrived', 'slow', 'ok', 'pointless', 'sorry', 'unoriginal', 'decent', 'disgusting', 'wrong', 'sucked', 'dissapointed', 'worse', 'biased', 'lame', 'horrendous', 'overrated', 'atrocious', 'depressing', 'disjointed', 'crappy', 'mediocre', 'irritating', 'dumbest', 'okay', 'disapointed', 'ridiculous', 'silly', 'useless', 'weak', 'stupidest', 'frustrating', 'wasting', 'ruined']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
5600 400 6000 17843 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 406.00254393, sen-loss: 77.27335972, dom-loss: 79.34773290, src-aux-loss: 128.30627370, tar-aux-loss: 121.07517886
Epoch: [1  ] train-acc: 0.64732143, dom-acc: 0.70687500, val-acc: 0.67750000, val_loss: 0.65747088
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 379.03942370, sen-loss: 71.51658148, dom-loss: 76.48869562, src-aux-loss: 119.23424178, tar-aux-loss: 111.79990542
Epoch: [2  ] train-acc: 0.75035714, dom-acc: 0.80491071, val-acc: 0.75500000, val_loss: 0.59555179
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 361.68037271, sen-loss: 65.05456370, dom-loss: 75.47669846, src-aux-loss: 114.13564628, tar-aux-loss: 107.01346773
Epoch: [3  ] train-acc: 0.76910714, dom-acc: 0.82535714, val-acc: 0.76000000, val_loss: 0.53009421
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 346.69184208, sen-loss: 57.72135794, dom-loss: 74.82478869, src-aux-loss: 110.43068677, tar-aux-loss: 103.71501136
Epoch: [4  ] train-acc: 0.81785714, dom-acc: 0.82053571, val-acc: 0.80000000, val_loss: 0.45596036
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 335.27715182, sen-loss: 50.80190194, dom-loss: 74.60256952, src-aux-loss: 107.14692259, tar-aux-loss: 102.72575688
Epoch: [5  ] train-acc: 0.82089286, dom-acc: 0.79339286, val-acc: 0.82500000, val_loss: 0.41720256
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 327.65877128, sen-loss: 45.37661105, dom-loss: 74.28179997, src-aux-loss: 105.41524523, tar-aux-loss: 102.58511496
Epoch: [6  ] train-acc: 0.85535714, dom-acc: 0.76428571, val-acc: 0.88000000, val_loss: 0.34733984
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 318.05895543, sen-loss: 40.73418914, dom-loss: 74.50453585, src-aux-loss: 103.26446319, tar-aux-loss: 99.55576378
Epoch: [7  ] train-acc: 0.87035714, dom-acc: 0.75964286, val-acc: 0.89000000, val_loss: 0.32293606
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 313.47112370, sen-loss: 38.09258491, dom-loss: 74.54784918, src-aux-loss: 102.27045834, tar-aux-loss: 98.56023204
Epoch: [8  ] train-acc: 0.87607143, dom-acc: 0.74937500, val-acc: 0.88750000, val_loss: 0.31227362
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 312.62934566, sen-loss: 36.84713456, dom-loss: 74.63540685, src-aux-loss: 100.86737484, tar-aux-loss: 100.27943009
Epoch: [9  ] train-acc: 0.88089286, dom-acc: 0.74526786, val-acc: 0.89000000, val_loss: 0.30447903
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 309.39809012, sen-loss: 35.49823225, dom-loss: 74.74489826, src-aux-loss: 100.21704471, tar-aux-loss: 98.93791366
Epoch: [10 ] train-acc: 0.88214286, dom-acc: 0.74071429, val-acc: 0.89500000, val_loss: 0.29776713
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 309.03945899, sen-loss: 34.93117665, dom-loss: 75.02963603, src-aux-loss: 99.66780031, tar-aux-loss: 99.41084737
Epoch: [11 ] train-acc: 0.88375000, dom-acc: 0.73544643, val-acc: 0.89500000, val_loss: 0.30634221
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 304.97958875, sen-loss: 34.17294446, dom-loss: 75.20741695, src-aux-loss: 98.80111420, tar-aux-loss: 96.79811263
Epoch: [12 ] train-acc: 0.88750000, dom-acc: 0.73357143, val-acc: 0.89750000, val_loss: 0.28908914
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 303.45582485, sen-loss: 33.50725364, dom-loss: 75.16659087, src-aux-loss: 97.93117213, tar-aux-loss: 96.85080779
Epoch: [13 ] train-acc: 0.89160714, dom-acc: 0.72848214, val-acc: 0.88750000, val_loss: 0.29177454
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 302.19175267, sen-loss: 32.77044138, dom-loss: 75.46594417, src-aux-loss: 97.30514210, tar-aux-loss: 96.65022331
Epoch: [14 ] train-acc: 0.89053571, dom-acc: 0.72294643, val-acc: 0.89500000, val_loss: 0.29299420
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 301.57888579, sen-loss: 32.21049047, dom-loss: 75.59177446, src-aux-loss: 96.78672713, tar-aux-loss: 96.98989230
Epoch: [15 ] train-acc: 0.89464286, dom-acc: 0.72232143, val-acc: 0.89250000, val_loss: 0.28581899
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 301.20406175, sen-loss: 31.77671748, dom-loss: 76.00200510, src-aux-loss: 96.07060975, tar-aux-loss: 97.35473102
Epoch: [16 ] train-acc: 0.89607143, dom-acc: 0.71660714, val-acc: 0.89750000, val_loss: 0.28337249
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 300.14220452, sen-loss: 31.35833971, dom-loss: 76.00245678, src-aux-loss: 95.55938196, tar-aux-loss: 97.22202557
Epoch: [17 ] train-acc: 0.89339286, dom-acc: 0.71151786, val-acc: 0.89250000, val_loss: 0.28999844
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 297.68262410, sen-loss: 30.96731617, dom-loss: 76.19391966, src-aux-loss: 95.06663334, tar-aux-loss: 95.45475465
Epoch: [18 ] train-acc: 0.89910714, dom-acc: 0.70705357, val-acc: 0.89000000, val_loss: 0.27802253
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 296.66693330, sen-loss: 30.64157359, dom-loss: 76.21517611, src-aux-loss: 94.54890078, tar-aux-loss: 95.26128173
Epoch: [19 ] train-acc: 0.90000000, dom-acc: 0.70178571, val-acc: 0.89000000, val_loss: 0.27463222
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.64833975, sen-loss: 30.33921552, dom-loss: 76.68268764, src-aux-loss: 94.19060135, tar-aux-loss: 96.43583494
Epoch: [20 ] train-acc: 0.90107143, dom-acc: 0.70098214, val-acc: 0.89250000, val_loss: 0.27395549
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 295.01580524, sen-loss: 29.89764141, dom-loss: 76.38258779, src-aux-loss: 93.72175479, tar-aux-loss: 95.01382059
Epoch: [21 ] train-acc: 0.89946429, dom-acc: 0.69741071, val-acc: 0.90000000, val_loss: 0.27701557
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 294.86954093, sen-loss: 29.56665360, dom-loss: 76.80162460, src-aux-loss: 93.39276177, tar-aux-loss: 95.10849929
Epoch: [22 ] train-acc: 0.89821429, dom-acc: 0.69607143, val-acc: 0.89750000, val_loss: 0.28867239
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 294.24231124, sen-loss: 29.45620821, dom-loss: 76.68298215, src-aux-loss: 92.82784152, tar-aux-loss: 95.27527905
Epoch: [23 ] train-acc: 0.90196429, dom-acc: 0.68946429, val-acc: 0.89750000, val_loss: 0.27340981
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 294.33807492, sen-loss: 29.05501379, dom-loss: 76.98791951, src-aux-loss: 92.46165758, tar-aux-loss: 95.83348262
Epoch: [24 ] train-acc: 0.90535714, dom-acc: 0.68830357, val-acc: 0.89500000, val_loss: 0.26903132
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 291.14554739, sen-loss: 28.93134373, dom-loss: 76.98939532, src-aux-loss: 92.04125994, tar-aux-loss: 93.18354887
Epoch: [25 ] train-acc: 0.90571429, dom-acc: 0.68410714, val-acc: 0.89250000, val_loss: 0.26819319
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.42780614, sen-loss: 28.44979327, dom-loss: 77.12244570, src-aux-loss: 91.59827143, tar-aux-loss: 96.25729656
Epoch: [26 ] train-acc: 0.90357143, dom-acc: 0.68544643, val-acc: 0.89750000, val_loss: 0.27264166
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 290.83974123, sen-loss: 28.30514238, dom-loss: 76.87930673, src-aux-loss: 91.15651530, tar-aux-loss: 94.49877751
Epoch: [27 ] train-acc: 0.90589286, dom-acc: 0.68508929, val-acc: 0.90000000, val_loss: 0.27124205
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 290.85586262, sen-loss: 27.93125145, dom-loss: 77.45376712, src-aux-loss: 90.82405746, tar-aux-loss: 94.64678657
Epoch: [28 ] train-acc: 0.90839286, dom-acc: 0.67419643, val-acc: 0.90000000, val_loss: 0.26301673
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 289.73605251, sen-loss: 27.90660137, dom-loss: 77.38217312, src-aux-loss: 90.53161871, tar-aux-loss: 93.91565859
Epoch: [29 ] train-acc: 0.90767857, dom-acc: 0.67098214, val-acc: 0.89250000, val_loss: 0.26098600
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.23327351, sen-loss: 27.58921537, dom-loss: 77.06326538, src-aux-loss: 89.95234519, tar-aux-loss: 94.62844801
Epoch: [30 ] train-acc: 0.90625000, dom-acc: 0.67580357, val-acc: 0.89500000, val_loss: 0.27317920
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 288.51372147, sen-loss: 27.17544802, dom-loss: 77.43634909, src-aux-loss: 89.69273698, tar-aux-loss: 94.20918554
Epoch: [31 ] train-acc: 0.91053571, dom-acc: 0.67258929, val-acc: 0.89750000, val_loss: 0.26440802
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 286.88937807, sen-loss: 26.93652135, dom-loss: 77.44680864, src-aux-loss: 89.06227827, tar-aux-loss: 93.44377077
Epoch: [32 ] train-acc: 0.91017857, dom-acc: 0.67348214, val-acc: 0.90000000, val_loss: 0.26945469
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 286.80171323, sen-loss: 26.62378172, dom-loss: 77.26512718, src-aux-loss: 88.80876511, tar-aux-loss: 94.10403925
Epoch: [33 ] train-acc: 0.89910714, dom-acc: 0.68303571, val-acc: 0.88750000, val_loss: 0.30078608
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 286.48213148, sen-loss: 26.77445916, dom-loss: 77.51861560, src-aux-loss: 88.37224299, tar-aux-loss: 93.81681389
Epoch: [34 ] train-acc: 0.91500000, dom-acc: 0.67044643, val-acc: 0.89500000, val_loss: 0.26108661
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_books_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26098600
Testing accuracy: 0.88016667
./work/attentions/dvd_books_train_HATN.txt
./work/attentions/dvd_books_test_HATN.txt
loading data...
source domain:  dvd target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 17009
vocab-size:  85442
['good', 'best', 'excellent', 'great', 'funny', 'enjoyable', 'nice', 'entertaining', 'classic', 'awesome', 'fantastic', 'funniest', 'hilarious', 'love', 'perfect', 'underrated', 'better', 'loved', 'amazing', 'sad', 'favorite', 'brilliant', 'interesting', 'superb', 'finest', 'wonderful', 'recommend', 'solid', 'easy', 'beautifully', 'fine', 'real', 'outstanding', 'enjoy', 'terrific', 'amusing', 'happy', 'beautiful', 'fascinating', 'sweet', 'well', 'likeable', 'worthy', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'disappointing', 'poor', 'awful', 'terrible', 'disappointed', 'dull', 'wasted', 'annoying', 'waste', 'decent', 'forgettable', 'sucks', 'lousy', 'stupid', 'worse', 'laughable', 'predictable', 'unwatchable', 'unfunny', 'uninspired', 'pathetic', 'ok', 'poorly', 'wrong', 'contrived', 'slow', 'dissapointed', 'disgusting', 'unoriginal', 'crappy', 'pointless', 'sorry', 'atrocious', 'mediocre', 'lame', 'sucked', 'overrated', 'biased', 'defective', 'disapointed', 'depressing', 'ridiculous', 'horrendous', 'irritating', 'dumbest', 'okay', 'silly', 'useless', 'cheap', 'weak', 'stupidest', 'unconvincing', 'frustrating', 'wasting', 'ruined']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
5600 400 6000 17843 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 393.53175783, sen-loss: 77.37788785, dom-loss: 78.50667113, src-aux-loss: 124.51940572, tar-aux-loss: 113.12779093
Epoch: [1  ] train-acc: 0.64625000, dom-acc: 0.82500000, val-acc: 0.69500000, val_loss: 0.65855527
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 366.34877729, sen-loss: 71.88917047, dom-loss: 74.77004999, src-aux-loss: 116.00831562, tar-aux-loss: 103.68124080
Epoch: [2  ] train-acc: 0.75553571, dom-acc: 0.87848214, val-acc: 0.75500000, val_loss: 0.59928417
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 348.51827216, sen-loss: 65.54432809, dom-loss: 73.62303829, src-aux-loss: 111.56774163, tar-aux-loss: 97.78316581
Epoch: [3  ] train-acc: 0.77875000, dom-acc: 0.87178571, val-acc: 0.77250000, val_loss: 0.53019994
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 336.52212787, sen-loss: 57.92792204, dom-loss: 73.80545628, src-aux-loss: 107.83711994, tar-aux-loss: 96.95163012
Epoch: [4  ] train-acc: 0.81767857, dom-acc: 0.85375000, val-acc: 0.81500000, val_loss: 0.45140609
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 324.43361592, sen-loss: 50.64009649, dom-loss: 74.25869280, src-aux-loss: 104.25291061, tar-aux-loss: 95.28191543
Epoch: [5  ] train-acc: 0.81714286, dom-acc: 0.82526786, val-acc: 0.82500000, val_loss: 0.41371673
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 315.20802402, sen-loss: 44.69564456, dom-loss: 74.88912994, src-aux-loss: 102.11879456, tar-aux-loss: 93.50445545
Epoch: [6  ] train-acc: 0.85625000, dom-acc: 0.76500000, val-acc: 0.88250000, val_loss: 0.33724955
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 309.20162559, sen-loss: 40.28536841, dom-loss: 75.76454014, src-aux-loss: 100.10032415, tar-aux-loss: 93.05139166
Epoch: [7  ] train-acc: 0.87035714, dom-acc: 0.71937500, val-acc: 0.89000000, val_loss: 0.31275359
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 304.50759077, sen-loss: 37.77557021, dom-loss: 76.38220274, src-aux-loss: 99.13538784, tar-aux-loss: 91.21443051
Epoch: [8  ] train-acc: 0.87696429, dom-acc: 0.68375000, val-acc: 0.89750000, val_loss: 0.30340904
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 303.38719654, sen-loss: 36.41547334, dom-loss: 77.05460948, src-aux-loss: 97.67992085, tar-aux-loss: 92.23719209
Epoch: [9  ] train-acc: 0.88232143, dom-acc: 0.65910714, val-acc: 0.90000000, val_loss: 0.29668894
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 301.38599801, sen-loss: 35.08644389, dom-loss: 77.77949083, src-aux-loss: 96.85378057, tar-aux-loss: 91.66628271
Epoch: [10 ] train-acc: 0.88089286, dom-acc: 0.62339286, val-acc: 0.90000000, val_loss: 0.29199445
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 300.00773954, sen-loss: 34.31988057, dom-loss: 78.33898842, src-aux-loss: 96.03415179, tar-aux-loss: 91.31471884
Epoch: [11 ] train-acc: 0.88732143, dom-acc: 0.61339286, val-acc: 0.90000000, val_loss: 0.28840446
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 296.97925925, sen-loss: 33.68266350, dom-loss: 78.55578595, src-aux-loss: 95.18703705, tar-aux-loss: 89.55377287
Epoch: [12 ] train-acc: 0.88982143, dom-acc: 0.59187500, val-acc: 0.89750000, val_loss: 0.28589404
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 295.98550010, sen-loss: 33.12621268, dom-loss: 78.71330398, src-aux-loss: 94.50307637, tar-aux-loss: 89.64290696
Epoch: [13 ] train-acc: 0.89000000, dom-acc: 0.59312500, val-acc: 0.89250000, val_loss: 0.29073042
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 293.94790220, sen-loss: 32.36075141, dom-loss: 78.92794937, src-aux-loss: 93.79207194, tar-aux-loss: 88.86712956
Epoch: [14 ] train-acc: 0.89142857, dom-acc: 0.58232143, val-acc: 0.89000000, val_loss: 0.29211468
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 293.23995018, sen-loss: 31.98944585, dom-loss: 78.92917049, src-aux-loss: 93.35276020, tar-aux-loss: 88.96857440
Epoch: [15 ] train-acc: 0.89571429, dom-acc: 0.55330357, val-acc: 0.90250000, val_loss: 0.28083548
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 293.40043044, sen-loss: 31.44481520, dom-loss: 78.67596453, src-aux-loss: 92.77724195, tar-aux-loss: 90.50240898
Epoch: [16 ] train-acc: 0.89750000, dom-acc: 0.59062500, val-acc: 0.90000000, val_loss: 0.27852470
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 290.23986864, sen-loss: 31.00457755, dom-loss: 78.57165676, src-aux-loss: 91.96982825, tar-aux-loss: 88.69380820
Epoch: [17 ] train-acc: 0.89589286, dom-acc: 0.58116071, val-acc: 0.89250000, val_loss: 0.28261819
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 288.63613510, sen-loss: 30.66275534, dom-loss: 78.23233604, src-aux-loss: 91.71980220, tar-aux-loss: 88.02124119
Epoch: [18 ] train-acc: 0.89964286, dom-acc: 0.60714286, val-acc: 0.89750000, val_loss: 0.27433905
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 288.10530853, sen-loss: 30.35217293, dom-loss: 78.14739233, src-aux-loss: 90.83117187, tar-aux-loss: 88.77457082
Epoch: [19 ] train-acc: 0.89964286, dom-acc: 0.60785714, val-acc: 0.89750000, val_loss: 0.27244455
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 285.64743137, sen-loss: 29.86654139, dom-loss: 77.99711680, src-aux-loss: 90.41739398, tar-aux-loss: 87.36637908
Epoch: [20 ] train-acc: 0.90178571, dom-acc: 0.60723214, val-acc: 0.89500000, val_loss: 0.27122992
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 286.94102263, sen-loss: 29.52409831, dom-loss: 77.98206526, src-aux-loss: 90.00284612, tar-aux-loss: 89.43201303
Epoch: [21 ] train-acc: 0.89982143, dom-acc: 0.61366071, val-acc: 0.89500000, val_loss: 0.27548465
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 284.17614841, sen-loss: 29.22381336, dom-loss: 77.91686088, src-aux-loss: 89.59272683, tar-aux-loss: 87.44274896
Epoch: [22 ] train-acc: 0.90125000, dom-acc: 0.59267857, val-acc: 0.89250000, val_loss: 0.28373975
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 283.71900892, sen-loss: 29.12657198, dom-loss: 77.80749857, src-aux-loss: 89.16145420, tar-aux-loss: 87.62348491
Epoch: [23 ] train-acc: 0.90321429, dom-acc: 0.61017857, val-acc: 0.89500000, val_loss: 0.26919490
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 283.01535702, sen-loss: 28.76954708, dom-loss: 77.91011971, src-aux-loss: 88.64007485, tar-aux-loss: 87.69561535
Epoch: [24 ] train-acc: 0.90535714, dom-acc: 0.61571429, val-acc: 0.89500000, val_loss: 0.26555926
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 281.42886996, sen-loss: 28.54361024, dom-loss: 77.85794687, src-aux-loss: 88.22049290, tar-aux-loss: 86.80681902
Epoch: [25 ] train-acc: 0.90553571, dom-acc: 0.60232143, val-acc: 0.89500000, val_loss: 0.26592925
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 281.43211675, sen-loss: 28.20438199, dom-loss: 78.04458660, src-aux-loss: 87.73042315, tar-aux-loss: 87.45272619
Epoch: [26 ] train-acc: 0.90321429, dom-acc: 0.60366071, val-acc: 0.89750000, val_loss: 0.27080029
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 281.91121078, sen-loss: 28.11610512, dom-loss: 78.12285054, src-aux-loss: 87.46267426, tar-aux-loss: 88.20958179
Epoch: [27 ] train-acc: 0.90535714, dom-acc: 0.59482143, val-acc: 0.89500000, val_loss: 0.28040165
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 279.52938581, sen-loss: 27.64427097, dom-loss: 78.20593721, src-aux-loss: 87.04513580, tar-aux-loss: 86.63403964
Epoch: [28 ] train-acc: 0.90803571, dom-acc: 0.58062500, val-acc: 0.90000000, val_loss: 0.25910500
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 278.76074505, sen-loss: 27.50624806, dom-loss: 78.25782782, src-aux-loss: 86.72391647, tar-aux-loss: 86.27275258
Epoch: [29 ] train-acc: 0.91107143, dom-acc: 0.59419643, val-acc: 0.90000000, val_loss: 0.25813869
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 279.50780702, sen-loss: 27.32921545, dom-loss: 78.51552856, src-aux-loss: 86.18191040, tar-aux-loss: 87.48115140
Epoch: [30 ] train-acc: 0.90875000, dom-acc: 0.55732143, val-acc: 0.89750000, val_loss: 0.27432507
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 277.98515320, sen-loss: 26.96122495, dom-loss: 78.50528765, src-aux-loss: 85.87572086, tar-aux-loss: 86.64292085
Epoch: [31 ] train-acc: 0.91053571, dom-acc: 0.56133929, val-acc: 0.89250000, val_loss: 0.26236069
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 276.88206339, sen-loss: 26.68824761, dom-loss: 78.56446767, src-aux-loss: 85.34260494, tar-aux-loss: 86.28674299
Epoch: [32 ] train-acc: 0.90875000, dom-acc: 0.56419643, val-acc: 0.89750000, val_loss: 0.26800057
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 276.58335686, sen-loss: 26.31319309, dom-loss: 78.59985292, src-aux-loss: 85.12155408, tar-aux-loss: 86.54875600
Epoch: [33 ] train-acc: 0.90464286, dom-acc: 0.53892857, val-acc: 0.89000000, val_loss: 0.29334861
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 274.84413815, sen-loss: 26.51151729, dom-loss: 78.50529903, src-aux-loss: 84.61429650, tar-aux-loss: 85.21302521
Epoch: [34 ] train-acc: 0.91339286, dom-acc: 0.55848214, val-acc: 0.89500000, val_loss: 0.26086530
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_electronics_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.25813869
Testing accuracy: 0.86783333
./work/attentions/dvd_electronics_train_HATN.txt
./work/attentions/dvd_electronics_test_HATN.txt
loading data...
source domain:  dvd target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 13856
vocab-size:  80685
['best', 'good', 'excellent', 'great', 'funny', 'enjoyable', 'entertaining', 'nice', 'classic', 'awesome', 'fantastic', 'love', 'hilarious', 'loved', 'funniest', 'amazing', 'perfect', 'underrated', 'brilliant', 'sad', 'favorite', 'wonderful', 'interesting', 'superb', 'finest', 'easy', 'recommend', 'solid', 'beautifully', 'outstanding', 'fine', 'beautiful', 'real', 'memorable', 'enjoy', 'terrific', 'pleasant', 'fascinating', 'amusing', 'worthy', 'brilliantly', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'poor', 'disappointing', 'terrible', 'awful', 'disappointed', 'dull', 'wasted', 'better', 'annoying', 'waste', 'stupid', 'unfunny', 'forgettable', 'laughable', 'unwatchable', 'sucks', 'lousy', 'predictable', 'decent', 'worse', 'pathetic', 'poorly', 'uninspired', 'wrong', 'slow', 'sorry', 'contrived', 'unoriginal', 'ok', 'disgusting', 'sucked', 'crappy', 'pointless', 'mediocre', 'lame', 'overrated', 'biased', 'atrocious', 'depressing', 'ridiculous', 'horrendous', 'irritating', 'dumbest', 'defective', 'okay', 'disapointed', 'silly', 'disjointed', 'cheap', 'weak', 'stupidest', 'dissapointed', 'frustrating', 'ruined']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
5600 400 6000 17843 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.93089771, sen-loss: 77.39434594, dom-loss: 78.65411681, src-aux-loss: 129.67498010, tar-aux-loss: 119.20745552
Epoch: [1  ] train-acc: 0.64714286, dom-acc: 0.83330357, val-acc: 0.68500000, val_loss: 0.65848356
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 375.33241701, sen-loss: 71.85086924, dom-loss: 74.24440330, src-aux-loss: 120.54671443, tar-aux-loss: 108.69042915
Epoch: [2  ] train-acc: 0.76285714, dom-acc: 0.91053571, val-acc: 0.75750000, val_loss: 0.59814775
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 358.42973137, sen-loss: 65.50348890, dom-loss: 72.80766135, src-aux-loss: 116.00942808, tar-aux-loss: 104.10915101
Epoch: [3  ] train-acc: 0.77000000, dom-acc: 0.89741071, val-acc: 0.76750000, val_loss: 0.53050482
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 344.47114944, sen-loss: 57.92772076, dom-loss: 72.50211215, src-aux-loss: 112.25723046, tar-aux-loss: 101.78408587
Epoch: [4  ] train-acc: 0.82053571, dom-acc: 0.86678571, val-acc: 0.82750000, val_loss: 0.45217448
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.74849010, sen-loss: 50.52152458, dom-loss: 72.57648665, src-aux-loss: 108.73001897, tar-aux-loss: 100.92045605
Epoch: [5  ] train-acc: 0.83375000, dom-acc: 0.81714286, val-acc: 0.84250000, val_loss: 0.40450656
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 323.97473621, sen-loss: 44.81651789, dom-loss: 72.92474568, src-aux-loss: 107.07763004, tar-aux-loss: 99.15584409
Epoch: [6  ] train-acc: 0.86071429, dom-acc: 0.74642857, val-acc: 0.88750000, val_loss: 0.33728775
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 317.68061495, sen-loss: 40.30047500, dom-loss: 73.66491550, src-aux-loss: 104.96695918, tar-aux-loss: 98.74826384
Epoch: [7  ] train-acc: 0.87089286, dom-acc: 0.72339286, val-acc: 0.88750000, val_loss: 0.31203675
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 312.08762169, sen-loss: 37.82106151, dom-loss: 74.52664369, src-aux-loss: 103.98014110, tar-aux-loss: 95.75977457
Epoch: [8  ] train-acc: 0.87767857, dom-acc: 0.69339286, val-acc: 0.89750000, val_loss: 0.30291569
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 312.01570630, sen-loss: 36.55800004, dom-loss: 75.70864308, src-aux-loss: 102.34958446, tar-aux-loss: 97.39947873
Epoch: [9  ] train-acc: 0.87964286, dom-acc: 0.67785714, val-acc: 0.89500000, val_loss: 0.29355028
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 310.47954392, sen-loss: 35.23051804, dom-loss: 76.39760625, src-aux-loss: 101.81254327, tar-aux-loss: 97.03887707
Epoch: [10 ] train-acc: 0.88232143, dom-acc: 0.64848214, val-acc: 0.89250000, val_loss: 0.29097241
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 309.81535339, sen-loss: 34.63407654, dom-loss: 77.35263419, src-aux-loss: 101.15760773, tar-aux-loss: 96.67103326
Epoch: [11 ] train-acc: 0.88482143, dom-acc: 0.64910714, val-acc: 0.89750000, val_loss: 0.29439366
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 306.18075085, sen-loss: 33.92330557, dom-loss: 77.61205333, src-aux-loss: 100.11477542, tar-aux-loss: 94.53061509
Epoch: [12 ] train-acc: 0.88964286, dom-acc: 0.61866071, val-acc: 0.90000000, val_loss: 0.28244311
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 306.94436336, sen-loss: 33.31654654, dom-loss: 78.49431020, src-aux-loss: 99.02990836, tar-aux-loss: 96.10359782
Epoch: [13 ] train-acc: 0.89017857, dom-acc: 0.62348214, val-acc: 0.90250000, val_loss: 0.28340214
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 302.78145313, sen-loss: 32.63372055, dom-loss: 78.39475554, src-aux-loss: 98.41572052, tar-aux-loss: 93.33725721
Epoch: [14 ] train-acc: 0.89017857, dom-acc: 0.61973214, val-acc: 0.89500000, val_loss: 0.28619093
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 302.62999916, sen-loss: 32.09000510, dom-loss: 78.88611633, src-aux-loss: 97.86228037, tar-aux-loss: 93.79159808
Epoch: [15 ] train-acc: 0.89375000, dom-acc: 0.59937500, val-acc: 0.90000000, val_loss: 0.27692106
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 301.49853992, sen-loss: 31.56614822, dom-loss: 78.63820225, src-aux-loss: 97.04765588, tar-aux-loss: 94.24653208
Epoch: [16 ] train-acc: 0.89607143, dom-acc: 0.60821429, val-acc: 0.90750000, val_loss: 0.27827603
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 299.89542627, sen-loss: 31.25883572, dom-loss: 78.56259274, src-aux-loss: 96.59714377, tar-aux-loss: 93.47685432
Epoch: [17 ] train-acc: 0.89232143, dom-acc: 0.61294643, val-acc: 0.90000000, val_loss: 0.28371295
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 298.64207125, sen-loss: 30.84942740, dom-loss: 78.46542865, src-aux-loss: 95.93868214, tar-aux-loss: 93.38853347
Epoch: [18 ] train-acc: 0.89964286, dom-acc: 0.61785714, val-acc: 0.90000000, val_loss: 0.27221483
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 297.57366419, sen-loss: 30.51315624, dom-loss: 78.38073134, src-aux-loss: 95.31950521, tar-aux-loss: 93.36027145
Epoch: [19 ] train-acc: 0.90089286, dom-acc: 0.61901786, val-acc: 0.90000000, val_loss: 0.26981929
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 295.63699985, sen-loss: 30.14492402, dom-loss: 78.05907738, src-aux-loss: 95.00129646, tar-aux-loss: 92.43170375
Epoch: [20 ] train-acc: 0.90125000, dom-acc: 0.62732143, val-acc: 0.90000000, val_loss: 0.26885414
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 294.28138614, sen-loss: 29.81611884, dom-loss: 77.84274036, src-aux-loss: 94.66517514, tar-aux-loss: 91.95735174
Epoch: [21 ] train-acc: 0.89928571, dom-acc: 0.63678571, val-acc: 0.90500000, val_loss: 0.26907086
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 294.49911976, sen-loss: 29.52845704, dom-loss: 77.88893789, src-aux-loss: 94.19473296, tar-aux-loss: 92.88699287
Epoch: [22 ] train-acc: 0.89857143, dom-acc: 0.64071429, val-acc: 0.89750000, val_loss: 0.27843323
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 292.08770990, sen-loss: 29.38705541, dom-loss: 77.66967100, src-aux-loss: 93.66762221, tar-aux-loss: 91.36336118
Epoch: [23 ] train-acc: 0.90267857, dom-acc: 0.63732143, val-acc: 0.90000000, val_loss: 0.26640218
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 292.19708204, sen-loss: 28.98869118, dom-loss: 77.47290885, src-aux-loss: 93.06082195, tar-aux-loss: 92.67465985
Epoch: [24 ] train-acc: 0.90535714, dom-acc: 0.64303571, val-acc: 0.90000000, val_loss: 0.26312220
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 290.04500890, sen-loss: 28.69724499, dom-loss: 77.52162546, src-aux-loss: 92.74740797, tar-aux-loss: 91.07873046
Epoch: [25 ] train-acc: 0.90357143, dom-acc: 0.64500000, val-acc: 0.90250000, val_loss: 0.26297450
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 291.38215637, sen-loss: 28.40545938, dom-loss: 77.54069161, src-aux-loss: 92.29015577, tar-aux-loss: 93.14584875
Epoch: [26 ] train-acc: 0.90517857, dom-acc: 0.64303571, val-acc: 0.90500000, val_loss: 0.26537955
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 289.07990336, sen-loss: 28.26777629, dom-loss: 77.32616091, src-aux-loss: 91.71182311, tar-aux-loss: 91.77414346
Epoch: [27 ] train-acc: 0.90517857, dom-acc: 0.64276786, val-acc: 0.90000000, val_loss: 0.26741427
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 287.62664962, sen-loss: 27.88549680, dom-loss: 77.77077955, src-aux-loss: 91.40958369, tar-aux-loss: 90.56078893
Epoch: [28 ] train-acc: 0.90714286, dom-acc: 0.63571429, val-acc: 0.90500000, val_loss: 0.25918862
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 287.33587694, sen-loss: 27.77142082, dom-loss: 77.68606257, src-aux-loss: 90.79267341, tar-aux-loss: 91.08572012
Epoch: [29 ] train-acc: 0.90785714, dom-acc: 0.63375000, val-acc: 0.90500000, val_loss: 0.25751987
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 286.31011844, sen-loss: 27.53848621, dom-loss: 77.91341108, src-aux-loss: 90.33442879, tar-aux-loss: 90.52379483
Epoch: [30 ] train-acc: 0.90500000, dom-acc: 0.63196429, val-acc: 0.90000000, val_loss: 0.27045169
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.47416139, sen-loss: 27.12926979, dom-loss: 78.00141835, src-aux-loss: 89.93806267, tar-aux-loss: 92.40541095
Epoch: [31 ] train-acc: 0.91000000, dom-acc: 0.62526786, val-acc: 0.90750000, val_loss: 0.26081571
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 284.34710932, sen-loss: 26.91007945, dom-loss: 77.99491751, src-aux-loss: 89.52639151, tar-aux-loss: 89.91572088
Epoch: [32 ] train-acc: 0.90875000, dom-acc: 0.62258929, val-acc: 0.89750000, val_loss: 0.26820070
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 283.68636322, sen-loss: 26.54694524, dom-loss: 78.16037381, src-aux-loss: 89.06350166, tar-aux-loss: 89.91554320
Epoch: [33 ] train-acc: 0.90285714, dom-acc: 0.62223214, val-acc: 0.89000000, val_loss: 0.28637767
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 283.60528588, sen-loss: 26.70274544, dom-loss: 78.20431167, src-aux-loss: 88.69931418, tar-aux-loss: 89.99891466
Epoch: [34 ] train-acc: 0.91357143, dom-acc: 0.61294643, val-acc: 0.90750000, val_loss: 0.25714436
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 283.59394741, sen-loss: 26.40877004, dom-loss: 78.44000959, src-aux-loss: 88.21922541, tar-aux-loss: 90.52594048
Epoch: [35 ] train-acc: 0.91517857, dom-acc: 0.61017857, val-acc: 0.90750000, val_loss: 0.25422877
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 281.86989045, sen-loss: 26.15307830, dom-loss: 78.34214979, src-aux-loss: 87.78388023, tar-aux-loss: 89.59078079
Epoch: [36 ] train-acc: 0.91392857, dom-acc: 0.60830357, val-acc: 0.90500000, val_loss: 0.25484595
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 281.99053717, sen-loss: 25.71325485, dom-loss: 78.17594361, src-aux-loss: 87.28931922, tar-aux-loss: 90.81202084
Epoch: [37 ] train-acc: 0.91607143, dom-acc: 0.61776786, val-acc: 0.90500000, val_loss: 0.25563401
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 280.32455468, sen-loss: 25.54489501, dom-loss: 78.25489450, src-aux-loss: 86.84289533, tar-aux-loss: 89.68186921
Epoch: [38 ] train-acc: 0.91553571, dom-acc: 0.62196429, val-acc: 0.90750000, val_loss: 0.25536707
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 279.79148030, sen-loss: 25.37219518, dom-loss: 78.01941657, src-aux-loss: 86.47052479, tar-aux-loss: 89.92934412
Epoch: [39 ] train-acc: 0.91767857, dom-acc: 0.62571429, val-acc: 0.90250000, val_loss: 0.25788382
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 279.08830428, sen-loss: 25.14571606, dom-loss: 77.87836462, src-aux-loss: 85.85074878, tar-aux-loss: 90.21347529
Epoch: [40 ] train-acc: 0.91732143, dom-acc: 0.64196429, val-acc: 0.90000000, val_loss: 0.26557526
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_kitchen_HATN.ckpt
Best Epoch: [ 35] best val accuracy: 0.00000000 best val loss: 0.25422877
Testing accuracy: 0.87000000
./work/attentions/dvd_kitchen_train_HATN.txt
./work/attentions/dvd_kitchen_test_HATN.txt
loading data...
source domain:  dvd target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 30180
vocab-size:  91852
['best', 'good', 'excellent', 'great', 'funny', 'enjoyable', 'entertaining', 'classic', 'nice', 'fantastic', 'awesome', 'hilarious', 'love', 'loved', 'funniest', 'underrated', 'amazing', 'better', 'wonderful', 'sad', 'perfect', 'favorite', 'brilliant', 'interesting', 'superb', 'finest', 'solid', 'beautifully', 'easy', 'real', 'worthy', 'believable', 'recommend', 'beautiful', 'terrific', 'fine', 'amusing', 'brilliantly', 'enjoy', 'outstanding', 'fascinating', 'likeable', 'spectacular', 'memorable', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'poor', 'disappointing', 'terrible', 'awful', 'disappointed', 'dull', 'wasted', 'annoying', 'stupid', 'waste', 'unfunny', 'worse', 'sucks', 'laughable', 'lousy', 'unwatchable', 'forgettable', 'pathetic', 'wrong', 'uninspired', 'predictable', 'decent', 'ok', 'poorly', 'disgusting', 'contrived', 'slow', 'unoriginal', 'dissapointed', 'sorry', 'atrocious', 'crappy', 'pointless', 'mediocre', 'lame', 'sucked', 'overrated', 'biased', 'okay', 'depressing', 'silly', 'horrendous', 'dumbest', 'disapointed', 'ridiculous', 'disjointed', 'weak', 'stupidest', 'frustrating', 'wasting', 'ruined']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 17843 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 399.55225420, sen-loss: 77.48150361, dom-loss: 80.22259659, src-aux-loss: 127.50767094, tar-aux-loss: 114.34048402
Epoch: [1  ] train-acc: 0.64482143, dom-acc: 0.44294643, val-acc: 0.67250000, val_loss: 0.65974897
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 376.78355312, sen-loss: 71.90522403, dom-loss: 78.89289683, src-aux-loss: 119.63596952, tar-aux-loss: 106.34946167
Epoch: [2  ] train-acc: 0.75964286, dom-acc: 0.47026786, val-acc: 0.76750000, val_loss: 0.59830791
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.22252107, sen-loss: 65.42419016, dom-loss: 78.69901031, src-aux-loss: 114.75550103, tar-aux-loss: 101.34382105
Epoch: [3  ] train-acc: 0.77910714, dom-acc: 0.53178571, val-acc: 0.77000000, val_loss: 0.52748686
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 345.35253620, sen-loss: 57.81235319, dom-loss: 78.47503835, src-aux-loss: 110.72917116, tar-aux-loss: 98.33597267
Epoch: [4  ] train-acc: 0.82303571, dom-acc: 0.59267857, val-acc: 0.82250000, val_loss: 0.45100543
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 331.45895362, sen-loss: 50.69279182, dom-loss: 78.13232380, src-aux-loss: 106.77426589, tar-aux-loss: 95.85957241
Epoch: [5  ] train-acc: 0.83750000, dom-acc: 0.64982143, val-acc: 0.86250000, val_loss: 0.39919230
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 321.86487126, sen-loss: 45.19086355, dom-loss: 77.70450222, src-aux-loss: 104.60105437, tar-aux-loss: 94.36845064
Epoch: [6  ] train-acc: 0.85267857, dom-acc: 0.62303571, val-acc: 0.87500000, val_loss: 0.34366894
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 315.04346228, sen-loss: 41.10364224, dom-loss: 77.67459601, src-aux-loss: 102.46211666, tar-aux-loss: 93.80310601
Epoch: [7  ] train-acc: 0.86839286, dom-acc: 0.65526786, val-acc: 0.89250000, val_loss: 0.31982327
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 309.14037204, sen-loss: 38.42813601, dom-loss: 77.68630022, src-aux-loss: 101.67672187, tar-aux-loss: 91.34921420
Epoch: [8  ] train-acc: 0.87875000, dom-acc: 0.65026786, val-acc: 0.90000000, val_loss: 0.30703083
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 309.35184813, sen-loss: 37.02879533, dom-loss: 77.52376151, src-aux-loss: 100.08189112, tar-aux-loss: 94.71739990
Epoch: [9  ] train-acc: 0.87946429, dom-acc: 0.65580357, val-acc: 0.89750000, val_loss: 0.29781899
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 303.97522664, sen-loss: 35.50669286, dom-loss: 77.10790455, src-aux-loss: 99.38487166, tar-aux-loss: 91.97575611
Epoch: [10 ] train-acc: 0.88089286, dom-acc: 0.64803571, val-acc: 0.89750000, val_loss: 0.29277784
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 301.80143881, sen-loss: 34.73795550, dom-loss: 77.23374850, src-aux-loss: 98.64459687, tar-aux-loss: 91.18513691
Epoch: [11 ] train-acc: 0.88678571, dom-acc: 0.66901786, val-acc: 0.89750000, val_loss: 0.29138139
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 298.87566638, sen-loss: 34.07916476, dom-loss: 77.04893655, src-aux-loss: 97.83691156, tar-aux-loss: 89.91065413
Epoch: [12 ] train-acc: 0.88750000, dom-acc: 0.65071429, val-acc: 0.90750000, val_loss: 0.28415936
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 298.23984241, sen-loss: 33.45630183, dom-loss: 77.10096413, src-aux-loss: 97.02565438, tar-aux-loss: 90.65692246
Epoch: [13 ] train-acc: 0.88910714, dom-acc: 0.67142857, val-acc: 0.90250000, val_loss: 0.28496301
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 295.51857114, sen-loss: 32.76752651, dom-loss: 77.07174098, src-aux-loss: 96.37452668, tar-aux-loss: 89.30477649
Epoch: [14 ] train-acc: 0.89089286, dom-acc: 0.67446429, val-acc: 0.89500000, val_loss: 0.28766096
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 294.45051742, sen-loss: 32.27625886, dom-loss: 77.03331411, src-aux-loss: 95.92141974, tar-aux-loss: 89.21952480
Epoch: [15 ] train-acc: 0.89196429, dom-acc: 0.66321429, val-acc: 0.90250000, val_loss: 0.28080934
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 294.03478169, sen-loss: 31.64744274, dom-loss: 76.72637415, src-aux-loss: 95.30166042, tar-aux-loss: 90.35930502
Epoch: [16 ] train-acc: 0.89285714, dom-acc: 0.67071429, val-acc: 0.90250000, val_loss: 0.27990732
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 291.64050150, sen-loss: 31.37224029, dom-loss: 76.83119988, src-aux-loss: 94.83040416, tar-aux-loss: 88.60665762
Epoch: [17 ] train-acc: 0.89232143, dom-acc: 0.66982143, val-acc: 0.89000000, val_loss: 0.28789297
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 290.23478889, sen-loss: 30.91556332, dom-loss: 76.82222587, src-aux-loss: 94.12602335, tar-aux-loss: 88.37097621
Epoch: [18 ] train-acc: 0.89875000, dom-acc: 0.66919643, val-acc: 0.90000000, val_loss: 0.27350762
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 289.40266204, sen-loss: 30.56929782, dom-loss: 76.72384483, src-aux-loss: 93.44984037, tar-aux-loss: 88.65967840
Epoch: [19 ] train-acc: 0.90000000, dom-acc: 0.67178571, val-acc: 0.90500000, val_loss: 0.27078906
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 289.45376945, sen-loss: 30.19649024, dom-loss: 76.84950083, src-aux-loss: 93.09346658, tar-aux-loss: 89.31431198
Epoch: [20 ] train-acc: 0.90035714, dom-acc: 0.67044643, val-acc: 0.90250000, val_loss: 0.26976797
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 287.64700079, sen-loss: 29.83907036, dom-loss: 76.68232203, src-aux-loss: 92.66474420, tar-aux-loss: 88.46086502
Epoch: [21 ] train-acc: 0.90178571, dom-acc: 0.67500000, val-acc: 0.90250000, val_loss: 0.26844871
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 284.88898659, sen-loss: 29.58027914, dom-loss: 76.81722230, src-aux-loss: 92.23620528, tar-aux-loss: 86.25528085
Epoch: [22 ] train-acc: 0.89678571, dom-acc: 0.67875000, val-acc: 0.88750000, val_loss: 0.28611046
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 284.56428528, sen-loss: 29.38003111, dom-loss: 76.53676659, src-aux-loss: 91.65971971, tar-aux-loss: 86.98776782
Epoch: [23 ] train-acc: 0.90392857, dom-acc: 0.66919643, val-acc: 0.90250000, val_loss: 0.26591176
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 285.79604721, sen-loss: 29.06897005, dom-loss: 76.80182439, src-aux-loss: 91.09376562, tar-aux-loss: 88.83148628
Epoch: [24 ] train-acc: 0.90339286, dom-acc: 0.67401786, val-acc: 0.90000000, val_loss: 0.26384503
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 281.33744788, sen-loss: 28.84027748, dom-loss: 76.64161575, src-aux-loss: 90.63545054, tar-aux-loss: 85.22010303
Epoch: [25 ] train-acc: 0.90428571, dom-acc: 0.66973214, val-acc: 0.90250000, val_loss: 0.26259953
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 282.97243524, sen-loss: 28.48663192, dom-loss: 76.51771408, src-aux-loss: 90.24828273, tar-aux-loss: 87.71980530
Epoch: [26 ] train-acc: 0.90428571, dom-acc: 0.67267857, val-acc: 0.90250000, val_loss: 0.26736322
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 282.21812797, sen-loss: 28.34170971, dom-loss: 76.55985433, src-aux-loss: 89.62096757, tar-aux-loss: 87.69559675
Epoch: [27 ] train-acc: 0.90357143, dom-acc: 0.68187500, val-acc: 0.90250000, val_loss: 0.27377447
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 278.79600382, sen-loss: 27.85978833, dom-loss: 76.58452594, src-aux-loss: 89.50696152, tar-aux-loss: 84.84472877
Epoch: [28 ] train-acc: 0.90607143, dom-acc: 0.66250000, val-acc: 0.90750000, val_loss: 0.25920141
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 279.39016843, sen-loss: 27.84537551, dom-loss: 76.46775377, src-aux-loss: 88.86537856, tar-aux-loss: 86.21166235
Epoch: [29 ] train-acc: 0.90482143, dom-acc: 0.66196429, val-acc: 0.90750000, val_loss: 0.25812924
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 278.54758668, sen-loss: 27.64389217, dom-loss: 76.63870114, src-aux-loss: 88.40523314, tar-aux-loss: 85.85976154
Epoch: [30 ] train-acc: 0.90642857, dom-acc: 0.68000000, val-acc: 0.90250000, val_loss: 0.26947609
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 279.08343935, sen-loss: 27.19574450, dom-loss: 76.66312003, src-aux-loss: 87.93359584, tar-aux-loss: 87.29097933
Epoch: [31 ] train-acc: 0.91017857, dom-acc: 0.67616071, val-acc: 0.90000000, val_loss: 0.25800517
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 275.80752110, sen-loss: 27.04103285, dom-loss: 76.44668025, src-aux-loss: 87.61007535, tar-aux-loss: 84.70973402
Epoch: [32 ] train-acc: 0.90910714, dom-acc: 0.68232143, val-acc: 0.90000000, val_loss: 0.26798290
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 275.67134976, sen-loss: 26.62229481, dom-loss: 76.54972166, src-aux-loss: 87.10725230, tar-aux-loss: 85.39208150
Epoch: [33 ] train-acc: 0.90142857, dom-acc: 0.68455357, val-acc: 0.89000000, val_loss: 0.29060200
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 275.19691420, sen-loss: 26.69957115, dom-loss: 76.59115636, src-aux-loss: 86.77077091, tar-aux-loss: 85.13541555
Epoch: [34 ] train-acc: 0.91357143, dom-acc: 0.67196429, val-acc: 0.90250000, val_loss: 0.25700256
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 273.57368112, sen-loss: 26.32262220, dom-loss: 76.57473594, src-aux-loss: 86.23506296, tar-aux-loss: 84.44126052
Epoch: [35 ] train-acc: 0.91285714, dom-acc: 0.66803571, val-acc: 0.90250000, val_loss: 0.25447586
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 272.41888928, sen-loss: 26.08835775, dom-loss: 76.44216949, src-aux-loss: 85.82688153, tar-aux-loss: 84.06148100
Epoch: [36 ] train-acc: 0.91553571, dom-acc: 0.66991071, val-acc: 0.89750000, val_loss: 0.25721842
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 273.42083097, sen-loss: 25.76327254, dom-loss: 76.25987571, src-aux-loss: 85.30215913, tar-aux-loss: 86.09552515
Epoch: [37 ] train-acc: 0.91410714, dom-acc: 0.66598214, val-acc: 0.90250000, val_loss: 0.25301617
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 271.63145518, sen-loss: 25.62886810, dom-loss: 76.45442605, src-aux-loss: 84.67207682, tar-aux-loss: 84.87608427
Epoch: [38 ] train-acc: 0.91714286, dom-acc: 0.67116071, val-acc: 0.90250000, val_loss: 0.25792703
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 270.61078525, sen-loss: 25.42677694, dom-loss: 76.49396819, src-aux-loss: 84.14781463, tar-aux-loss: 84.54222518
Epoch: [39 ] train-acc: 0.91928571, dom-acc: 0.67214286, val-acc: 0.90750000, val_loss: 0.25450817
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 270.92418671, sen-loss: 25.18464252, dom-loss: 76.24007070, src-aux-loss: 83.57998973, tar-aux-loss: 85.91948384
Epoch: [40 ] train-acc: 0.91732143, dom-acc: 0.68142857, val-acc: 0.90000000, val_loss: 0.26538548
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 268.77902603, sen-loss: 24.91752319, dom-loss: 76.57837802, src-aux-loss: 83.22181726, tar-aux-loss: 84.06130862
Epoch: [41 ] train-acc: 0.92000000, dom-acc: 0.67553571, val-acc: 0.90750000, val_loss: 0.25440064
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 266.53681326, sen-loss: 24.77589231, dom-loss: 76.34275275, src-aux-loss: 82.96552229, tar-aux-loss: 82.45264661
Epoch: [42 ] train-acc: 0.92035714, dom-acc: 0.66410714, val-acc: 0.91000000, val_loss: 0.25194743
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 267.54163909, sen-loss: 24.58104020, dom-loss: 76.46933985, src-aux-loss: 82.23119712, tar-aux-loss: 84.26006317
Epoch: [43 ] train-acc: 0.92053571, dom-acc: 0.67857143, val-acc: 0.90250000, val_loss: 0.26579094
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 265.05101609, sen-loss: 24.35767671, dom-loss: 76.28625238, src-aux-loss: 81.88973713, tar-aux-loss: 82.51735026
Epoch: [44 ] train-acc: 0.92125000, dom-acc: 0.66294643, val-acc: 0.91000000, val_loss: 0.25240555
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 266.23167038, sen-loss: 24.16373481, dom-loss: 76.26235157, src-aux-loss: 81.14540082, tar-aux-loss: 84.66018319
Epoch: [45 ] train-acc: 0.92053571, dom-acc: 0.66705357, val-acc: 0.91250000, val_loss: 0.25308511
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 264.19126666, sen-loss: 23.86964632, dom-loss: 76.38223547, src-aux-loss: 80.63099664, tar-aux-loss: 83.30838931
Epoch: [46 ] train-acc: 0.92464286, dom-acc: 0.67151786, val-acc: 0.90500000, val_loss: 0.25880456
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 263.82620573, sen-loss: 23.49163128, dom-loss: 76.70981073, src-aux-loss: 80.20445561, tar-aux-loss: 83.42030853
Epoch: [47 ] train-acc: 0.92500000, dom-acc: 0.67196429, val-acc: 0.90750000, val_loss: 0.25967869
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_video_HATN.ckpt
Best Epoch: [ 42] best val accuracy: 0.00000000 best val loss: 0.25194743
Testing accuracy: 0.88966667
./work/attentions/dvd_video_train_HATN.txt
./work/attentions/dvd_video_test_HATN.txt
loading data...
source domain:  electronics target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 9750
vocab-size:  83050
['great', 'good', 'excellent', 'easy', 'best', 'works', 'perfect', 'happy', 'fantastic', 'awesome', 'satisfied', 'solid', 'outstanding', 'perfectly', 'worth', 'recommended', 'love', 'fine', 'durable', 'reliable', 'comfortable', 'amazing', 'pleased', 'quick', 'arrived', 'impressive', 'old', 'decent', 'low', 'greatest', 'recommend', 'useful', 'enough', 'clear', 'nice', 'cool', 'exactly', 'sound', 'loves', 'slick']
['returned', 'return', 'poor', 'worst', 'disappointed', 'frustrating', 'disappointing', 'useless', 'horrible', 'wrong', 'returning', 'stopped', 'awful', 'lasted', 'poorly', 'bad', 'unreliable', 'failed', 'cheap', 'unacceptable', 'terrible', 'broke', 'hard', 'overpriced', 'impossible', 'expensive', 'worthless', 'ridiculous', 'worked', 'quit', 'beware', 'misleading', 'sad', 'save', 'died', 'uncomfortable', 'difficult', 'short', 'defective', 'unhappy', 'waste', 'annoying', 'stay', 'lousy', 'unusable', 'wasted', 'slow', 'spent', 'lose', 'incompatible', 'goes', 'sucks', 'tried', 'crashed', 'false', 'average', 'cheaply', 'back', 'broken', 'ok', 'flaky', 'flawed', 'trying', 'mediocre']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
5600 400 6000 23009 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 408.05838084, sen-loss: 75.76250571, dom-loss: 78.86270911, src-aux-loss: 128.86518151, tar-aux-loss: 124.56798434
Epoch: [1  ] train-acc: 0.68892857, dom-acc: 0.82875000, val-acc: 0.69500000, val_loss: 0.63696307
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 377.53451586, sen-loss: 66.70145786, dom-loss: 74.14800996, src-aux-loss: 120.03224194, tar-aux-loss: 116.65280455
Epoch: [2  ] train-acc: 0.77428571, dom-acc: 0.82553571, val-acc: 0.78000000, val_loss: 0.54503512
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 354.04552674, sen-loss: 55.26848578, dom-loss: 72.20342100, src-aux-loss: 114.97934788, tar-aux-loss: 111.59427124
Epoch: [3  ] train-acc: 0.80750000, dom-acc: 0.62508929, val-acc: 0.83250000, val_loss: 0.42750120
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 340.58408189, sen-loss: 47.03471383, dom-loss: 72.11280257, src-aux-loss: 111.55981696, tar-aux-loss: 109.87674963
Epoch: [4  ] train-acc: 0.83625000, dom-acc: 0.60125000, val-acc: 0.86500000, val_loss: 0.36999372
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.90615559, sen-loss: 42.54367273, dom-loss: 72.69248831, src-aux-loss: 108.97410840, tar-aux-loss: 108.69588578
Epoch: [5  ] train-acc: 0.85339286, dom-acc: 0.58830357, val-acc: 0.86500000, val_loss: 0.33795831
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 328.25742245, sen-loss: 39.44798344, dom-loss: 73.63288176, src-aux-loss: 106.92495167, tar-aux-loss: 108.25160587
Epoch: [6  ] train-acc: 0.86982143, dom-acc: 0.57517857, val-acc: 0.89000000, val_loss: 0.30729631
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 323.24985313, sen-loss: 37.00972876, dom-loss: 74.49998599, src-aux-loss: 105.79908329, tar-aux-loss: 105.94105560
Epoch: [7  ] train-acc: 0.87714286, dom-acc: 0.54178571, val-acc: 0.89250000, val_loss: 0.28785747
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 320.22347236, sen-loss: 35.13818495, dom-loss: 75.54873830, src-aux-loss: 104.29421121, tar-aux-loss: 105.24233967
Epoch: [8  ] train-acc: 0.88464286, dom-acc: 0.49616071, val-acc: 0.88750000, val_loss: 0.27578354
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 318.04493999, sen-loss: 33.77336270, dom-loss: 76.66148967, src-aux-loss: 102.95668668, tar-aux-loss: 104.65339923
Epoch: [9  ] train-acc: 0.88821429, dom-acc: 0.48455357, val-acc: 0.90250000, val_loss: 0.25643057
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 316.79072094, sen-loss: 32.38257328, dom-loss: 77.55190772, src-aux-loss: 102.04424804, tar-aux-loss: 104.81199133
Epoch: [10 ] train-acc: 0.88678571, dom-acc: 0.45669643, val-acc: 0.91250000, val_loss: 0.24619363
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 314.14278483, sen-loss: 31.46929400, dom-loss: 78.16185880, src-aux-loss: 101.28337169, tar-aux-loss: 103.22826022
Epoch: [11 ] train-acc: 0.89892857, dom-acc: 0.42035714, val-acc: 0.91500000, val_loss: 0.23885411
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 314.26583481, sen-loss: 30.84237537, dom-loss: 78.78532076, src-aux-loss: 100.64684027, tar-aux-loss: 103.99129760
Epoch: [12 ] train-acc: 0.88482143, dom-acc: 0.41919643, val-acc: 0.91000000, val_loss: 0.23880301
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 310.99917388, sen-loss: 29.86537153, dom-loss: 79.00242412, src-aux-loss: 99.81106472, tar-aux-loss: 102.32031256
Epoch: [13 ] train-acc: 0.90464286, dom-acc: 0.39589286, val-acc: 0.93000000, val_loss: 0.22829908
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 311.46248007, sen-loss: 29.41663048, dom-loss: 79.12851202, src-aux-loss: 99.13572145, tar-aux-loss: 103.78161728
Epoch: [14 ] train-acc: 0.90857143, dom-acc: 0.40830357, val-acc: 0.93000000, val_loss: 0.22334859
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 308.52383208, sen-loss: 28.74495975, dom-loss: 79.26622522, src-aux-loss: 98.44486946, tar-aux-loss: 102.06777835
Epoch: [15 ] train-acc: 0.91142857, dom-acc: 0.42580357, val-acc: 0.93750000, val_loss: 0.21973144
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 307.31904602, sen-loss: 28.21802913, dom-loss: 79.22713268, src-aux-loss: 98.08047891, tar-aux-loss: 101.79340518
Epoch: [16 ] train-acc: 0.91214286, dom-acc: 0.43312500, val-acc: 0.93750000, val_loss: 0.21903478
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 306.83817697, sen-loss: 27.80458671, dom-loss: 79.04456562, src-aux-loss: 97.47855276, tar-aux-loss: 102.51047164
Epoch: [17 ] train-acc: 0.91232143, dom-acc: 0.44348214, val-acc: 0.93500000, val_loss: 0.21835200
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 305.04227138, sen-loss: 27.29674845, dom-loss: 78.93954855, src-aux-loss: 96.85803008, tar-aux-loss: 101.94794279
Epoch: [18 ] train-acc: 0.91535714, dom-acc: 0.45535714, val-acc: 0.92750000, val_loss: 0.21104994
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 301.83515477, sen-loss: 26.81729091, dom-loss: 78.59331685, src-aux-loss: 96.20625001, tar-aux-loss: 100.21829718
Epoch: [19 ] train-acc: 0.91750000, dom-acc: 0.47151786, val-acc: 0.92500000, val_loss: 0.20851435
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 302.42364526, sen-loss: 26.38229841, dom-loss: 78.44890648, src-aux-loss: 96.03039777, tar-aux-loss: 101.56204265
Epoch: [20 ] train-acc: 0.91232143, dom-acc: 0.50732143, val-acc: 0.92000000, val_loss: 0.20818208
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 300.27447772, sen-loss: 26.04032579, dom-loss: 78.01032329, src-aux-loss: 95.53255469, tar-aux-loss: 100.69127208
Epoch: [21 ] train-acc: 0.91625000, dom-acc: 0.50303571, val-acc: 0.92000000, val_loss: 0.20567377
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 300.07005644, sen-loss: 25.68851198, dom-loss: 77.80383569, src-aux-loss: 94.94442749, tar-aux-loss: 101.63327891
Epoch: [22 ] train-acc: 0.92142857, dom-acc: 0.52446429, val-acc: 0.93000000, val_loss: 0.20458253
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 297.14636350, sen-loss: 25.40869034, dom-loss: 77.41784769, src-aux-loss: 94.51501513, tar-aux-loss: 99.80481017
Epoch: [23 ] train-acc: 0.91875000, dom-acc: 0.53919643, val-acc: 0.92250000, val_loss: 0.20327768
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 297.27079844, sen-loss: 25.14227618, dom-loss: 77.30171669, src-aux-loss: 94.11987090, tar-aux-loss: 100.70693302
Epoch: [24 ] train-acc: 0.92357143, dom-acc: 0.52089286, val-acc: 0.92500000, val_loss: 0.20285331
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 294.91143703, sen-loss: 24.81681806, dom-loss: 77.16036260, src-aux-loss: 93.67193234, tar-aux-loss: 99.26232320
Epoch: [25 ] train-acc: 0.92357143, dom-acc: 0.55133929, val-acc: 0.93750000, val_loss: 0.20269096
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.99658179, sen-loss: 24.35520758, dom-loss: 77.01605684, src-aux-loss: 93.47188401, tar-aux-loss: 101.15343356
Epoch: [26 ] train-acc: 0.92125000, dom-acc: 0.53625000, val-acc: 0.94250000, val_loss: 0.20919904
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 293.53284192, sen-loss: 24.12028043, dom-loss: 76.89606798, src-aux-loss: 92.99864650, tar-aux-loss: 99.51784647
Epoch: [27 ] train-acc: 0.92607143, dom-acc: 0.56812500, val-acc: 0.92500000, val_loss: 0.19943085
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 293.85675406, sen-loss: 23.82351112, dom-loss: 76.93356234, src-aux-loss: 92.60547453, tar-aux-loss: 100.49420571
Epoch: [28 ] train-acc: 0.92875000, dom-acc: 0.55357143, val-acc: 0.93500000, val_loss: 0.19945467
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 292.62415576, sen-loss: 23.51023139, dom-loss: 76.78018177, src-aux-loss: 92.25941044, tar-aux-loss: 100.07432973
Epoch: [29 ] train-acc: 0.92875000, dom-acc: 0.54303571, val-acc: 0.92250000, val_loss: 0.19900343
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.64322925, sen-loss: 23.15028936, dom-loss: 76.88576031, src-aux-loss: 91.78841579, tar-aux-loss: 98.81876647
Epoch: [30 ] train-acc: 0.93053571, dom-acc: 0.55187500, val-acc: 0.93500000, val_loss: 0.19940197
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 291.75849462, sen-loss: 22.88567834, dom-loss: 76.97198868, src-aux-loss: 91.36171389, tar-aux-loss: 100.53911507
Epoch: [31 ] train-acc: 0.92892857, dom-acc: 0.51598214, val-acc: 0.93250000, val_loss: 0.20118645
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 289.70281386, sen-loss: 22.58796284, dom-loss: 77.13428867, src-aux-loss: 91.17161417, tar-aux-loss: 98.80894864
Epoch: [32 ] train-acc: 0.93178571, dom-acc: 0.51830357, val-acc: 0.92750000, val_loss: 0.19718689
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 289.52265406, sen-loss: 22.33992372, dom-loss: 77.35146159, src-aux-loss: 90.65819806, tar-aux-loss: 99.17307210
Epoch: [33 ] train-acc: 0.93232143, dom-acc: 0.52767857, val-acc: 0.93250000, val_loss: 0.19866140
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 288.47403455, sen-loss: 22.04618292, dom-loss: 77.45626694, src-aux-loss: 90.06066805, tar-aux-loss: 98.91091764
Epoch: [34 ] train-acc: 0.93339286, dom-acc: 0.49178571, val-acc: 0.93000000, val_loss: 0.19809686
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 288.90742803, sen-loss: 21.76764603, dom-loss: 77.65203953, src-aux-loss: 89.89411539, tar-aux-loss: 99.59362674
Epoch: [35 ] train-acc: 0.93535714, dom-acc: 0.49000000, val-acc: 0.93750000, val_loss: 0.19708751
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 287.86329055, sen-loss: 21.61002570, dom-loss: 77.80352783, src-aux-loss: 89.33353436, tar-aux-loss: 99.11620218
Epoch: [36 ] train-acc: 0.93607143, dom-acc: 0.46419643, val-acc: 0.93000000, val_loss: 0.19722280
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 287.00557661, sen-loss: 21.23768897, dom-loss: 77.92621601, src-aux-loss: 88.88106167, tar-aux-loss: 98.96061021
Epoch: [37 ] train-acc: 0.93625000, dom-acc: 0.46892857, val-acc: 0.93000000, val_loss: 0.19787282
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 287.33506370, sen-loss: 21.01566409, dom-loss: 78.15276694, src-aux-loss: 88.67814171, tar-aux-loss: 99.48849028
Epoch: [38 ] train-acc: 0.93482143, dom-acc: 0.45276786, val-acc: 0.93000000, val_loss: 0.20206550
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 284.27645898, sen-loss: 20.88734504, dom-loss: 78.31630063, src-aux-loss: 88.02367443, tar-aux-loss: 97.04913843
Epoch: [39 ] train-acc: 0.93892857, dom-acc: 0.43473214, val-acc: 0.93250000, val_loss: 0.19847092
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 285.29147911, sen-loss: 20.44699319, dom-loss: 78.29583597, src-aux-loss: 87.52314281, tar-aux-loss: 99.02550781
Epoch: [40 ] train-acc: 0.94053571, dom-acc: 0.46785714, val-acc: 0.92750000, val_loss: 0.19801924
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_books_HATN.ckpt
Best Epoch: [ 35] best val accuracy: 0.00000000 best val loss: 0.19708751
Testing accuracy: 0.83616667
./work/attentions/electronics_books_train_HATN.txt
./work/attentions/electronics_books_test_HATN.txt
loading data...
source domain:  electronics target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 11843
vocab-size:  85442
['great', 'good', 'easy', 'excellent', 'best', 'works', 'perfect', 'fantastic', 'solid', 'awesome', 'outstanding', 'satisfied', 'perfectly', 'happy', 'fine', 'reliable', 'worth', 'highly', 'comfortable', 'durable', 'love', 'old', 'pleased', 'amazing', 'inexpensive', 'decent', 'impressive', 'quick', 'sturdy', 'nice', 'high', 'simple', 'recommend', 'enough', 'awsome', 'lightweight', 'arrived', 'greatest', 'useful', 'easily', 'cool', 'reasonable', 'impressed', 'dependable', 'expected', 'effective', 'loves', 'clear', 'slick', 'well', 'ultimate']
['returned', 'return', 'poor', 'worst', 'disappointing', 'frustrating', 'disappointed', 'horrible', 'useless', 'stopped', 'bad', 'returning', 'terrible', 'failed', 'poorly', 'awful', 'unreliable', 'broke', 'wrong', 'cheap', 'worthless', 'lasted', 'unacceptable', 'hard', 'overpriced', 'expensive', 'beware', 'quit', 'back', 'ridiculous', 'misleading', 'low', 'sad', 'save', 'defective', 'unhappy', 'waste', 'died', 'uncomfortable', 'tried', 'short', 'lousy', 'impossible', 'slow', 'difficult', 'unusable', 'wasted', 'weak', 'lose', 'broken', 'mediocre', 'scratched', 'crashed', 'fooled', 'false', 'average', 'cheaply', 'unable', 'worked', 'dead', 'ok', 'flaky', 'flawed', 'trying', 'true', 'annoying']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
5600 400 6000 23009 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 396.94515371, sen-loss: 75.65796244, dom-loss: 78.85065573, src-aux-loss: 125.77492821, tar-aux-loss: 116.66160780
Epoch: [1  ] train-acc: 0.68839286, dom-acc: 0.76919643, val-acc: 0.70000000, val_loss: 0.63563895
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 366.38764477, sen-loss: 66.68135101, dom-loss: 74.85051125, src-aux-loss: 117.31006616, tar-aux-loss: 107.54571694
Epoch: [2  ] train-acc: 0.77267857, dom-acc: 0.77116071, val-acc: 0.79000000, val_loss: 0.54556549
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 344.61949921, sen-loss: 55.50059971, dom-loss: 73.33286273, src-aux-loss: 112.69876999, tar-aux-loss: 103.08726424
Epoch: [3  ] train-acc: 0.80642857, dom-acc: 0.61107143, val-acc: 0.83000000, val_loss: 0.42885411
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 332.13041496, sen-loss: 47.22411489, dom-loss: 73.11273926, src-aux-loss: 109.26783204, tar-aux-loss: 102.52572757
Epoch: [4  ] train-acc: 0.83517857, dom-acc: 0.58366071, val-acc: 0.87250000, val_loss: 0.37073365
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 322.76300573, sen-loss: 42.60729592, dom-loss: 73.35171962, src-aux-loss: 106.56647778, tar-aux-loss: 100.23751259
Epoch: [5  ] train-acc: 0.85464286, dom-acc: 0.56044643, val-acc: 0.87500000, val_loss: 0.34095961
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 317.27547050, sen-loss: 39.34795074, dom-loss: 74.01559299, src-aux-loss: 104.71502739, tar-aux-loss: 99.19689858
Epoch: [6  ] train-acc: 0.86964286, dom-acc: 0.55705357, val-acc: 0.89500000, val_loss: 0.30492502
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 313.36432147, sen-loss: 36.81149019, dom-loss: 74.66111690, src-aux-loss: 103.41610509, tar-aux-loss: 98.47560906
Epoch: [7  ] train-acc: 0.87821429, dom-acc: 0.53687500, val-acc: 0.89500000, val_loss: 0.28419665
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 309.30136824, sen-loss: 34.86091623, dom-loss: 75.21565080, src-aux-loss: 102.16603100, tar-aux-loss: 97.05877030
Epoch: [8  ] train-acc: 0.88946429, dom-acc: 0.50598214, val-acc: 0.89750000, val_loss: 0.27182010
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 307.24511719, sen-loss: 33.77229460, dom-loss: 75.93504554, src-aux-loss: 100.81356740, tar-aux-loss: 96.72420931
Epoch: [9  ] train-acc: 0.88785714, dom-acc: 0.50187500, val-acc: 0.91500000, val_loss: 0.25359040
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 304.89002442, sen-loss: 32.31167029, dom-loss: 76.90060771, src-aux-loss: 99.85850304, tar-aux-loss: 95.81924385
Epoch: [10 ] train-acc: 0.88535714, dom-acc: 0.48687500, val-acc: 0.92000000, val_loss: 0.24586067
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 303.63438845, sen-loss: 31.39096674, dom-loss: 77.13877517, src-aux-loss: 99.36202723, tar-aux-loss: 95.74262112
Epoch: [11 ] train-acc: 0.89982143, dom-acc: 0.45580357, val-acc: 0.92250000, val_loss: 0.23493704
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 303.72566128, sen-loss: 30.74482399, dom-loss: 77.88341236, src-aux-loss: 98.62110090, tar-aux-loss: 96.47632396
Epoch: [12 ] train-acc: 0.88428571, dom-acc: 0.46312500, val-acc: 0.91000000, val_loss: 0.24047226
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 301.05261874, sen-loss: 29.82455833, dom-loss: 77.90491021, src-aux-loss: 97.84125602, tar-aux-loss: 95.48189330
Epoch: [13 ] train-acc: 0.90678571, dom-acc: 0.42705357, val-acc: 0.93000000, val_loss: 0.22802886
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 299.97935891, sen-loss: 29.37188746, dom-loss: 78.23941135, src-aux-loss: 97.20934063, tar-aux-loss: 95.15872020
Epoch: [14 ] train-acc: 0.90821429, dom-acc: 0.43687500, val-acc: 0.93000000, val_loss: 0.21942222
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 297.87553453, sen-loss: 28.70032012, dom-loss: 78.30762726, src-aux-loss: 96.67165780, tar-aux-loss: 94.19593138
Epoch: [15 ] train-acc: 0.90964286, dom-acc: 0.43026786, val-acc: 0.93500000, val_loss: 0.21734811
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 297.32083893, sen-loss: 28.21819960, dom-loss: 78.34863335, src-aux-loss: 96.09965432, tar-aux-loss: 94.65435231
Epoch: [16 ] train-acc: 0.91053571, dom-acc: 0.42794643, val-acc: 0.94000000, val_loss: 0.21713807
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 295.11420679, sen-loss: 27.77565002, dom-loss: 78.15971929, src-aux-loss: 95.65884918, tar-aux-loss: 93.51998794
Epoch: [17 ] train-acc: 0.91303571, dom-acc: 0.43133929, val-acc: 0.93500000, val_loss: 0.21625903
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 295.27244282, sen-loss: 27.26576459, dom-loss: 78.19100738, src-aux-loss: 95.19878960, tar-aux-loss: 94.61688197
Epoch: [18 ] train-acc: 0.91535714, dom-acc: 0.45321429, val-acc: 0.93250000, val_loss: 0.20750740
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 292.53247905, sen-loss: 26.80486114, dom-loss: 77.80334997, src-aux-loss: 94.66376287, tar-aux-loss: 93.26050490
Epoch: [19 ] train-acc: 0.91660714, dom-acc: 0.45428571, val-acc: 0.93250000, val_loss: 0.20536590
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 292.73585200, sen-loss: 26.38989234, dom-loss: 77.97949088, src-aux-loss: 94.14234036, tar-aux-loss: 94.22413081
Epoch: [20 ] train-acc: 0.91678571, dom-acc: 0.45660714, val-acc: 0.92500000, val_loss: 0.20438001
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 290.97214866, sen-loss: 26.05536850, dom-loss: 77.67057025, src-aux-loss: 93.71906143, tar-aux-loss: 93.52714866
Epoch: [21 ] train-acc: 0.91964286, dom-acc: 0.46491071, val-acc: 0.92750000, val_loss: 0.20335685
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 290.37193656, sen-loss: 25.67930014, dom-loss: 77.69503659, src-aux-loss: 93.20500135, tar-aux-loss: 93.79259920
Epoch: [22 ] train-acc: 0.92250000, dom-acc: 0.47973214, val-acc: 0.93000000, val_loss: 0.20179626
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 287.61000752, sen-loss: 25.39407498, dom-loss: 77.55511254, src-aux-loss: 92.58974671, tar-aux-loss: 92.07107335
Epoch: [23 ] train-acc: 0.91982143, dom-acc: 0.47660714, val-acc: 0.92500000, val_loss: 0.20185162
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 287.60634470, sen-loss: 25.25650665, dom-loss: 77.29738408, src-aux-loss: 92.39462113, tar-aux-loss: 92.65783280
Epoch: [24 ] train-acc: 0.92321429, dom-acc: 0.46455357, val-acc: 0.92500000, val_loss: 0.20046711
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 287.35235357, sen-loss: 24.77702333, dom-loss: 77.44736284, src-aux-loss: 92.03517932, tar-aux-loss: 93.09278810
Epoch: [25 ] train-acc: 0.92696429, dom-acc: 0.47330357, val-acc: 0.93500000, val_loss: 0.20045008
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 285.94450974, sen-loss: 24.35239253, dom-loss: 77.37854058, src-aux-loss: 91.83000207, tar-aux-loss: 92.38357359
Epoch: [26 ] train-acc: 0.92232143, dom-acc: 0.46008929, val-acc: 0.93750000, val_loss: 0.20561278
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 286.47404146, sen-loss: 24.09691516, dom-loss: 77.46260744, src-aux-loss: 91.54369718, tar-aux-loss: 93.37082124
Epoch: [27 ] train-acc: 0.92732143, dom-acc: 0.47946429, val-acc: 0.93500000, val_loss: 0.19967306
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 284.37081981, sen-loss: 23.81412973, dom-loss: 77.39659202, src-aux-loss: 90.73014545, tar-aux-loss: 92.42995268
Epoch: [28 ] train-acc: 0.92767857, dom-acc: 0.47133929, val-acc: 0.92750000, val_loss: 0.19744256
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 283.90523934, sen-loss: 23.47385926, dom-loss: 77.43791306, src-aux-loss: 90.44703490, tar-aux-loss: 92.54643059
Epoch: [29 ] train-acc: 0.92875000, dom-acc: 0.47687500, val-acc: 0.93000000, val_loss: 0.19819996
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 282.26172543, sen-loss: 23.23859591, dom-loss: 77.61114734, src-aux-loss: 90.11366367, tar-aux-loss: 91.29831851
Epoch: [30 ] train-acc: 0.92875000, dom-acc: 0.46339286, val-acc: 0.93500000, val_loss: 0.19996764
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 283.55733705, sen-loss: 22.91108201, dom-loss: 77.61322087, src-aux-loss: 89.76210189, tar-aux-loss: 93.27093112
Epoch: [31 ] train-acc: 0.92767857, dom-acc: 0.44687500, val-acc: 0.93250000, val_loss: 0.20558129
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 282.18152475, sen-loss: 22.60956568, dom-loss: 77.83735430, src-aux-loss: 89.44553912, tar-aux-loss: 92.28906506
Epoch: [32 ] train-acc: 0.93267857, dom-acc: 0.45187500, val-acc: 0.93500000, val_loss: 0.19869995
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 280.62476230, sen-loss: 22.34484481, dom-loss: 77.84502381, src-aux-loss: 88.99587703, tar-aux-loss: 91.43901694
Epoch: [33 ] train-acc: 0.93392857, dom-acc: 0.46053571, val-acc: 0.93750000, val_loss: 0.19669756
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 279.98114157, sen-loss: 21.96220819, dom-loss: 77.95618027, src-aux-loss: 88.57268631, tar-aux-loss: 91.49006736
Epoch: [34 ] train-acc: 0.93464286, dom-acc: 0.45008929, val-acc: 0.93500000, val_loss: 0.19606799
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 281.03816605, sen-loss: 21.71262524, dom-loss: 78.07690525, src-aux-loss: 88.26608151, tar-aux-loss: 92.98255366
Epoch: [35 ] train-acc: 0.93446429, dom-acc: 0.45348214, val-acc: 0.93250000, val_loss: 0.19669713
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 278.89505553, sen-loss: 21.59034524, dom-loss: 78.08123231, src-aux-loss: 87.73480916, tar-aux-loss: 91.48867017
Epoch: [36 ] train-acc: 0.93750000, dom-acc: 0.44366071, val-acc: 0.93250000, val_loss: 0.19634637
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 278.97086215, sen-loss: 21.19371232, dom-loss: 78.19029242, src-aux-loss: 87.44720209, tar-aux-loss: 92.13965690
Epoch: [37 ] train-acc: 0.93785714, dom-acc: 0.45473214, val-acc: 0.93750000, val_loss: 0.19725688
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 277.35121012, sen-loss: 20.86766074, dom-loss: 78.06286049, src-aux-loss: 87.22531199, tar-aux-loss: 91.19537747
Epoch: [38 ] train-acc: 0.93821429, dom-acc: 0.43464286, val-acc: 0.93500000, val_loss: 0.19985938
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 276.30880928, sen-loss: 20.74608472, dom-loss: 78.03862286, src-aux-loss: 86.77205181, tar-aux-loss: 90.75204951
Epoch: [39 ] train-acc: 0.93892857, dom-acc: 0.44696429, val-acc: 0.93500000, val_loss: 0.19687772
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_dvd_HATN.ckpt
Best Epoch: [ 34] best val accuracy: 0.00000000 best val loss: 0.19606799
Testing accuracy: 0.83866667
./work/attentions/electronics_dvd_train_HATN.txt
./work/attentions/electronics_dvd_test_HATN.txt
loading data...
source domain:  electronics target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 13856
vocab-size:  49470
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'happy', 'solid', 'fantastic', 'awesome', 'outstanding', 'perfectly', 'satisfied', 'fine', 'recommended', 'worth', 'love', 'reliable', 'inexpensive', 'comfortable', 'durable', 'quick', 'low', 'pleased', 'cool', 'impressive', 'old', 'decent', 'nice', 'greatest', 'amazing', 'reasonable', 'recommend', 'superb', 'impressed', 'quickly', 'much', 'high', 'slick', 'arrived', 'useful', 'loves', 'enough', 'awsome', 'effective', 'clear', 'lightweight', 'well']
['returned', 'poor', 'return', 'worst', 'disappointed', 'disappointing', 'useless', 'frustrating', 'horrible', 'stopped', 'bad', 'returning', 'terrible', 'poorly', 'wrong', 'cheap', 'failed', 'broke', 'awful', 'unreliable', 'worthless', 'unacceptable', 'hard', 'lasted', 'overpriced', 'beware', 'quit', 'died', 'ridiculous', 'misleading', 'sad', 'defective', 'impossible', 'unusable', 'waste', 'difficult', 'save', 'uncomfortable', 'wasted', 'unhappy', 'weak', 'ok', 'stay', 'lousy', 'slow', 'lose', 'trying', 'goes', 'sucks', 'tried', 'went', 'false', 'average', 'cheaply', 'dead', 'broken', 'flaky', 'flawed', 'true', 'mediocre', 'annoying']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
5600 400 6000 23009 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 391.87669468, sen-loss: 75.69529778, dom-loss: 79.19028258, src-aux-loss: 128.19124293, tar-aux-loss: 108.79987234
Epoch: [1  ] train-acc: 0.69267857, dom-acc: 0.71366071, val-acc: 0.71000000, val_loss: 0.63435906
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 363.62479258, sen-loss: 66.65589100, dom-loss: 76.12388581, src-aux-loss: 119.33798963, tar-aux-loss: 101.50702709
Epoch: [2  ] train-acc: 0.77642857, dom-acc: 0.78910714, val-acc: 0.79500000, val_loss: 0.54406416
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 341.77174854, sen-loss: 55.69805360, dom-loss: 75.24532700, src-aux-loss: 114.13820517, tar-aux-loss: 96.69016463
Epoch: [3  ] train-acc: 0.81035714, dom-acc: 0.77473214, val-acc: 0.84750000, val_loss: 0.42645520
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 325.54322505, sen-loss: 47.09582630, dom-loss: 74.82967424, src-aux-loss: 109.83001316, tar-aux-loss: 93.78770953
Epoch: [4  ] train-acc: 0.83625000, dom-acc: 0.73901786, val-acc: 0.86750000, val_loss: 0.36880755
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 317.29611325, sen-loss: 42.65518427, dom-loss: 74.71632862, src-aux-loss: 107.33569562, tar-aux-loss: 92.58890682
Epoch: [5  ] train-acc: 0.85160714, dom-acc: 0.74375000, val-acc: 0.86500000, val_loss: 0.34551945
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 310.84588909, sen-loss: 39.57887432, dom-loss: 74.57534778, src-aux-loss: 105.24499303, tar-aux-loss: 91.44667536
Epoch: [6  ] train-acc: 0.86589286, dom-acc: 0.73000000, val-acc: 0.89500000, val_loss: 0.30358177
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 307.18085575, sen-loss: 37.15832669, dom-loss: 74.68288213, src-aux-loss: 103.77021509, tar-aux-loss: 91.56943363
Epoch: [7  ] train-acc: 0.87357143, dom-acc: 0.73526786, val-acc: 0.89750000, val_loss: 0.28375483
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 302.04825544, sen-loss: 35.20167093, dom-loss: 74.79917586, src-aux-loss: 102.94040799, tar-aux-loss: 89.10700154
Epoch: [8  ] train-acc: 0.88339286, dom-acc: 0.73785714, val-acc: 0.89750000, val_loss: 0.27253491
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 298.81547165, sen-loss: 34.05241609, dom-loss: 75.06287837, src-aux-loss: 101.27440774, tar-aux-loss: 88.42576796
Epoch: [9  ] train-acc: 0.88107143, dom-acc: 0.71607143, val-acc: 0.91250000, val_loss: 0.25451279
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 297.38563728, sen-loss: 32.65783118, dom-loss: 75.03625101, src-aux-loss: 100.09655672, tar-aux-loss: 89.59499753
Epoch: [10 ] train-acc: 0.88535714, dom-acc: 0.70616071, val-acc: 0.91250000, val_loss: 0.24169225
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 293.84898043, sen-loss: 31.60176891, dom-loss: 75.44857663, src-aux-loss: 99.49303085, tar-aux-loss: 87.30560374
Epoch: [11 ] train-acc: 0.89785714, dom-acc: 0.72035714, val-acc: 0.92250000, val_loss: 0.23456454
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 294.40338111, sen-loss: 30.94721057, dom-loss: 75.50419164, src-aux-loss: 98.56497043, tar-aux-loss: 89.38700753
Epoch: [12 ] train-acc: 0.88428571, dom-acc: 0.68910714, val-acc: 0.91250000, val_loss: 0.23619217
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 291.95166731, sen-loss: 29.98807524, dom-loss: 76.04199064, src-aux-loss: 98.02996725, tar-aux-loss: 87.89163512
Epoch: [13 ] train-acc: 0.90285714, dom-acc: 0.71107143, val-acc: 0.93750000, val_loss: 0.22314794
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 290.26513433, sen-loss: 29.60108922, dom-loss: 76.08946103, src-aux-loss: 97.22232544, tar-aux-loss: 87.35225850
Epoch: [14 ] train-acc: 0.90500000, dom-acc: 0.70375000, val-acc: 0.92750000, val_loss: 0.21547052
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 289.15487194, sen-loss: 28.86291824, dom-loss: 76.31325233, src-aux-loss: 96.61880153, tar-aux-loss: 87.35989809
Epoch: [15 ] train-acc: 0.90678571, dom-acc: 0.69330357, val-acc: 0.94000000, val_loss: 0.21694638
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 286.30710316, sen-loss: 28.45059873, dom-loss: 76.43173271, src-aux-loss: 96.23318774, tar-aux-loss: 85.19158447
Epoch: [16 ] train-acc: 0.90803571, dom-acc: 0.69696429, val-acc: 0.93500000, val_loss: 0.21314648
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 286.87619519, sen-loss: 27.96183057, dom-loss: 76.73585612, src-aux-loss: 95.60508615, tar-aux-loss: 86.57342213
Epoch: [17 ] train-acc: 0.91107143, dom-acc: 0.68955357, val-acc: 0.94250000, val_loss: 0.21280263
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 287.41689038, sen-loss: 27.56844649, dom-loss: 77.07850718, src-aux-loss: 95.03319609, tar-aux-loss: 87.73674047
Epoch: [18 ] train-acc: 0.91285714, dom-acc: 0.67875000, val-acc: 0.93500000, val_loss: 0.20408040
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 283.89488530, sen-loss: 27.03860027, dom-loss: 77.24414831, src-aux-loss: 94.39521092, tar-aux-loss: 85.21692848
Epoch: [19 ] train-acc: 0.91303571, dom-acc: 0.66696429, val-acc: 0.93500000, val_loss: 0.20093425
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 283.26915073, sen-loss: 26.62781511, dom-loss: 77.08970165, src-aux-loss: 94.05346704, tar-aux-loss: 85.49816644
Epoch: [20 ] train-acc: 0.91285714, dom-acc: 0.66232143, val-acc: 0.92750000, val_loss: 0.20018388
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 284.51931667, sen-loss: 26.28781269, dom-loss: 77.57626629, src-aux-loss: 93.56355339, tar-aux-loss: 87.09168297
Epoch: [21 ] train-acc: 0.91660714, dom-acc: 0.64973214, val-acc: 0.93500000, val_loss: 0.19808380
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 281.94918609, sen-loss: 26.00499850, dom-loss: 77.54919952, src-aux-loss: 93.10140252, tar-aux-loss: 85.29358470
Epoch: [22 ] train-acc: 0.91732143, dom-acc: 0.65651786, val-acc: 0.94750000, val_loss: 0.19951935
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 281.12539816, sen-loss: 25.60513323, dom-loss: 77.53189540, src-aux-loss: 92.80038589, tar-aux-loss: 85.18798435
Epoch: [23 ] train-acc: 0.91607143, dom-acc: 0.64294643, val-acc: 0.93250000, val_loss: 0.19601256
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 279.12541175, sen-loss: 25.36889970, dom-loss: 77.88328987, src-aux-loss: 92.33396792, tar-aux-loss: 83.53925329
Epoch: [24 ] train-acc: 0.92071429, dom-acc: 0.64098214, val-acc: 0.94500000, val_loss: 0.19435276
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 280.41725898, sen-loss: 24.98623062, dom-loss: 77.80725908, src-aux-loss: 92.09747845, tar-aux-loss: 85.52628928
Epoch: [25 ] train-acc: 0.92142857, dom-acc: 0.64892857, val-acc: 0.94000000, val_loss: 0.19443363
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 279.82024121, sen-loss: 24.57689530, dom-loss: 77.94393098, src-aux-loss: 91.80816746, tar-aux-loss: 85.49124610
Epoch: [26 ] train-acc: 0.92196429, dom-acc: 0.65392857, val-acc: 0.94250000, val_loss: 0.19648547
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 278.23846936, sen-loss: 24.32338170, dom-loss: 77.72817320, src-aux-loss: 91.39821976, tar-aux-loss: 84.78869599
Epoch: [27 ] train-acc: 0.92464286, dom-acc: 0.65142857, val-acc: 0.94250000, val_loss: 0.19037609
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 277.67510891, sen-loss: 24.09395862, dom-loss: 78.10049599, src-aux-loss: 90.93414098, tar-aux-loss: 84.54651278
Epoch: [28 ] train-acc: 0.92535714, dom-acc: 0.64437500, val-acc: 0.94000000, val_loss: 0.18969490
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 276.30022049, sen-loss: 23.70197815, dom-loss: 77.91558558, src-aux-loss: 90.46824098, tar-aux-loss: 84.21441579
Epoch: [29 ] train-acc: 0.92535714, dom-acc: 0.63741071, val-acc: 0.94000000, val_loss: 0.18874820
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 276.08175778, sen-loss: 23.39923205, dom-loss: 77.93722534, src-aux-loss: 90.11219496, tar-aux-loss: 84.63310558
Epoch: [30 ] train-acc: 0.92750000, dom-acc: 0.64437500, val-acc: 0.94000000, val_loss: 0.18806235
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 275.21469021, sen-loss: 23.13427706, dom-loss: 77.90966785, src-aux-loss: 89.90647441, tar-aux-loss: 84.26427054
Epoch: [31 ] train-acc: 0.93089286, dom-acc: 0.64964286, val-acc: 0.94000000, val_loss: 0.18984035
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 275.87907362, sen-loss: 22.89419916, dom-loss: 77.89338207, src-aux-loss: 89.57773775, tar-aux-loss: 85.51375467
Epoch: [32 ] train-acc: 0.93107143, dom-acc: 0.64017857, val-acc: 0.94000000, val_loss: 0.18730561
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 271.93673134, sen-loss: 22.59983721, dom-loss: 77.90076953, src-aux-loss: 89.18234533, tar-aux-loss: 82.25377864
Epoch: [33 ] train-acc: 0.93392857, dom-acc: 0.64080357, val-acc: 0.94000000, val_loss: 0.18642457
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 272.93840384, sen-loss: 22.23785514, dom-loss: 77.80438650, src-aux-loss: 88.67555130, tar-aux-loss: 84.22061163
Epoch: [34 ] train-acc: 0.93214286, dom-acc: 0.64303571, val-acc: 0.93750000, val_loss: 0.18463984
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 273.38751602, sen-loss: 21.99368975, dom-loss: 77.83077216, src-aux-loss: 88.44822705, tar-aux-loss: 85.11482787
Epoch: [35 ] train-acc: 0.93071429, dom-acc: 0.63848214, val-acc: 0.93750000, val_loss: 0.18556221
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 270.55434752, sen-loss: 21.98680549, dom-loss: 77.68111128, src-aux-loss: 87.92839295, tar-aux-loss: 82.95803893
Epoch: [36 ] train-acc: 0.93464286, dom-acc: 0.63812500, val-acc: 0.93750000, val_loss: 0.18304768
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 271.34672761, sen-loss: 21.50991227, dom-loss: 77.59225667, src-aux-loss: 87.49861598, tar-aux-loss: 84.74594450
Epoch: [37 ] train-acc: 0.93642857, dom-acc: 0.65008929, val-acc: 0.93750000, val_loss: 0.18393898
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 270.63381505, sen-loss: 21.14247611, dom-loss: 77.70375192, src-aux-loss: 87.27023506, tar-aux-loss: 84.51735133
Epoch: [38 ] train-acc: 0.93714286, dom-acc: 0.64589286, val-acc: 0.93750000, val_loss: 0.18362151
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 268.14519286, sen-loss: 21.05456640, dom-loss: 77.57891685, src-aux-loss: 86.89703506, tar-aux-loss: 82.61467552
Epoch: [39 ] train-acc: 0.93821429, dom-acc: 0.64383929, val-acc: 0.93750000, val_loss: 0.18222922
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 268.92322254, sen-loss: 20.60642969, dom-loss: 77.39679545, src-aux-loss: 86.47236580, tar-aux-loss: 84.44763148
Epoch: [40 ] train-acc: 0.94107143, dom-acc: 0.65437500, val-acc: 0.93500000, val_loss: 0.18514054
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 267.30667424, sen-loss: 20.50062515, dom-loss: 77.33160365, src-aux-loss: 86.05593276, tar-aux-loss: 83.41851443
Epoch: [41 ] train-acc: 0.93785714, dom-acc: 0.67169643, val-acc: 0.93500000, val_loss: 0.19072728
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 265.39293694, sen-loss: 20.19071259, dom-loss: 77.45003104, src-aux-loss: 85.62198132, tar-aux-loss: 82.13021165
Epoch: [42 ] train-acc: 0.93928571, dom-acc: 0.64348214, val-acc: 0.94500000, val_loss: 0.18344599
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 267.04788542, sen-loss: 19.97724738, dom-loss: 77.34195334, src-aux-loss: 85.35577443, tar-aux-loss: 84.37291020
Epoch: [43 ] train-acc: 0.94125000, dom-acc: 0.64232143, val-acc: 0.94750000, val_loss: 0.18246076
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 264.02634788, sen-loss: 19.68455017, dom-loss: 77.23968560, src-aux-loss: 85.02328807, tar-aux-loss: 82.07882297
Epoch: [44 ] train-acc: 0.94464286, dom-acc: 0.64660714, val-acc: 0.94250000, val_loss: 0.18042961
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 265.13776159, sen-loss: 19.62623749, dom-loss: 77.11819547, src-aux-loss: 84.59261882, tar-aux-loss: 83.80071008
Epoch: [45 ] train-acc: 0.94571429, dom-acc: 0.65125000, val-acc: 0.93500000, val_loss: 0.18416028
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 263.19094181, sen-loss: 19.16761474, dom-loss: 77.12550974, src-aux-loss: 84.28673980, tar-aux-loss: 82.61107773
Epoch: [46 ] train-acc: 0.94446429, dom-acc: 0.65473214, val-acc: 0.94000000, val_loss: 0.18093921
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 263.66123605, sen-loss: 18.87166348, dom-loss: 77.10487545, src-aux-loss: 83.80100775, tar-aux-loss: 83.88368815
Epoch: [47 ] train-acc: 0.94267857, dom-acc: 0.65776786, val-acc: 0.94000000, val_loss: 0.19163585
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 262.50190687, sen-loss: 18.66613096, dom-loss: 77.14853734, src-aux-loss: 83.36732012, tar-aux-loss: 83.31991732
Epoch: [48 ] train-acc: 0.94732143, dom-acc: 0.64937500, val-acc: 0.94250000, val_loss: 0.17973006
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 261.27731085, sen-loss: 18.40041136, dom-loss: 77.01304990, src-aux-loss: 82.92819083, tar-aux-loss: 82.93565774
Epoch: [49 ] train-acc: 0.94964286, dom-acc: 0.66642857, val-acc: 0.94250000, val_loss: 0.18259457
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 260.35820389, sen-loss: 18.24370942, dom-loss: 76.97618616, src-aux-loss: 82.36789262, tar-aux-loss: 82.77041608
Epoch: [50 ] train-acc: 0.94910714, dom-acc: 0.65464286, val-acc: 0.94250000, val_loss: 0.18813029
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 259.48332787, sen-loss: 18.04326957, dom-loss: 77.01539457, src-aux-loss: 81.93658251, tar-aux-loss: 82.48808151
Epoch: [51 ] train-acc: 0.95071429, dom-acc: 0.65473214, val-acc: 0.94250000, val_loss: 0.18367353
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 260.19806170, sen-loss: 17.76388610, dom-loss: 77.07714230, src-aux-loss: 81.63593364, tar-aux-loss: 83.72110093
Epoch: [52 ] train-acc: 0.95285714, dom-acc: 0.64812500, val-acc: 0.94250000, val_loss: 0.18601453
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 257.52109075, sen-loss: 17.45786914, dom-loss: 77.02303880, src-aux-loss: 80.97297996, tar-aux-loss: 82.06720346
Epoch: [53 ] train-acc: 0.95321429, dom-acc: 0.65687500, val-acc: 0.94000000, val_loss: 0.18524125
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_kitchen_HATN.ckpt
Best Epoch: [ 48] best val accuracy: 0.00000000 best val loss: 0.17973006
Testing accuracy: 0.90116667
./work/attentions/electronics_kitchen_train_HATN.txt
./work/attentions/electronics_kitchen_test_HATN.txt
loading data...
source domain:  electronics target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 30180
vocab-size:  83059
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'solid', 'outstanding', 'awesome', 'happy', 'fantastic', 'satisfied', 'worth', 'perfectly', 'recommended', 'durable', 'reliable', 'fine', 'love', 'inexpensive', 'pleased', 'old', 'arrived', 'comfortable', 'impressive', 'decent', 'useful', 'amazing', 'expected', 'sturdy', 'cool', 'superb', 'awsome', 'supposed', 'nice', 'greatest', 'reasonable', 'quick', 'recommend', 'effective', 'high', 'slick', 'well']
['returned', 'return', 'poor', 'worst', 'stopped', 'disappointing', 'useless', 'frustrating', 'disappointed', 'horrible', 'failed', 'bad', 'wrong', 'returning', 'awful', 'lasted', 'poorly', 'terrible', 'unreliable', 'worthless', 'broke', 'unacceptable', 'cheap', 'hard', 'overpriced', 'beware', 'expensive', 'quit', 'ridiculous', 'misleading', 'died', 'unusable', 'impossible', 'sad', 'slow', 'uncomfortable', 'unhappy', 'save', 'ok', 'short', 'defective', 'low', 'wasted', 'went', 'lousy', 'waste', 'difficult', 'broken', 'annoying', 'goes', 'scratched', 'sucks', 'crashed', 'average', 'cheaply', 'unable', 'lose', 'back', 'dead', 'flaky', 'flawed', 'mediocre', 'worse', 'stay']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 23009 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 405.20387983, sen-loss: 75.79848951, dom-loss: 78.99656087, src-aux-loss: 129.07708204, tar-aux-loss: 121.33174700
Epoch: [1  ] train-acc: 0.69017857, dom-acc: 0.80169643, val-acc: 0.70250000, val_loss: 0.63667738
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 374.74154234, sen-loss: 66.87073433, dom-loss: 74.88304186, src-aux-loss: 120.55174786, tar-aux-loss: 112.43601793
Epoch: [2  ] train-acc: 0.77625000, dom-acc: 0.81160714, val-acc: 0.79000000, val_loss: 0.54598635
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 353.27158356, sen-loss: 55.72646141, dom-loss: 73.41035849, src-aux-loss: 116.00778002, tar-aux-loss: 108.12698263
Epoch: [3  ] train-acc: 0.80964286, dom-acc: 0.63589286, val-acc: 0.83250000, val_loss: 0.42658323
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 338.89491010, sen-loss: 47.42321053, dom-loss: 73.28488445, src-aux-loss: 112.61685753, tar-aux-loss: 105.56995732
Epoch: [4  ] train-acc: 0.83553571, dom-acc: 0.59705357, val-acc: 0.86250000, val_loss: 0.36961591
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 329.99276280, sen-loss: 42.79035333, dom-loss: 73.94165474, src-aux-loss: 109.81317586, tar-aux-loss: 103.44757843
Epoch: [5  ] train-acc: 0.84982143, dom-acc: 0.59250000, val-acc: 0.86000000, val_loss: 0.34706321
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 324.87279940, sen-loss: 39.57982117, dom-loss: 74.48907918, src-aux-loss: 108.21573758, tar-aux-loss: 102.58815992
Epoch: [6  ] train-acc: 0.86928571, dom-acc: 0.56767857, val-acc: 0.89500000, val_loss: 0.30228335
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 321.49310970, sen-loss: 36.92208628, dom-loss: 74.95799702, src-aux-loss: 106.56848782, tar-aux-loss: 103.04453760
Epoch: [7  ] train-acc: 0.87732143, dom-acc: 0.53767857, val-acc: 0.90000000, val_loss: 0.28238323
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 316.50176048, sen-loss: 35.11366607, dom-loss: 75.80953366, src-aux-loss: 105.59297854, tar-aux-loss: 99.98558193
Epoch: [8  ] train-acc: 0.88500000, dom-acc: 0.49562500, val-acc: 0.90750000, val_loss: 0.26998463
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 314.73041725, sen-loss: 33.82937463, dom-loss: 76.54103363, src-aux-loss: 104.08662611, tar-aux-loss: 100.27338123
Epoch: [9  ] train-acc: 0.88892857, dom-acc: 0.48937500, val-acc: 0.90750000, val_loss: 0.24965414
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 312.62507606, sen-loss: 32.29967743, dom-loss: 77.37191051, src-aux-loss: 103.06041491, tar-aux-loss: 99.89307213
Epoch: [10 ] train-acc: 0.88553571, dom-acc: 0.47848214, val-acc: 0.90750000, val_loss: 0.24186747
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 310.13717151, sen-loss: 31.46616964, dom-loss: 77.95490772, src-aux-loss: 102.34387434, tar-aux-loss: 98.37221938
Epoch: [11 ] train-acc: 0.90017857, dom-acc: 0.43473214, val-acc: 0.91750000, val_loss: 0.23256159
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 311.54032612, sen-loss: 30.79538947, dom-loss: 78.32939231, src-aux-loss: 102.02164567, tar-aux-loss: 100.39389938
Epoch: [12 ] train-acc: 0.88607143, dom-acc: 0.44607143, val-acc: 0.91000000, val_loss: 0.23406138
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 307.15232015, sen-loss: 29.82237126, dom-loss: 78.40837175, src-aux-loss: 101.07006639, tar-aux-loss: 97.85151166
Epoch: [13 ] train-acc: 0.90482143, dom-acc: 0.41205357, val-acc: 0.93250000, val_loss: 0.22423577
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 307.38557982, sen-loss: 29.49811620, dom-loss: 78.55691594, src-aux-loss: 100.45344287, tar-aux-loss: 98.87710649
Epoch: [14 ] train-acc: 0.90535714, dom-acc: 0.42428571, val-acc: 0.92500000, val_loss: 0.21708722
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 303.89727616, sen-loss: 28.64329269, dom-loss: 78.71119022, src-aux-loss: 99.73003578, tar-aux-loss: 96.81275588
Epoch: [15 ] train-acc: 0.90857143, dom-acc: 0.42482143, val-acc: 0.93750000, val_loss: 0.21537928
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 302.43733525, sen-loss: 28.20089380, dom-loss: 78.50906402, src-aux-loss: 99.54033887, tar-aux-loss: 96.18703711
Epoch: [16 ] train-acc: 0.91035714, dom-acc: 0.43482143, val-acc: 0.93750000, val_loss: 0.21574146
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 302.34064841, sen-loss: 27.76977715, dom-loss: 78.36037028, src-aux-loss: 98.77970105, tar-aux-loss: 97.43079895
Epoch: [17 ] train-acc: 0.91196429, dom-acc: 0.43375000, val-acc: 0.93500000, val_loss: 0.21433379
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 300.44102550, sen-loss: 27.22619427, dom-loss: 78.15568322, src-aux-loss: 97.97425091, tar-aux-loss: 97.08489621
Epoch: [18 ] train-acc: 0.91446429, dom-acc: 0.45330357, val-acc: 0.92750000, val_loss: 0.20765813
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 297.39810514, sen-loss: 26.87047511, dom-loss: 77.75873774, src-aux-loss: 97.51607937, tar-aux-loss: 95.25281346
Epoch: [19 ] train-acc: 0.91642857, dom-acc: 0.45482143, val-acc: 0.92750000, val_loss: 0.20545338
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.68523788, sen-loss: 26.40723657, dom-loss: 77.87740242, src-aux-loss: 97.01151860, tar-aux-loss: 96.38907945
Epoch: [20 ] train-acc: 0.91678571, dom-acc: 0.48062500, val-acc: 0.93000000, val_loss: 0.20448126
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 295.39714599, sen-loss: 25.98161793, dom-loss: 77.58606821, src-aux-loss: 96.49263364, tar-aux-loss: 95.33682567
Epoch: [21 ] train-acc: 0.91928571, dom-acc: 0.48294643, val-acc: 0.92750000, val_loss: 0.20344959
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 295.10585856, sen-loss: 25.69626873, dom-loss: 77.47132838, src-aux-loss: 96.05507207, tar-aux-loss: 95.88318902
Epoch: [22 ] train-acc: 0.91946429, dom-acc: 0.49598214, val-acc: 0.93500000, val_loss: 0.20368864
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 293.86100268, sen-loss: 25.35046398, dom-loss: 77.41435993, src-aux-loss: 95.55264819, tar-aux-loss: 95.54353040
Epoch: [23 ] train-acc: 0.92035714, dom-acc: 0.50437500, val-acc: 0.93000000, val_loss: 0.20176366
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 291.56752753, sen-loss: 25.14722733, dom-loss: 77.29880553, src-aux-loss: 95.19113678, tar-aux-loss: 93.93035901
Epoch: [24 ] train-acc: 0.92357143, dom-acc: 0.48026786, val-acc: 0.93250000, val_loss: 0.20092787
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 292.83586001, sen-loss: 24.71975400, dom-loss: 77.35914487, src-aux-loss: 94.68335378, tar-aux-loss: 96.07360721
Epoch: [25 ] train-acc: 0.92553571, dom-acc: 0.48348214, val-acc: 0.93500000, val_loss: 0.20094141
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 290.68767476, sen-loss: 24.24843481, dom-loss: 77.50782287, src-aux-loss: 94.38510633, tar-aux-loss: 94.54630947
Epoch: [26 ] train-acc: 0.92035714, dom-acc: 0.48660714, val-acc: 0.93500000, val_loss: 0.20719108
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 289.95904469, sen-loss: 24.11550270, dom-loss: 77.41755909, src-aux-loss: 94.08720481, tar-aux-loss: 94.33877772
Epoch: [27 ] train-acc: 0.92625000, dom-acc: 0.49348214, val-acc: 0.93500000, val_loss: 0.19973460
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 287.95725894, sen-loss: 23.76373466, dom-loss: 77.64703929, src-aux-loss: 93.43557823, tar-aux-loss: 93.11090630
Epoch: [28 ] train-acc: 0.92946429, dom-acc: 0.49669643, val-acc: 0.93250000, val_loss: 0.19767088
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 288.57562160, sen-loss: 23.38878590, dom-loss: 77.66194439, src-aux-loss: 93.04522908, tar-aux-loss: 94.47966206
Epoch: [29 ] train-acc: 0.93107143, dom-acc: 0.49098214, val-acc: 0.93250000, val_loss: 0.19816081
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 286.53899431, sen-loss: 23.08778475, dom-loss: 77.72060031, src-aux-loss: 92.77829492, tar-aux-loss: 92.95231646
Epoch: [30 ] train-acc: 0.93071429, dom-acc: 0.47339286, val-acc: 0.93750000, val_loss: 0.19933034
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.26821303, sen-loss: 22.78231335, dom-loss: 77.87023968, src-aux-loss: 92.15487134, tar-aux-loss: 94.46078908
Epoch: [31 ] train-acc: 0.92821429, dom-acc: 0.44562500, val-acc: 0.93750000, val_loss: 0.20219554
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 286.84855723, sen-loss: 22.48859517, dom-loss: 78.20658970, src-aux-loss: 91.96042013, tar-aux-loss: 94.19295108
Epoch: [32 ] train-acc: 0.93267857, dom-acc: 0.45517857, val-acc: 0.93750000, val_loss: 0.19840330
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 285.48993468, sen-loss: 22.27036519, dom-loss: 78.14435899, src-aux-loss: 91.50787264, tar-aux-loss: 93.56733608
Epoch: [33 ] train-acc: 0.93446429, dom-acc: 0.45151786, val-acc: 0.93750000, val_loss: 0.19753397
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 284.58424997, sen-loss: 21.94162965, dom-loss: 78.30578983, src-aux-loss: 90.92660153, tar-aux-loss: 93.41022795
Epoch: [34 ] train-acc: 0.93517857, dom-acc: 0.43544643, val-acc: 0.93250000, val_loss: 0.19676553
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 283.95323777, sen-loss: 21.65142138, dom-loss: 78.39873165, src-aux-loss: 90.90769243, tar-aux-loss: 92.99539202
Epoch: [35 ] train-acc: 0.93696429, dom-acc: 0.44669643, val-acc: 0.93500000, val_loss: 0.19616896
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 283.33774233, sen-loss: 21.50083739, dom-loss: 78.45745772, src-aux-loss: 90.02714628, tar-aux-loss: 93.35230231
Epoch: [36 ] train-acc: 0.93785714, dom-acc: 0.44214286, val-acc: 0.93250000, val_loss: 0.19731744
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 283.28474593, sen-loss: 21.11245878, dom-loss: 78.37600762, src-aux-loss: 89.80400068, tar-aux-loss: 93.99227875
Epoch: [37 ] train-acc: 0.93821429, dom-acc: 0.44160714, val-acc: 0.93750000, val_loss: 0.19749425
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 282.54920435, sen-loss: 20.80405853, dom-loss: 78.31925082, src-aux-loss: 89.58360672, tar-aux-loss: 93.84228736
Epoch: [38 ] train-acc: 0.93714286, dom-acc: 0.43357143, val-acc: 0.93750000, val_loss: 0.19806083
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 280.20122457, sen-loss: 20.65401037, dom-loss: 78.31639344, src-aux-loss: 88.93718117, tar-aux-loss: 92.29364014
Epoch: [39 ] train-acc: 0.94089286, dom-acc: 0.44741071, val-acc: 0.93750000, val_loss: 0.19824538
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 279.28785872, sen-loss: 20.27346289, dom-loss: 78.14056391, src-aux-loss: 88.58035570, tar-aux-loss: 92.29347652
Epoch: [40 ] train-acc: 0.94000000, dom-acc: 0.44955357, val-acc: 0.93750000, val_loss: 0.19806400
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_video_HATN.ckpt
Best Epoch: [ 35] best val accuracy: 0.00000000 best val loss: 0.19616896
Testing accuracy: 0.83450000
./work/attentions/electronics_video_train_HATN.txt
./work/attentions/electronics_video_test_HATN.txt
loading data...
source domain:  kitchen target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 9750
vocab-size:  78006
['great', 'love', 'easy', 'best', 'excellent', 'good', 'perfect', 'happy', 'pleased', 'loves', 'wonderful', 'amazing', 'nice', 'satisfied', 'fantastic', 'awesome', 'well', 'useful', 'solid', 'favorite', 'attractive', 'fabulous', 'quick', 'perfectly', 'outstanding', 'elegant', 'fine', 'durable', 'comfortable', 'sturdy', 'impressed', 'terrific', 'reasonable', 'works', 'incredible', 'exceptional', 'safe', 'decent', 'gorgeous', 'pretty', 'easiest', 'lovely', 'beautiful', 'unbelievable', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'broke', 'worst', 'poorly', 'bad', 'useless', 'horrible', 'returned', 'defective', 'wrong', 'broken', 'impossible', 'flimsy', 'awful', 'cheap', 'dissapointed', 'worked', 'difficult', 'stopped', 'dangerous', 'ruined', 'worse', 'hard', 'worthless', 'overpriced', 'frustrating', 'disapointed', 'misleading', 'sad', 'return', 'stuck', 'lasted', 'unacceptable', 'unusable', 'unhappy', 'failed', 'flawed', 'uneven', 'lousy', 'ridiculous', 'ugly', 'refunded', 'messy', 'hate', 'leaky', 'wasted', 'unfortunately', 'false', 'expensive', 'dissappointed', 'overrated']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
5600 400 6000 19856 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 414.54378915, sen-loss: 75.65720081, dom-loss: 78.97210562, src-aux-loss: 139.00316966, tar-aux-loss: 120.91131181
Epoch: [1  ] train-acc: 0.69732143, dom-acc: 0.79321429, val-acc: 0.71000000, val_loss: 0.63189501
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 385.66354251, sen-loss: 65.95499897, dom-loss: 74.55592203, src-aux-loss: 131.23574436, tar-aux-loss: 113.91687506
Epoch: [2  ] train-acc: 0.78410714, dom-acc: 0.80241071, val-acc: 0.80500000, val_loss: 0.53008813
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.99679565, sen-loss: 53.58548039, dom-loss: 72.96143764, src-aux-loss: 125.19872129, tar-aux-loss: 109.25115657
Epoch: [3  ] train-acc: 0.84910714, dom-acc: 0.64008929, val-acc: 0.84500000, val_loss: 0.41192544
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 342.88048053, sen-loss: 43.10155553, dom-loss: 72.51065576, src-aux-loss: 120.77768642, tar-aux-loss: 106.49058133
Epoch: [4  ] train-acc: 0.86714286, dom-acc: 0.58598214, val-acc: 0.84750000, val_loss: 0.35664827
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.93135667, sen-loss: 38.01074725, dom-loss: 72.91182834, src-aux-loss: 117.78873330, tar-aux-loss: 104.22004914
Epoch: [5  ] train-acc: 0.88089286, dom-acc: 0.56250000, val-acc: 0.87000000, val_loss: 0.33596215
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 327.55600834, sen-loss: 34.43189496, dom-loss: 74.08547920, src-aux-loss: 115.14783508, tar-aux-loss: 103.89079779
Epoch: [6  ] train-acc: 0.88750000, dom-acc: 0.55839286, val-acc: 0.88500000, val_loss: 0.32641345
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 322.28685594, sen-loss: 32.11364394, dom-loss: 74.64472377, src-aux-loss: 113.36936539, tar-aux-loss: 102.15912259
Epoch: [7  ] train-acc: 0.89678571, dom-acc: 0.53553571, val-acc: 0.89500000, val_loss: 0.31246945
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 319.57440686, sen-loss: 30.46492774, dom-loss: 75.76905751, src-aux-loss: 111.81384474, tar-aux-loss: 101.52657801
Epoch: [8  ] train-acc: 0.90285714, dom-acc: 0.51330357, val-acc: 0.89250000, val_loss: 0.31129470
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 317.89579368, sen-loss: 29.22103421, dom-loss: 76.56419450, src-aux-loss: 110.63183421, tar-aux-loss: 101.47873068
Epoch: [9  ] train-acc: 0.90375000, dom-acc: 0.48955357, val-acc: 0.89750000, val_loss: 0.30165619
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 315.90029693, sen-loss: 28.23978151, dom-loss: 77.11437213, src-aux-loss: 109.42514938, tar-aux-loss: 101.12099147
Epoch: [10 ] train-acc: 0.90696429, dom-acc: 0.45839286, val-acc: 0.90250000, val_loss: 0.30653861
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 313.15369439, sen-loss: 27.55311134, dom-loss: 77.61267322, src-aux-loss: 108.83045971, tar-aux-loss: 99.15744931
Epoch: [11 ] train-acc: 0.91232143, dom-acc: 0.44294643, val-acc: 0.89750000, val_loss: 0.30389640
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 312.26893950, sen-loss: 26.90210340, dom-loss: 78.00193244, src-aux-loss: 107.64370477, tar-aux-loss: 99.72119778
Epoch: [12 ] train-acc: 0.91196429, dom-acc: 0.44964286, val-acc: 0.91250000, val_loss: 0.29134300
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 309.97852063, sen-loss: 26.27093501, dom-loss: 78.36207652, src-aux-loss: 106.77610993, tar-aux-loss: 98.56939965
Epoch: [13 ] train-acc: 0.91607143, dom-acc: 0.45714286, val-acc: 0.89500000, val_loss: 0.29858789
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 310.28367829, sen-loss: 25.88222609, dom-loss: 78.54146206, src-aux-loss: 105.86070710, tar-aux-loss: 99.99928397
Epoch: [14 ] train-acc: 0.91750000, dom-acc: 0.45357143, val-acc: 0.90250000, val_loss: 0.29474244
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 306.83448339, sen-loss: 25.31211450, dom-loss: 78.75966197, src-aux-loss: 105.41161513, tar-aux-loss: 97.35109162
Epoch: [15 ] train-acc: 0.91464286, dom-acc: 0.46803571, val-acc: 0.89500000, val_loss: 0.31043249
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 308.95315766, sen-loss: 24.98950855, dom-loss: 78.93788612, src-aux-loss: 104.42030925, tar-aux-loss: 100.60545456
Epoch: [16 ] train-acc: 0.91714286, dom-acc: 0.46026786, val-acc: 0.91750000, val_loss: 0.28423616
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 304.02867484, sen-loss: 24.56605382, dom-loss: 78.76633483, src-aux-loss: 103.90608096, tar-aux-loss: 96.79020506
Epoch: [17 ] train-acc: 0.92107143, dom-acc: 0.45410714, val-acc: 0.90750000, val_loss: 0.29056051
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 304.06520653, sen-loss: 24.23037675, dom-loss: 78.93956238, src-aux-loss: 103.03199387, tar-aux-loss: 97.86327475
Epoch: [18 ] train-acc: 0.92214286, dom-acc: 0.45687500, val-acc: 0.91750000, val_loss: 0.28540507
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 303.23649120, sen-loss: 23.86305533, dom-loss: 78.88700259, src-aux-loss: 102.77570921, tar-aux-loss: 97.71072263
Epoch: [19 ] train-acc: 0.92321429, dom-acc: 0.46544643, val-acc: 0.91750000, val_loss: 0.28278416
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 302.20877337, sen-loss: 23.53819001, dom-loss: 78.65949154, src-aux-loss: 101.96350491, tar-aux-loss: 98.04758680
Epoch: [20 ] train-acc: 0.92571429, dom-acc: 0.46651786, val-acc: 0.92000000, val_loss: 0.28350258
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 300.11654449, sen-loss: 23.31500775, dom-loss: 78.68238181, src-aux-loss: 101.62889779, tar-aux-loss: 96.49025738
Epoch: [21 ] train-acc: 0.92660714, dom-acc: 0.47616071, val-acc: 0.91000000, val_loss: 0.29873630
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 298.91010952, sen-loss: 22.89626259, dom-loss: 78.48396939, src-aux-loss: 100.77817273, tar-aux-loss: 96.75170422
Epoch: [22 ] train-acc: 0.92821429, dom-acc: 0.48892857, val-acc: 0.92250000, val_loss: 0.28468943
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 298.63257837, sen-loss: 22.62815145, dom-loss: 78.31938863, src-aux-loss: 100.25628245, tar-aux-loss: 97.42875516
Epoch: [23 ] train-acc: 0.92910714, dom-acc: 0.49250000, val-acc: 0.92250000, val_loss: 0.28563648
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 296.32210207, sen-loss: 22.51904910, dom-loss: 78.20416826, src-aux-loss: 99.63927966, tar-aux-loss: 95.95960373
Epoch: [24 ] train-acc: 0.92642857, dom-acc: 0.48714286, val-acc: 0.92500000, val_loss: 0.28070295
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.82411790, sen-loss: 22.28379646, dom-loss: 78.18623370, src-aux-loss: 99.09323514, tar-aux-loss: 96.26085389
Epoch: [25 ] train-acc: 0.93035714, dom-acc: 0.49714286, val-acc: 0.93000000, val_loss: 0.27973866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.54375315, sen-loss: 22.22588668, dom-loss: 77.90191394, src-aux-loss: 98.76612955, tar-aux-loss: 96.64982218
Epoch: [26 ] train-acc: 0.92857143, dom-acc: 0.49955357, val-acc: 0.92500000, val_loss: 0.27618784
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 294.83259654, sen-loss: 21.94577320, dom-loss: 77.83002406, src-aux-loss: 98.25426352, tar-aux-loss: 96.80253392
Epoch: [27 ] train-acc: 0.93071429, dom-acc: 0.51732143, val-acc: 0.92750000, val_loss: 0.27824789
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 293.93168950, sen-loss: 21.71104178, dom-loss: 77.70846313, src-aux-loss: 97.71627420, tar-aux-loss: 96.79590976
Epoch: [28 ] train-acc: 0.93303571, dom-acc: 0.50053571, val-acc: 0.92500000, val_loss: 0.27670175
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 291.79380035, sen-loss: 21.35619722, dom-loss: 77.68240339, src-aux-loss: 97.55194271, tar-aux-loss: 95.20325613
Epoch: [29 ] train-acc: 0.93196429, dom-acc: 0.51455357, val-acc: 0.92250000, val_loss: 0.27141863
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.63842988, sen-loss: 21.22981773, dom-loss: 77.44222611, src-aux-loss: 96.94481498, tar-aux-loss: 95.02157259
Epoch: [30 ] train-acc: 0.93178571, dom-acc: 0.52678571, val-acc: 0.92750000, val_loss: 0.27325231
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 290.51830363, sen-loss: 21.05414055, dom-loss: 77.42057508, src-aux-loss: 96.34846425, tar-aux-loss: 95.69512475
Epoch: [31 ] train-acc: 0.93375000, dom-acc: 0.49669643, val-acc: 0.91000000, val_loss: 0.29379442
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 290.24828649, sen-loss: 20.85521204, dom-loss: 77.32092404, src-aux-loss: 95.87219429, tar-aux-loss: 96.19995654
Epoch: [32 ] train-acc: 0.93660714, dom-acc: 0.49544643, val-acc: 0.92000000, val_loss: 0.28393796
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 287.86929941, sen-loss: 20.60958666, dom-loss: 77.32035077, src-aux-loss: 95.32662725, tar-aux-loss: 94.61273533
Epoch: [33 ] train-acc: 0.93625000, dom-acc: 0.49383929, val-acc: 0.91500000, val_loss: 0.28857476
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 288.16359878, sen-loss: 20.37300956, dom-loss: 77.33113742, src-aux-loss: 94.73156488, tar-aux-loss: 95.72788435
Epoch: [34 ] train-acc: 0.93660714, dom-acc: 0.52339286, val-acc: 0.92750000, val_loss: 0.27398086
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_books_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.27141863
Testing accuracy: 0.84833333
./work/attentions/kitchen_books_train_HATN.txt
./work/attentions/kitchen_books_test_HATN.txt
loading data...
source domain:  kitchen target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 11843
vocab-size:  80685
['great', 'easy', 'love', 'best', 'good', 'excellent', 'perfect', 'happy', 'pleased', 'loves', 'wonderful', 'nice', 'fantastic', 'amazing', 'works', 'satisfied', 'awesome', 'solid', 'useful', 'attractive', 'perfectly', 'fabulous', 'favorite', 'comfortable', 'sturdy', 'outstanding', 'quick', 'durable', 'terrific', 'fine', 'elegant', 'safe', 'simple', 'highly', 'impressed', 'gorgeous', 'easiest', 'incredible', 'exceptional', 'reasonable', 'decent', 'fast', 'lovely', 'pretty', 'loved', 'beautiful', 'unbelievable', 'well', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'worst', 'poorly', 'broke', 'useless', 'bad', 'horrible', 'returned', 'defective', 'wrong', 'broken', 'flimsy', 'awful', 'impossible', 'cheap', 'dissapointed', 'difficult', 'worked', 'stopped', 'dangerous', 'hard', 'stuck', 'worthless', 'ruined', 'worse', 'overpriced', 'frustrating', 'disapointed', 'misleading', 'sad', 'unhappy', 'fell', 'lasted', 'unacceptable', 'unusable', 'failed', 'flawed', 'uneven', 'sorry', 'rusted', 'lousy', 'ridiculous', 'frustrated', 'ugly', 'messy', 'hate', 'uncomfortable', 'leaky', 'better', 'return', 'went', 'wasted', 'expensive', 'overrated']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
5600 400 6000 19856 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 409.54335856, sen-loss: 75.58868611, dom-loss: 78.78887367, src-aux-loss: 136.67297632, tar-aux-loss: 118.49282390
Epoch: [1  ] train-acc: 0.71125000, dom-acc: 0.79205357, val-acc: 0.72500000, val_loss: 0.63025826
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 380.57829142, sen-loss: 65.91979775, dom-loss: 74.67661071, src-aux-loss: 128.71407461, tar-aux-loss: 111.26780868
Epoch: [2  ] train-acc: 0.78928571, dom-acc: 0.79803571, val-acc: 0.81500000, val_loss: 0.53000498
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 356.23676419, sen-loss: 53.88310170, dom-loss: 73.34074223, src-aux-loss: 123.18066853, tar-aux-loss: 105.83225167
Epoch: [3  ] train-acc: 0.84732143, dom-acc: 0.65250000, val-acc: 0.84750000, val_loss: 0.41395026
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 339.89715219, sen-loss: 43.22215870, dom-loss: 72.99247277, src-aux-loss: 118.69312090, tar-aux-loss: 104.98940092
Epoch: [4  ] train-acc: 0.86196429, dom-acc: 0.59089286, val-acc: 0.86000000, val_loss: 0.35808507
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 329.16353202, sen-loss: 38.14335373, dom-loss: 73.19356126, src-aux-loss: 115.91588360, tar-aux-loss: 101.91073394
Epoch: [5  ] train-acc: 0.87946429, dom-acc: 0.55794643, val-acc: 0.87000000, val_loss: 0.33672509
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 325.07436848, sen-loss: 34.78557359, dom-loss: 74.42569101, src-aux-loss: 113.70774060, tar-aux-loss: 102.15536213
Epoch: [6  ] train-acc: 0.88607143, dom-acc: 0.53803571, val-acc: 0.88250000, val_loss: 0.33307976
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 318.72272372, sen-loss: 32.47252005, dom-loss: 75.00562465, src-aux-loss: 111.60923856, tar-aux-loss: 99.63534212
Epoch: [7  ] train-acc: 0.89571429, dom-acc: 0.53008929, val-acc: 0.89750000, val_loss: 0.31474519
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 315.50021815, sen-loss: 30.73044807, dom-loss: 75.89183152, src-aux-loss: 110.08600020, tar-aux-loss: 98.79193747
Epoch: [8  ] train-acc: 0.89946429, dom-acc: 0.50455357, val-acc: 0.89750000, val_loss: 0.30951053
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 315.04175949, sen-loss: 29.57242955, dom-loss: 76.37992787, src-aux-loss: 109.00685996, tar-aux-loss: 100.08254069
Epoch: [9  ] train-acc: 0.90375000, dom-acc: 0.48366071, val-acc: 0.90250000, val_loss: 0.30125052
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 311.91533279, sen-loss: 28.51707025, dom-loss: 77.03303355, src-aux-loss: 108.06432146, tar-aux-loss: 98.30090755
Epoch: [10 ] train-acc: 0.90642857, dom-acc: 0.46696429, val-acc: 0.90250000, val_loss: 0.30512252
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 308.66706634, sen-loss: 27.74567748, dom-loss: 77.21651077, src-aux-loss: 107.13134456, tar-aux-loss: 96.57353157
Epoch: [11 ] train-acc: 0.90982143, dom-acc: 0.45142857, val-acc: 0.90000000, val_loss: 0.30157289
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 307.86153793, sen-loss: 27.10649508, dom-loss: 77.59207672, src-aux-loss: 106.15684527, tar-aux-loss: 97.00612009
Epoch: [12 ] train-acc: 0.90964286, dom-acc: 0.45651786, val-acc: 0.91250000, val_loss: 0.29090881
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 307.68159318, sen-loss: 26.37995706, dom-loss: 77.74145466, src-aux-loss: 105.19129753, tar-aux-loss: 98.36888421
Epoch: [13 ] train-acc: 0.91321429, dom-acc: 0.44205357, val-acc: 0.90500000, val_loss: 0.29652759
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 305.11201191, sen-loss: 26.02742010, dom-loss: 77.88668483, src-aux-loss: 104.39178860, tar-aux-loss: 96.80611819
Epoch: [14 ] train-acc: 0.91589286, dom-acc: 0.45294643, val-acc: 0.90500000, val_loss: 0.29198620
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 302.57933211, sen-loss: 25.47635616, dom-loss: 77.96851432, src-aux-loss: 103.92470604, tar-aux-loss: 95.20975506
Epoch: [15 ] train-acc: 0.91035714, dom-acc: 0.43767857, val-acc: 0.89000000, val_loss: 0.32114717
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 303.30722070, sen-loss: 25.16333984, dom-loss: 78.14347929, src-aux-loss: 103.00968462, tar-aux-loss: 96.99071735
Epoch: [16 ] train-acc: 0.91660714, dom-acc: 0.44919643, val-acc: 0.91000000, val_loss: 0.28593314
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 300.18455839, sen-loss: 24.68429468, dom-loss: 77.98824793, src-aux-loss: 102.31675386, tar-aux-loss: 95.19526225
Epoch: [17 ] train-acc: 0.92035714, dom-acc: 0.44205357, val-acc: 0.91500000, val_loss: 0.28969854
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 299.71240091, sen-loss: 24.41801199, dom-loss: 78.01037073, src-aux-loss: 101.55415320, tar-aux-loss: 95.72986335
Epoch: [18 ] train-acc: 0.91982143, dom-acc: 0.45750000, val-acc: 0.91500000, val_loss: 0.29041782
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 298.99323106, sen-loss: 24.05750161, dom-loss: 78.12288600, src-aux-loss: 100.97185093, tar-aux-loss: 95.84099221
Epoch: [19 ] train-acc: 0.92232143, dom-acc: 0.44964286, val-acc: 0.91500000, val_loss: 0.28386888
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.67763281, sen-loss: 23.67835650, dom-loss: 77.96375287, src-aux-loss: 100.54452789, tar-aux-loss: 95.49099481
Epoch: [20 ] train-acc: 0.92392857, dom-acc: 0.44508929, val-acc: 0.91750000, val_loss: 0.28479254
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 297.15003347, sen-loss: 23.42775655, dom-loss: 78.23070818, src-aux-loss: 100.24260581, tar-aux-loss: 95.24896300
Epoch: [21 ] train-acc: 0.92321429, dom-acc: 0.44642857, val-acc: 0.91500000, val_loss: 0.30152437
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 294.94565201, sen-loss: 23.08504545, dom-loss: 78.13495260, src-aux-loss: 99.46474290, tar-aux-loss: 94.26091015
Epoch: [22 ] train-acc: 0.92500000, dom-acc: 0.45687500, val-acc: 0.91750000, val_loss: 0.28934166
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 295.03688741, sen-loss: 22.74883583, dom-loss: 78.15935409, src-aux-loss: 98.93842995, tar-aux-loss: 95.19026691
Epoch: [23 ] train-acc: 0.92500000, dom-acc: 0.45991071, val-acc: 0.91500000, val_loss: 0.28295803
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 293.34407401, sen-loss: 22.59892018, dom-loss: 77.91351509, src-aux-loss: 98.16203934, tar-aux-loss: 94.66960096
Epoch: [24 ] train-acc: 0.92625000, dom-acc: 0.45044643, val-acc: 0.92000000, val_loss: 0.28548682
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 293.13819337, sen-loss: 22.35098402, dom-loss: 78.18684542, src-aux-loss: 97.71860427, tar-aux-loss: 94.88175970
Epoch: [25 ] train-acc: 0.92607143, dom-acc: 0.45116071, val-acc: 0.91500000, val_loss: 0.28164646
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 291.29944634, sen-loss: 22.27464123, dom-loss: 78.04076511, src-aux-loss: 97.42078727, tar-aux-loss: 93.56325352
Epoch: [26 ] train-acc: 0.92714286, dom-acc: 0.44633929, val-acc: 0.91250000, val_loss: 0.28008604
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 291.98666549, sen-loss: 22.00519586, dom-loss: 78.11674148, src-aux-loss: 96.68848300, tar-aux-loss: 95.17624444
Epoch: [27 ] train-acc: 0.92857143, dom-acc: 0.45089286, val-acc: 0.91500000, val_loss: 0.28800356
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 290.46914220, sen-loss: 21.80145147, dom-loss: 78.02115351, src-aux-loss: 96.48024035, tar-aux-loss: 94.16629660
Epoch: [28 ] train-acc: 0.92982143, dom-acc: 0.44803571, val-acc: 0.92250000, val_loss: 0.27927050
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 288.83319759, sen-loss: 21.43229094, dom-loss: 78.18053991, src-aux-loss: 96.03709835, tar-aux-loss: 93.18326837
Epoch: [29 ] train-acc: 0.93000000, dom-acc: 0.45357143, val-acc: 0.92500000, val_loss: 0.27835348
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 288.13105178, sen-loss: 21.26584183, dom-loss: 77.98990017, src-aux-loss: 95.59237677, tar-aux-loss: 93.28293228
Epoch: [30 ] train-acc: 0.93160714, dom-acc: 0.45223214, val-acc: 0.92250000, val_loss: 0.28155807
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.76747727, sen-loss: 21.13250922, dom-loss: 77.94354790, src-aux-loss: 95.10472935, tar-aux-loss: 93.58669102
Epoch: [31 ] train-acc: 0.93053571, dom-acc: 0.43401786, val-acc: 0.91500000, val_loss: 0.30390736
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.07248950, sen-loss: 20.95188208, dom-loss: 77.82945246, src-aux-loss: 94.64340645, tar-aux-loss: 94.64774990
Epoch: [32 ] train-acc: 0.93482143, dom-acc: 0.44937500, val-acc: 0.92250000, val_loss: 0.28986633
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 284.40020847, sen-loss: 20.64301166, dom-loss: 77.67701340, src-aux-loss: 94.06519872, tar-aux-loss: 92.01498741
Epoch: [33 ] train-acc: 0.93589286, dom-acc: 0.45517857, val-acc: 0.92000000, val_loss: 0.29114166
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 286.09749365, sen-loss: 20.40052304, dom-loss: 77.90504944, src-aux-loss: 93.58079290, tar-aux-loss: 94.21112883
Epoch: [34 ] train-acc: 0.93482143, dom-acc: 0.46767857, val-acc: 0.92500000, val_loss: 0.27842727
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_dvd_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.27835348
Testing accuracy: 0.84733333
./work/attentions/kitchen_dvd_train_HATN.txt
./work/attentions/kitchen_dvd_test_HATN.txt
loading data...
source domain:  kitchen target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 17009
vocab-size:  49470
['great', 'love', 'easy', 'best', 'good', 'excellent', 'perfect', 'happy', 'pleased', 'wonderful', 'loves', 'nice', 'satisfied', 'awesome', 'well', 'fantastic', 'amazing', 'attractive', 'useful', 'solid', 'fabulous', 'favorite', 'comfortable', 'quick', 'perfectly', 'highly', 'outstanding', 'fine', 'works', 'incredible', 'elegant', 'terrific', 'impressed', 'gorgeous', 'exceptional', 'reasonable', 'decent', 'durable', 'lovely', 'easiest', 'loved', 'beautiful', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'worst', 'poorly', 'broke', 'bad', 'useless', 'horrible', 'returned', 'wrong', 'defective', 'broken', 'cheap', 'awful', 'flimsy', 'worked', 'impossible', 'dissapointed', 'hard', 'difficult', 'dangerous', 'ruined', 'worthless', 'stopped', 'overpriced', 'frustrating', 'disapointed', 'misleading', 'sad', 'stuck', 'unhappy', 'worse', 'ridiculous', 'failed', 'frustrated', 'flawed', 'lasted', 'hate', 'uneven', 'sorry', 'unacceptable', 'still', 'return', 'went', 'lousy', 'unusable', 'expensive', 'ugly', 'refunded', 'dissappointed', 'leaky', 'rusted', 'better', 'unfortunately', 'overrated']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
5600 400 6000 19856 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 411.66848636, sen-loss: 75.60756809, dom-loss: 79.26163059, src-aux-loss: 137.83090174, tar-aux-loss: 118.96838570
Epoch: [1  ] train-acc: 0.69553571, dom-acc: 0.67705357, val-acc: 0.70750000, val_loss: 0.63104707
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 384.65987110, sen-loss: 66.00077349, dom-loss: 76.24012393, src-aux-loss: 130.18315601, tar-aux-loss: 112.23581600
Epoch: [2  ] train-acc: 0.78660714, dom-acc: 0.68732143, val-acc: 0.81000000, val_loss: 0.53182888
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.58416224, sen-loss: 53.72092110, dom-loss: 75.40188032, src-aux-loss: 124.59849215, tar-aux-loss: 106.86287004
Epoch: [3  ] train-acc: 0.84392857, dom-acc: 0.60071429, val-acc: 0.83500000, val_loss: 0.41066217
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 343.28693581, sen-loss: 42.99226131, dom-loss: 75.37077755, src-aux-loss: 120.13485801, tar-aux-loss: 104.78904027
Epoch: [4  ] train-acc: 0.86375000, dom-acc: 0.65901786, val-acc: 0.85250000, val_loss: 0.35639518
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.26294136, sen-loss: 37.78699854, dom-loss: 75.29360276, src-aux-loss: 116.79428846, tar-aux-loss: 102.38804919
Epoch: [5  ] train-acc: 0.88303571, dom-acc: 0.65535714, val-acc: 0.87250000, val_loss: 0.33708432
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 325.80120945, sen-loss: 34.42461143, dom-loss: 75.44474983, src-aux-loss: 114.55860102, tar-aux-loss: 101.37324822
Epoch: [6  ] train-acc: 0.88910714, dom-acc: 0.70723214, val-acc: 0.88000000, val_loss: 0.32585981
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 319.72275305, sen-loss: 32.10655785, dom-loss: 75.51971471, src-aux-loss: 112.52563667, tar-aux-loss: 99.57084256
Epoch: [7  ] train-acc: 0.89571429, dom-acc: 0.66598214, val-acc: 0.89000000, val_loss: 0.31488648
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 315.76423693, sen-loss: 30.36079660, dom-loss: 75.89892507, src-aux-loss: 110.96338099, tar-aux-loss: 98.54113436
Epoch: [8  ] train-acc: 0.89982143, dom-acc: 0.68767857, val-acc: 0.89500000, val_loss: 0.31416401
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 314.53466606, sen-loss: 29.26607195, dom-loss: 76.11041915, src-aux-loss: 110.12392288, tar-aux-loss: 99.03425246
Epoch: [9  ] train-acc: 0.90553571, dom-acc: 0.65651786, val-acc: 0.89750000, val_loss: 0.30228251
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 311.29043770, sen-loss: 28.17319287, dom-loss: 76.45368767, src-aux-loss: 108.66079605, tar-aux-loss: 98.00276005
Epoch: [10 ] train-acc: 0.90928571, dom-acc: 0.62678571, val-acc: 0.90250000, val_loss: 0.30261782
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 308.56348181, sen-loss: 27.55679474, dom-loss: 76.55473882, src-aux-loss: 108.10465723, tar-aux-loss: 96.34729153
Epoch: [11 ] train-acc: 0.91017857, dom-acc: 0.61473214, val-acc: 0.90250000, val_loss: 0.30036193
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 307.81802845, sen-loss: 26.88508105, dom-loss: 76.68335336, src-aux-loss: 107.19662911, tar-aux-loss: 97.05296624
Epoch: [12 ] train-acc: 0.91267857, dom-acc: 0.65482143, val-acc: 0.90750000, val_loss: 0.29415780
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 305.78965974, sen-loss: 26.22090176, dom-loss: 76.80301976, src-aux-loss: 106.23204434, tar-aux-loss: 96.53369576
Epoch: [13 ] train-acc: 0.91517857, dom-acc: 0.67080357, val-acc: 0.90750000, val_loss: 0.30118680
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 304.77037764, sen-loss: 25.78601234, dom-loss: 77.09428334, src-aux-loss: 105.37236667, tar-aux-loss: 96.51771653
Epoch: [14 ] train-acc: 0.91678571, dom-acc: 0.65428571, val-acc: 0.91000000, val_loss: 0.29194525
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 302.35841084, sen-loss: 25.21116731, dom-loss: 77.16690320, src-aux-loss: 104.74068660, tar-aux-loss: 95.23965496
Epoch: [15 ] train-acc: 0.91357143, dom-acc: 0.64892857, val-acc: 0.90000000, val_loss: 0.31318802
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 302.74468136, sen-loss: 24.89250159, dom-loss: 77.35233754, src-aux-loss: 103.98453492, tar-aux-loss: 96.51530701
Epoch: [16 ] train-acc: 0.91732143, dom-acc: 0.65232143, val-acc: 0.91750000, val_loss: 0.28629649
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 299.91049099, sen-loss: 24.45578510, dom-loss: 77.44941157, src-aux-loss: 103.42230356, tar-aux-loss: 94.58299088
Epoch: [17 ] train-acc: 0.92160714, dom-acc: 0.60482143, val-acc: 0.92250000, val_loss: 0.29166886
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 299.02938628, sen-loss: 24.16633166, dom-loss: 77.50042361, src-aux-loss: 102.52144277, tar-aux-loss: 94.84119010
Epoch: [18 ] train-acc: 0.92267857, dom-acc: 0.63491071, val-acc: 0.92250000, val_loss: 0.28921643
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 298.18188572, sen-loss: 23.81253012, dom-loss: 77.63200343, src-aux-loss: 101.91578561, tar-aux-loss: 94.82156742
Epoch: [19 ] train-acc: 0.92375000, dom-acc: 0.60687500, val-acc: 0.92000000, val_loss: 0.28350067
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.18274879, sen-loss: 23.41245911, dom-loss: 77.78183931, src-aux-loss: 101.45825863, tar-aux-loss: 94.53018999
Epoch: [20 ] train-acc: 0.92571429, dom-acc: 0.59098214, val-acc: 0.91750000, val_loss: 0.29233578
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 296.51348257, sen-loss: 23.17531187, dom-loss: 77.77886838, src-aux-loss: 101.26200193, tar-aux-loss: 94.29729933
Epoch: [21 ] train-acc: 0.92571429, dom-acc: 0.60455357, val-acc: 0.91250000, val_loss: 0.29792294
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 295.84010458, sen-loss: 22.81658796, dom-loss: 77.90780997, src-aux-loss: 100.32599640, tar-aux-loss: 94.78971058
Epoch: [22 ] train-acc: 0.92732143, dom-acc: 0.59062500, val-acc: 0.92250000, val_loss: 0.29250026
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 294.23917389, sen-loss: 22.44344475, dom-loss: 77.72370392, src-aux-loss: 99.89426953, tar-aux-loss: 94.17775637
Epoch: [23 ] train-acc: 0.92750000, dom-acc: 0.62169643, val-acc: 0.92750000, val_loss: 0.28356957
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 292.73297644, sen-loss: 22.35757729, dom-loss: 78.00702161, src-aux-loss: 99.27363396, tar-aux-loss: 93.09474158
Epoch: [24 ] train-acc: 0.92875000, dom-acc: 0.61035714, val-acc: 0.92500000, val_loss: 0.28366265
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_electronics_HATN.ckpt
Best Epoch: [ 19] best val accuracy: 0.00000000 best val loss: 0.28350067
Testing accuracy: 0.89083333
./work/attentions/kitchen_electronics_train_HATN.txt
./work/attentions/kitchen_electronics_test_HATN.txt
loading data...
source domain:  kitchen target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 30180
vocab-size:  78115
['great', 'love', 'easy', 'best', 'excellent', 'good', 'perfect', 'happy', 'pleased', 'loves', 'wonderful', 'nice', 'satisfied', 'amazing', 'fantastic', 'awesome', 'well', 'useful', 'favorite', 'solid', 'attractive', 'fabulous', 'perfectly', 'outstanding', 'comfortable', 'sturdy', 'quick', 'durable', 'fine', 'reasonable', 'works', 'elegant', 'impressed', 'easiest', 'incredible', 'terrific', 'exceptional', 'safe', 'simple', 'decent', 'lovely', 'pretty', 'gorgeous', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'broke', 'poorly', 'worst', 'useless', 'bad', 'horrible', 'returned', 'defective', 'wrong', 'broken', 'flimsy', 'cheap', 'awful', 'impossible', 'dissapointed', 'difficult', 'worked', 'dangerous', 'stopped', 'hard', 'worthless', 'stuck', 'ruined', 'overpriced', 'frustrating', 'worse', 'return', 'disapointed', 'sad', 'misleading', 'lasted', 'unusable', 'unhappy', 'failed', 'flawed', 'hate', 'uneven', 'rusted', 'unacceptable', 'lousy', 'ridiculous', 'ugly', 'refunded', 'messy', 'leaky', 'sucks', 'wasted', 'unfortunately', 'inconsistent', 'frustrated', 'expensive', 'overrated', 'fell']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 19856 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 413.48593616, sen-loss: 75.66148388, dom-loss: 78.86704040, src-aux-loss: 139.73736846, tar-aux-loss: 119.22004372
Epoch: [1  ] train-acc: 0.70339286, dom-acc: 0.78812500, val-acc: 0.72000000, val_loss: 0.63126123
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 386.46341467, sen-loss: 65.99176463, dom-loss: 74.64843690, src-aux-loss: 131.86117893, tar-aux-loss: 113.96203256
Epoch: [2  ] train-acc: 0.78607143, dom-acc: 0.79294643, val-acc: 0.80750000, val_loss: 0.53107369
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 363.01277041, sen-loss: 53.86649567, dom-loss: 73.10659766, src-aux-loss: 126.31262445, tar-aux-loss: 109.72705251
Epoch: [3  ] train-acc: 0.84714286, dom-acc: 0.64732143, val-acc: 0.84500000, val_loss: 0.41372076
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 344.21812224, sen-loss: 43.45400587, dom-loss: 73.09866208, src-aux-loss: 121.74791133, tar-aux-loss: 105.91754079
Epoch: [4  ] train-acc: 0.86857143, dom-acc: 0.59642857, val-acc: 0.84750000, val_loss: 0.35976806
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 333.31578279, sen-loss: 38.09061223, dom-loss: 73.65554649, src-aux-loss: 118.58412725, tar-aux-loss: 102.98549837
Epoch: [5  ] train-acc: 0.87875000, dom-acc: 0.57875000, val-acc: 0.87000000, val_loss: 0.33598584
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 328.43451643, sen-loss: 34.76540166, dom-loss: 74.65968889, src-aux-loss: 116.01282644, tar-aux-loss: 102.99660015
Epoch: [6  ] train-acc: 0.88642857, dom-acc: 0.55312500, val-acc: 0.88500000, val_loss: 0.32657403
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 322.95030141, sen-loss: 32.33624919, dom-loss: 75.02174538, src-aux-loss: 114.06964910, tar-aux-loss: 101.52265835
Epoch: [7  ] train-acc: 0.89642857, dom-acc: 0.52973214, val-acc: 0.89500000, val_loss: 0.31088701
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 320.48018146, sen-loss: 30.73600934, dom-loss: 76.11587089, src-aux-loss: 112.52846563, tar-aux-loss: 101.09983599
Epoch: [8  ] train-acc: 0.90017857, dom-acc: 0.49955357, val-acc: 0.89750000, val_loss: 0.31136259
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 319.57490373, sen-loss: 29.53775664, dom-loss: 76.61280298, src-aux-loss: 111.45405662, tar-aux-loss: 101.97028732
Epoch: [9  ] train-acc: 0.90500000, dom-acc: 0.47848214, val-acc: 0.90500000, val_loss: 0.30033010
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 315.95094657, sen-loss: 28.41366465, dom-loss: 77.19969696, src-aux-loss: 110.10372043, tar-aux-loss: 100.23386419
Epoch: [10 ] train-acc: 0.90696429, dom-acc: 0.46410714, val-acc: 0.90000000, val_loss: 0.30243257
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 312.36729884, sen-loss: 27.60762782, dom-loss: 77.51735270, src-aux-loss: 109.17272025, tar-aux-loss: 98.06959808
Epoch: [11 ] train-acc: 0.90857143, dom-acc: 0.44883929, val-acc: 0.89750000, val_loss: 0.30676305
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 312.01554585, sen-loss: 27.06670615, dom-loss: 77.67595851, src-aux-loss: 108.43032789, tar-aux-loss: 98.84255266
Epoch: [12 ] train-acc: 0.91053571, dom-acc: 0.45785714, val-acc: 0.91750000, val_loss: 0.28825521
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 309.78152037, sen-loss: 26.37343171, dom-loss: 77.75108796, src-aux-loss: 107.38562167, tar-aux-loss: 98.27138072
Epoch: [13 ] train-acc: 0.91267857, dom-acc: 0.44696429, val-acc: 0.90500000, val_loss: 0.29145011
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 308.81968236, sen-loss: 25.99721873, dom-loss: 77.84857726, src-aux-loss: 106.40399480, tar-aux-loss: 98.56989211
Epoch: [14 ] train-acc: 0.91464286, dom-acc: 0.46482143, val-acc: 0.90750000, val_loss: 0.28747427
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 306.83354449, sen-loss: 25.44921472, dom-loss: 78.00534087, src-aux-loss: 106.04892254, tar-aux-loss: 97.33006424
Epoch: [15 ] train-acc: 0.91232143, dom-acc: 0.46116071, val-acc: 0.89000000, val_loss: 0.31390426
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 305.65995836, sen-loss: 25.08530277, dom-loss: 78.03704751, src-aux-loss: 105.08359891, tar-aux-loss: 97.45400864
Epoch: [16 ] train-acc: 0.91589286, dom-acc: 0.46705357, val-acc: 0.92000000, val_loss: 0.27993214
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 303.38502383, sen-loss: 24.71305338, dom-loss: 77.91826004, src-aux-loss: 104.33744210, tar-aux-loss: 96.41626853
Epoch: [17 ] train-acc: 0.92125000, dom-acc: 0.44875000, val-acc: 0.91250000, val_loss: 0.28497437
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 303.29528952, sen-loss: 24.37354580, dom-loss: 77.95932347, src-aux-loss: 103.59147370, tar-aux-loss: 97.37094623
Epoch: [18 ] train-acc: 0.92178571, dom-acc: 0.46258929, val-acc: 0.91000000, val_loss: 0.28339434
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 301.05928588, sen-loss: 24.00094287, dom-loss: 78.04342234, src-aux-loss: 103.11305296, tar-aux-loss: 95.90186828
Epoch: [19 ] train-acc: 0.92321429, dom-acc: 0.45375000, val-acc: 0.91250000, val_loss: 0.27817452
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 300.36728024, sen-loss: 23.66127420, dom-loss: 77.95743823, src-aux-loss: 102.41166121, tar-aux-loss: 96.33690631
Epoch: [20 ] train-acc: 0.92482143, dom-acc: 0.46098214, val-acc: 0.91500000, val_loss: 0.28269920
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 298.60929060, sen-loss: 23.42461805, dom-loss: 78.20553517, src-aux-loss: 101.97916478, tar-aux-loss: 94.99997377
Epoch: [21 ] train-acc: 0.92553571, dom-acc: 0.45562500, val-acc: 0.91000000, val_loss: 0.29065874
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 297.65062642, sen-loss: 23.02667286, dom-loss: 78.13475102, src-aux-loss: 101.26488745, tar-aux-loss: 95.22431427
Epoch: [22 ] train-acc: 0.92607143, dom-acc: 0.46000000, val-acc: 0.91500000, val_loss: 0.28223926
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 298.04027152, sen-loss: 22.64873093, dom-loss: 78.19809175, src-aux-loss: 100.68425173, tar-aux-loss: 96.50919634
Epoch: [23 ] train-acc: 0.92875000, dom-acc: 0.46160714, val-acc: 0.91750000, val_loss: 0.27836531
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 295.32075834, sen-loss: 22.62246860, dom-loss: 78.16852033, src-aux-loss: 99.95632285, tar-aux-loss: 94.57344592
Epoch: [24 ] train-acc: 0.92750000, dom-acc: 0.45857143, val-acc: 0.91500000, val_loss: 0.27409887
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.10248613, sen-loss: 22.37492821, dom-loss: 78.16794920, src-aux-loss: 99.33653486, tar-aux-loss: 95.22307336
Epoch: [25 ] train-acc: 0.92892857, dom-acc: 0.44955357, val-acc: 0.92000000, val_loss: 0.27130869
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 292.80564404, sen-loss: 22.28170376, dom-loss: 78.28805429, src-aux-loss: 98.96733540, tar-aux-loss: 93.26855022
Epoch: [26 ] train-acc: 0.92750000, dom-acc: 0.45669643, val-acc: 0.91500000, val_loss: 0.26867488
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 293.97632551, sen-loss: 21.96119714, dom-loss: 78.15961945, src-aux-loss: 98.36281902, tar-aux-loss: 95.49269021
Epoch: [27 ] train-acc: 0.93178571, dom-acc: 0.45000000, val-acc: 0.91500000, val_loss: 0.27415347
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.38469958, sen-loss: 21.76497121, dom-loss: 78.29101670, src-aux-loss: 97.81098968, tar-aux-loss: 93.51772153
Epoch: [28 ] train-acc: 0.93178571, dom-acc: 0.45428571, val-acc: 0.92250000, val_loss: 0.26992357
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 291.32100224, sen-loss: 21.39051514, dom-loss: 78.35253525, src-aux-loss: 97.47648752, tar-aux-loss: 94.10146332
Epoch: [29 ] train-acc: 0.92928571, dom-acc: 0.46348214, val-acc: 0.92500000, val_loss: 0.26452002
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.30807900, sen-loss: 21.28722462, dom-loss: 78.08278370, src-aux-loss: 96.92587590, tar-aux-loss: 93.01219559
Epoch: [30 ] train-acc: 0.93375000, dom-acc: 0.45892857, val-acc: 0.91750000, val_loss: 0.26673031
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 289.83589792, sen-loss: 21.15292424, dom-loss: 78.05046678, src-aux-loss: 96.35140932, tar-aux-loss: 94.28109729
Epoch: [31 ] train-acc: 0.93267857, dom-acc: 0.43035714, val-acc: 0.91250000, val_loss: 0.28485143
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.46148753, sen-loss: 20.90796885, dom-loss: 77.99550325, src-aux-loss: 95.83286899, tar-aux-loss: 93.72514564
Epoch: [32 ] train-acc: 0.93571429, dom-acc: 0.44491071, val-acc: 0.91500000, val_loss: 0.27630258
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 286.71840692, sen-loss: 20.69988270, dom-loss: 77.80727613, src-aux-loss: 95.23114687, tar-aux-loss: 92.98010129
Epoch: [33 ] train-acc: 0.93607143, dom-acc: 0.45151786, val-acc: 0.91750000, val_loss: 0.27433440
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 287.66497326, sen-loss: 20.45876489, dom-loss: 77.97919089, src-aux-loss: 94.65622544, tar-aux-loss: 94.57079190
Epoch: [34 ] train-acc: 0.93714286, dom-acc: 0.45776786, val-acc: 0.92500000, val_loss: 0.26738748
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_video_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26452002
Testing accuracy: 0.84033333
./work/attentions/kitchen_video_train_HATN.txt
./work/attentions/kitchen_video_test_HATN.txt
loading data...
source domain:  video target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 9750
vocab-size:  98084
['best', 'funny', 'great', 'good', 'excellent', 'enjoyable', 'classic', 'amazing', 'favorite', 'beautifully', 'awesome', 'brilliant', 'love', 'perfect', 'loved', 'hilarious', 'superb', 'fantastic', 'underrated', 'informative', 'entertaining', 'funniest', 'wonderful', 'enjoyed', 'beautiful', 'solid', 'greatest', 'easy', 'brilliantly', 'nice', 'fabulous', 'sad', 'recommended', 'epic', 'un', 'clever', 'interesting', 'terrific', 'fine', 'poignant', 'incredible', 'cool', 'recommend', 'outstanding', 'masterful', 'decent', 'watchable', 'memorable', 'realistic', 'hard', 'pleasant', 'liked', 'lovely', 'magnificent', 'witty']
['worst', 'horrible', 'bad', 'terrible', 'boring', 'poor', 'disappointing', 'garbled', 'dissapointing', 'bother', 'worse', 'awful', 'harmful', 'wasted', 'wrong', 'read', 'incomplete', 'better', 'disappointed', 'unfunny', 'dull', 'cuts', 'poorly', 'annoying', 'ruined', 'atrocious', 'forgettable', 'dreadful', 'low', 'waste', 'unwatchable', 'laughable', 'uninspired', 'disgusting', 'amusing', 'uncooked', 'miserable', 'weak', 'ok', 'contrived', 'sorry', 'stupidest', 'dumbest', 'pointless', 'unrealistic', 'dissapointed', 'unlikeable', 'mediocre', 'stupid', 'pretentious', 'unbearable', 'unmemorable', 'lousy', 'ridiculous', 'vapid', 'lame', 'unconvincing']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 36180 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 416.62923241, sen-loss: 76.61726511, dom-loss: 79.39901757, src-aux-loss: 137.39146888, tar-aux-loss: 123.22148186
Epoch: [1  ] train-acc: 0.66642857, dom-acc: 0.69080357, val-acc: 0.65250000, val_loss: 0.65250838
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 386.74171996, sen-loss: 68.93357247, dom-loss: 76.45992261, src-aux-loss: 126.35019755, tar-aux-loss: 114.99802703
Epoch: [2  ] train-acc: 0.74089286, dom-acc: 0.78633929, val-acc: 0.72500000, val_loss: 0.58159345
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 365.10808969, sen-loss: 60.53264120, dom-loss: 75.62916613, src-aux-loss: 117.36170304, tar-aux-loss: 111.58458078
Epoch: [3  ] train-acc: 0.78589286, dom-acc: 0.81232143, val-acc: 0.77250000, val_loss: 0.50408065
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 347.82731533, sen-loss: 53.01377317, dom-loss: 75.17592901, src-aux-loss: 110.92299330, tar-aux-loss: 108.71461922
Epoch: [4  ] train-acc: 0.82053571, dom-acc: 0.82508929, val-acc: 0.82000000, val_loss: 0.43842283
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 334.54103827, sen-loss: 46.71296489, dom-loss: 75.05095208, src-aux-loss: 105.92048347, tar-aux-loss: 106.85663772
Epoch: [5  ] train-acc: 0.84660714, dom-acc: 0.82910714, val-acc: 0.84500000, val_loss: 0.38929650
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 325.33474970, sen-loss: 41.69242930, dom-loss: 74.81999701, src-aux-loss: 102.16695005, tar-aux-loss: 106.65537357
Epoch: [6  ] train-acc: 0.86375000, dom-acc: 0.82169643, val-acc: 0.87250000, val_loss: 0.35305786
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 316.92989230, sen-loss: 37.84365433, dom-loss: 75.04011416, src-aux-loss: 99.04253024, tar-aux-loss: 105.00359493
Epoch: [7  ] train-acc: 0.87732143, dom-acc: 0.80250000, val-acc: 0.87500000, val_loss: 0.32937396
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 310.79730844, sen-loss: 34.90374810, dom-loss: 75.05273968, src-aux-loss: 96.44563186, tar-aux-loss: 104.39518851
Epoch: [8  ] train-acc: 0.88660714, dom-acc: 0.79062500, val-acc: 0.87500000, val_loss: 0.31416169
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 306.50171804, sen-loss: 33.02138706, dom-loss: 75.23461533, src-aux-loss: 94.84370625, tar-aux-loss: 103.40201145
Epoch: [9  ] train-acc: 0.89160714, dom-acc: 0.77875000, val-acc: 0.88250000, val_loss: 0.30193090
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 302.43878078, sen-loss: 31.93064483, dom-loss: 75.59299117, src-aux-loss: 92.66744316, tar-aux-loss: 102.24770206
Epoch: [10 ] train-acc: 0.89500000, dom-acc: 0.76866071, val-acc: 0.88250000, val_loss: 0.29917616
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 299.99464440, sen-loss: 31.10187528, dom-loss: 75.66803038, src-aux-loss: 91.12569255, tar-aux-loss: 102.09904522
Epoch: [11 ] train-acc: 0.89982143, dom-acc: 0.75544643, val-acc: 0.88000000, val_loss: 0.29573312
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 296.94995332, sen-loss: 30.13529959, dom-loss: 75.77287406, src-aux-loss: 89.27458555, tar-aux-loss: 101.76719564
Epoch: [12 ] train-acc: 0.90321429, dom-acc: 0.74973214, val-acc: 0.88000000, val_loss: 0.28857750
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 294.95326591, sen-loss: 29.64181019, dom-loss: 76.02892691, src-aux-loss: 88.19502771, tar-aux-loss: 101.08750045
Epoch: [13 ] train-acc: 0.90625000, dom-acc: 0.74267857, val-acc: 0.88000000, val_loss: 0.28853780
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 292.27877951, sen-loss: 28.89779191, dom-loss: 76.26488072, src-aux-loss: 86.90983415, tar-aux-loss: 100.20627153
Epoch: [14 ] train-acc: 0.90589286, dom-acc: 0.72767857, val-acc: 0.88250000, val_loss: 0.29017127
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 291.31580710, sen-loss: 28.31362421, dom-loss: 76.25649375, src-aux-loss: 85.67767280, tar-aux-loss: 101.06801528
Epoch: [15 ] train-acc: 0.91035714, dom-acc: 0.72919643, val-acc: 0.88500000, val_loss: 0.28145885
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 288.95126295, sen-loss: 27.95474400, dom-loss: 76.63702804, src-aux-loss: 84.51890820, tar-aux-loss: 99.84058279
Epoch: [16 ] train-acc: 0.90982143, dom-acc: 0.70089286, val-acc: 0.89000000, val_loss: 0.27327469
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 287.10924959, sen-loss: 27.29522446, dom-loss: 76.72314382, src-aux-loss: 83.52757537, tar-aux-loss: 99.56330568
Epoch: [17 ] train-acc: 0.91482143, dom-acc: 0.70044643, val-acc: 0.88250000, val_loss: 0.27093846
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 285.60115194, sen-loss: 26.77075493, dom-loss: 77.02916682, src-aux-loss: 82.28390023, tar-aux-loss: 99.51733047
Epoch: [18 ] train-acc: 0.91607143, dom-acc: 0.67544643, val-acc: 0.88750000, val_loss: 0.27254152
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 283.88737273, sen-loss: 26.30071734, dom-loss: 77.00878245, src-aux-loss: 81.45832217, tar-aux-loss: 99.11955094
Epoch: [19 ] train-acc: 0.91714286, dom-acc: 0.66973214, val-acc: 0.88500000, val_loss: 0.26836976
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 283.67924166, sen-loss: 26.05663688, dom-loss: 77.35749942, src-aux-loss: 80.65277469, tar-aux-loss: 99.61233038
Epoch: [20 ] train-acc: 0.91785714, dom-acc: 0.65750000, val-acc: 0.89250000, val_loss: 0.27858850
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 280.26338649, sen-loss: 25.60641379, dom-loss: 77.11039460, src-aux-loss: 79.72514662, tar-aux-loss: 97.82142985
Epoch: [21 ] train-acc: 0.91910714, dom-acc: 0.66598214, val-acc: 0.88250000, val_loss: 0.27206427
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 279.70013332, sen-loss: 25.40460636, dom-loss: 77.25865567, src-aux-loss: 78.79927641, tar-aux-loss: 98.23759550
Epoch: [22 ] train-acc: 0.92053571, dom-acc: 0.65821429, val-acc: 0.88750000, val_loss: 0.26799858
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 277.88883829, sen-loss: 24.96157361, dom-loss: 77.18335795, src-aux-loss: 78.11581045, tar-aux-loss: 97.62809706
Epoch: [23 ] train-acc: 0.92392857, dom-acc: 0.65116071, val-acc: 0.88500000, val_loss: 0.26369503
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 279.16058779, sen-loss: 24.68198743, dom-loss: 77.61860383, src-aux-loss: 77.21666321, tar-aux-loss: 99.64333248
Epoch: [24 ] train-acc: 0.92625000, dom-acc: 0.64616071, val-acc: 0.88500000, val_loss: 0.26549202
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 274.62500072, sen-loss: 24.34012934, dom-loss: 77.54705966, src-aux-loss: 76.59440646, tar-aux-loss: 96.14340657
Epoch: [25 ] train-acc: 0.92714286, dom-acc: 0.64330357, val-acc: 0.88500000, val_loss: 0.26465848
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 275.81893253, sen-loss: 24.05918131, dom-loss: 77.61768639, src-aux-loss: 75.63358548, tar-aux-loss: 98.50847852
Epoch: [26 ] train-acc: 0.92732143, dom-acc: 0.64535714, val-acc: 0.88750000, val_loss: 0.26457787
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 273.01139355, sen-loss: 23.70014739, dom-loss: 77.55669671, src-aux-loss: 75.14015254, tar-aux-loss: 96.61439782
Epoch: [27 ] train-acc: 0.92625000, dom-acc: 0.63848214, val-acc: 0.88500000, val_loss: 0.26066428
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 271.45376396, sen-loss: 23.53901643, dom-loss: 77.59554446, src-aux-loss: 74.07514638, tar-aux-loss: 96.24405611
Epoch: [28 ] train-acc: 0.92928571, dom-acc: 0.63250000, val-acc: 0.88500000, val_loss: 0.26140329
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 271.84377551, sen-loss: 23.09441761, dom-loss: 77.65559226, src-aux-loss: 73.66890356, tar-aux-loss: 97.42486078
Epoch: [29 ] train-acc: 0.92928571, dom-acc: 0.62830357, val-acc: 0.89000000, val_loss: 0.26012874
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 270.38123441, sen-loss: 22.70320709, dom-loss: 77.54834843, src-aux-loss: 73.00477266, tar-aux-loss: 97.12490594
Epoch: [30 ] train-acc: 0.93250000, dom-acc: 0.63517857, val-acc: 0.88750000, val_loss: 0.26261750
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 269.77716684, sen-loss: 22.48785868, dom-loss: 77.60002953, src-aux-loss: 72.34205192, tar-aux-loss: 97.34722680
Epoch: [31 ] train-acc: 0.93339286, dom-acc: 0.63383929, val-acc: 0.89000000, val_loss: 0.26514226
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 268.85193539, sen-loss: 22.26232629, dom-loss: 77.77273875, src-aux-loss: 71.43526790, tar-aux-loss: 97.38160199
Epoch: [32 ] train-acc: 0.93089286, dom-acc: 0.62241071, val-acc: 0.88750000, val_loss: 0.27477434
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 266.54468822, sen-loss: 21.95792137, dom-loss: 77.48404515, src-aux-loss: 70.61662364, tar-aux-loss: 96.48609728
Epoch: [33 ] train-acc: 0.93232143, dom-acc: 0.64580357, val-acc: 0.89000000, val_loss: 0.27807072
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 264.41671681, sen-loss: 21.66853994, dom-loss: 77.58403438, src-aux-loss: 69.94270986, tar-aux-loss: 95.22143418
Epoch: [34 ] train-acc: 0.93392857, dom-acc: 0.64339286, val-acc: 0.89000000, val_loss: 0.27589503
---------------------------------------------------

Successfully load model from save path: ./work/models/video_books_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26012874
Testing accuracy: 0.87483333
./work/attentions/video_books_train_HATN.txt
./work/attentions/video_books_test_HATN.txt
loading data...
source domain:  video target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 11843
vocab-size:  91852
['best', 'funny', 'great', 'good', 'excellent', 'enjoyable', 'classic', 'favorite', 'amazing', 'love', 'loved', 'beautifully', 'perfect', 'underrated', 'brilliant', 'awesome', 'superb', 'nice', 'funniest', 'hilarious', 'informative', 'greatest', 'fantastic', 'entertaining', 'brilliantly', 'sad', 'enjoyed', 'solid', 'beautiful', 'easy', 'clever', 'epic', 'fabulous', 'fine', 'un', 'wonderful', 'terrific', 'cool', 'liked', 'magnificent', 'wonderfully', 'comedic', 'superbly', 'masterful', 'decent', 'poignant', 'incredible', 'delightful', 'pleasant', 'perfectly', 'outstanding', 'funnier', 'riveting', 'happy', 'recommend', 'romantic']
['worst', 'horrible', 'bad', 'terrible', 'boring', 'poor', 'disappointing', 'garbled', 'bother', 'worse', 'dissapointing', 'awful', 'average', 'read', 'wasted', 'wrong', 'better', 'disappointed', 'unfunny', 'dull', 'poorly', 'annoying', 'unwatchable', 'forgettable', 'ruined', 'ok', 'dissapointed', 'atrocious', 'skip', 'dreadful', 'unnecessary', 'needless', 'uninspired', 'disgusting', 'uncooked', 'pretentious', 'laughable', 'pointless', 'unbelievable', 'contrived', 'saves', 'dumbest', 'sorry', 'stupidest', 'cheesy', 'low', 'hard', 'unrealistic', 'waste', 'unlikeable', 'mediocre', 'amusing', 'sappy', 'miserable', 'weak', 'unmemorable', 'lousy', 'unbearable', 'vapid', 'lame', 'stupid']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 36180 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 415.33608603, sen-loss: 76.71485114, dom-loss: 79.96215230, src-aux-loss: 138.70530748, tar-aux-loss: 119.95377421
Epoch: [1  ] train-acc: 0.66160714, dom-acc: 0.46633929, val-acc: 0.66000000, val_loss: 0.65357232
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 386.63554788, sen-loss: 69.19996095, dom-loss: 78.64747739, src-aux-loss: 128.31608659, tar-aux-loss: 110.47202206
Epoch: [2  ] train-acc: 0.76214286, dom-acc: 0.47107143, val-acc: 0.73250000, val_loss: 0.58033723
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 365.11254239, sen-loss: 60.69971046, dom-loss: 78.56238490, src-aux-loss: 120.48638666, tar-aux-loss: 105.36405897
Epoch: [3  ] train-acc: 0.79714286, dom-acc: 0.48241071, val-acc: 0.81250000, val_loss: 0.50211716
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 346.97973061, sen-loss: 52.67249563, dom-loss: 78.47781849, src-aux-loss: 112.86895066, tar-aux-loss: 102.96046603
Epoch: [4  ] train-acc: 0.82428571, dom-acc: 0.49116071, val-acc: 0.81000000, val_loss: 0.43490264
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 331.08282614, sen-loss: 46.30176607, dom-loss: 78.44250059, src-aux-loss: 106.71585435, tar-aux-loss: 99.62270427
Epoch: [5  ] train-acc: 0.84464286, dom-acc: 0.52125000, val-acc: 0.85250000, val_loss: 0.38562578
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 321.60299158, sen-loss: 41.49219730, dom-loss: 78.34717661, src-aux-loss: 102.47572619, tar-aux-loss: 99.28789139
Epoch: [6  ] train-acc: 0.86267857, dom-acc: 0.59464286, val-acc: 0.87250000, val_loss: 0.35480371
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 313.96598768, sen-loss: 37.98267759, dom-loss: 78.29244870, src-aux-loss: 99.31353033, tar-aux-loss: 98.37733042
Epoch: [7  ] train-acc: 0.87625000, dom-acc: 0.61258929, val-acc: 0.87500000, val_loss: 0.33061391
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 306.99115562, sen-loss: 35.12760666, dom-loss: 78.23020685, src-aux-loss: 96.94657308, tar-aux-loss: 96.68676931
Epoch: [8  ] train-acc: 0.88964286, dom-acc: 0.62857143, val-acc: 0.87000000, val_loss: 0.31606776
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 302.22880411, sen-loss: 33.23127184, dom-loss: 78.30031943, src-aux-loss: 95.23886079, tar-aux-loss: 95.45835114
Epoch: [9  ] train-acc: 0.89178571, dom-acc: 0.62098214, val-acc: 0.87500000, val_loss: 0.30649808
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 299.69026709, sen-loss: 32.02219000, dom-loss: 78.17556125, src-aux-loss: 93.62540787, tar-aux-loss: 95.86710894
Epoch: [10 ] train-acc: 0.89714286, dom-acc: 0.62535714, val-acc: 0.88500000, val_loss: 0.30146584
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 295.12328196, sen-loss: 31.00193827, dom-loss: 78.16967195, src-aux-loss: 91.51980686, tar-aux-loss: 94.43186396
Epoch: [11 ] train-acc: 0.89964286, dom-acc: 0.62714286, val-acc: 0.87750000, val_loss: 0.30267102
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 292.32208848, sen-loss: 30.12149274, dom-loss: 78.07507020, src-aux-loss: 89.78516167, tar-aux-loss: 94.34036487
Epoch: [12 ] train-acc: 0.90660714, dom-acc: 0.64580357, val-acc: 0.88250000, val_loss: 0.29390404
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 288.84024453, sen-loss: 29.40941373, dom-loss: 78.14736426, src-aux-loss: 88.13778156, tar-aux-loss: 93.14568454
Epoch: [13 ] train-acc: 0.90678571, dom-acc: 0.63642857, val-acc: 0.88500000, val_loss: 0.29580766
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 286.75262523, sen-loss: 28.65721402, dom-loss: 78.08219647, src-aux-loss: 86.53804982, tar-aux-loss: 93.47516531
Epoch: [14 ] train-acc: 0.90857143, dom-acc: 0.62571429, val-acc: 0.88500000, val_loss: 0.29573745
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 284.64254451, sen-loss: 28.25538163, dom-loss: 77.93381375, src-aux-loss: 85.64695793, tar-aux-loss: 92.80639076
Epoch: [15 ] train-acc: 0.91339286, dom-acc: 0.65348214, val-acc: 0.89000000, val_loss: 0.28676423
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 281.47983289, sen-loss: 27.72832676, dom-loss: 78.02337909, src-aux-loss: 84.21304724, tar-aux-loss: 91.51508147
Epoch: [16 ] train-acc: 0.91375000, dom-acc: 0.62589286, val-acc: 0.89000000, val_loss: 0.28059262
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 280.81842351, sen-loss: 27.19422483, dom-loss: 77.98826212, src-aux-loss: 83.24322891, tar-aux-loss: 92.39270836
Epoch: [17 ] train-acc: 0.91392857, dom-acc: 0.62919643, val-acc: 0.88500000, val_loss: 0.27781066
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 277.21153545, sen-loss: 26.68401300, dom-loss: 77.97985071, src-aux-loss: 82.06723392, tar-aux-loss: 90.48043650
Epoch: [18 ] train-acc: 0.91821429, dom-acc: 0.62839286, val-acc: 0.89000000, val_loss: 0.28188094
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 276.97003603, sen-loss: 26.21561886, dom-loss: 78.05353588, src-aux-loss: 81.09763962, tar-aux-loss: 91.60324097
Epoch: [19 ] train-acc: 0.91857143, dom-acc: 0.61125000, val-acc: 0.89250000, val_loss: 0.27482072
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 275.43899226, sen-loss: 25.91148744, dom-loss: 77.93940437, src-aux-loss: 80.23636812, tar-aux-loss: 91.35173059
Epoch: [20 ] train-acc: 0.92160714, dom-acc: 0.61705357, val-acc: 0.89000000, val_loss: 0.28268027
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 272.11713457, sen-loss: 25.44355261, dom-loss: 77.96409029, src-aux-loss: 78.99168566, tar-aux-loss: 89.71780425
Epoch: [21 ] train-acc: 0.92250000, dom-acc: 0.61383929, val-acc: 0.89500000, val_loss: 0.28201181
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 271.34724522, sen-loss: 25.24801672, dom-loss: 77.89817590, src-aux-loss: 78.04938275, tar-aux-loss: 90.15167090
Epoch: [22 ] train-acc: 0.92553571, dom-acc: 0.63910714, val-acc: 0.89750000, val_loss: 0.27766502
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 268.47647262, sen-loss: 24.79407304, dom-loss: 77.82034326, src-aux-loss: 77.19770268, tar-aux-loss: 88.66435462
Epoch: [23 ] train-acc: 0.92571429, dom-acc: 0.64035714, val-acc: 0.89500000, val_loss: 0.27108774
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 269.74680352, sen-loss: 24.57878322, dom-loss: 77.91566277, src-aux-loss: 76.37303430, tar-aux-loss: 90.87932330
Epoch: [24 ] train-acc: 0.92750000, dom-acc: 0.61071429, val-acc: 0.89250000, val_loss: 0.27379647
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 268.80614614, sen-loss: 24.27211298, dom-loss: 77.86534172, src-aux-loss: 75.87664813, tar-aux-loss: 90.79204220
Epoch: [25 ] train-acc: 0.92785714, dom-acc: 0.60857143, val-acc: 0.88750000, val_loss: 0.27346575
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 265.79272914, sen-loss: 23.97948500, dom-loss: 77.82316750, src-aux-loss: 74.83724463, tar-aux-loss: 89.15283114
Epoch: [26 ] train-acc: 0.92982143, dom-acc: 0.62178571, val-acc: 0.88750000, val_loss: 0.27408528
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 264.63389099, sen-loss: 23.63735256, dom-loss: 77.89573425, src-aux-loss: 74.03696847, tar-aux-loss: 89.06383479
Epoch: [27 ] train-acc: 0.93053571, dom-acc: 0.60000000, val-acc: 0.89000000, val_loss: 0.27232298
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 263.37543750, sen-loss: 23.46005375, dom-loss: 77.83044100, src-aux-loss: 73.00746164, tar-aux-loss: 89.07748181
Epoch: [28 ] train-acc: 0.92928571, dom-acc: 0.60857143, val-acc: 0.88500000, val_loss: 0.27188882
---------------------------------------------------

Successfully load model from save path: ./work/models/video_dvd_HATN.ckpt
Best Epoch: [ 23] best val accuracy: 0.00000000 best val loss: 0.27108774
Testing accuracy: 0.87600000
./work/attentions/video_dvd_train_HATN.txt
./work/attentions/video_dvd_test_HATN.txt
loading data...
source domain:  video target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 17009
vocab-size:  83059
['best', 'great', 'funny', 'good', 'excellent', 'enjoyable', 'amazing', 'classic', 'beautifully', 'favorite', 'love', 'superb', 'loved', 'nice', 'fantastic', 'awesome', 'perfect', 'underrated', 'brilliant', 'hilarious', 'solid', 'informative', 'greatest', 'funniest', 'brilliantly', 'beautiful', 'hard', 'fine', 'enjoyed', 'entertaining', 'easy', 'terrific', 'immortal', 'epic', 'clever', 'sad', 'un', 'fabulous', 'wonderful', 'perfectly', 'decent', 'magnificent', 'incredible', 'cool', 'wonderfully', 'recommend', 'superbly', 'pleasant', 'masterful', 'liked', 'surprisingly', 'believable', 'outstanding', 'recommended', 'pretty', 'deserved', 'happy']
['worst', 'horrible', 'terrible', 'bad', 'boring', 'poor', 'garbled', 'dissapointing', 'bother', 'disappointing', 'awful', 'worse', 'swallowed', 'average', 'wrong', 'wasted', 'frustrating', 'disappointed', 'dull', 'cuts', 'better', 'unfunny', 'poorly', 'put', 'annoying', 'ok', 'asleep', 'unbelievable', 'dreadful', 'low', 'unwatchable', 'unnecessary', 'needless', 'dissapointed', 'forgettable', 'uninspired', 'watchable', 'uncooked', 'predictable', 'ruined', 'atrocious', 'contrived', 'disgusting', 'horribly', 'sorry', 'stupidest', 'pointless', 'unrealistic', 'unlikeable', 'mediocre', 'miserable', 'unmemorable', 'laughable', 'lousy', 'wasting', 'cheesy', 'unbearable', 'angst', 'blah', 'amusing']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 36180 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 412.39841866, sen-loss: 76.73644507, dom-loss: 78.35688430, src-aux-loss: 137.35141981, tar-aux-loss: 119.95366949
Epoch: [1  ] train-acc: 0.66035714, dom-acc: 0.80517857, val-acc: 0.65250000, val_loss: 0.65442514
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 382.25943041, sen-loss: 69.41369641, dom-loss: 74.36381984, src-aux-loss: 127.20072275, tar-aux-loss: 111.28119296
Epoch: [2  ] train-acc: 0.75678571, dom-acc: 0.84053571, val-acc: 0.72500000, val_loss: 0.58526760
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.02008510, sen-loss: 61.17437467, dom-loss: 73.04639065, src-aux-loss: 119.70545757, tar-aux-loss: 106.09386396
Epoch: [3  ] train-acc: 0.79500000, dom-acc: 0.76625000, val-acc: 0.78750000, val_loss: 0.50867367
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 342.61746144, sen-loss: 53.23018640, dom-loss: 73.32965100, src-aux-loss: 112.93352365, tar-aux-loss: 103.12410045
Epoch: [4  ] train-acc: 0.82160714, dom-acc: 0.62732143, val-acc: 0.81500000, val_loss: 0.44143870
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 329.81412220, sen-loss: 46.69746619, dom-loss: 74.43087071, src-aux-loss: 107.09600228, tar-aux-loss: 101.58978188
Epoch: [5  ] train-acc: 0.84714286, dom-acc: 0.57803571, val-acc: 0.85250000, val_loss: 0.39210543
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 322.45480323, sen-loss: 42.00036323, dom-loss: 75.64322710, src-aux-loss: 102.67429519, tar-aux-loss: 102.13691670
Epoch: [6  ] train-acc: 0.86142857, dom-acc: 0.57446429, val-acc: 0.86000000, val_loss: 0.35930759
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 314.71363330, sen-loss: 38.64652105, dom-loss: 76.37279797, src-aux-loss: 99.48663527, tar-aux-loss: 100.20767933
Epoch: [7  ] train-acc: 0.87267857, dom-acc: 0.62750000, val-acc: 0.87250000, val_loss: 0.33497480
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 309.14479542, sen-loss: 35.90730427, dom-loss: 77.09214532, src-aux-loss: 97.00845063, tar-aux-loss: 99.13689399
Epoch: [8  ] train-acc: 0.88392857, dom-acc: 0.61892857, val-acc: 0.86500000, val_loss: 0.32213965
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 304.22024322, sen-loss: 33.97519692, dom-loss: 77.81602263, src-aux-loss: 94.98630577, tar-aux-loss: 97.44271803
Epoch: [9  ] train-acc: 0.89017857, dom-acc: 0.60982143, val-acc: 0.88500000, val_loss: 0.30761603
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 301.78885913, sen-loss: 32.84871106, dom-loss: 78.47023928, src-aux-loss: 93.22081709, tar-aux-loss: 97.24909228
Epoch: [10 ] train-acc: 0.89589286, dom-acc: 0.54357143, val-acc: 0.88500000, val_loss: 0.30297402
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 298.11357737, sen-loss: 31.85982323, dom-loss: 78.73472613, src-aux-loss: 91.43296546, tar-aux-loss: 96.08606344
Epoch: [11 ] train-acc: 0.89642857, dom-acc: 0.51946429, val-acc: 0.88500000, val_loss: 0.29655883
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 295.77266622, sen-loss: 30.75738728, dom-loss: 78.84600961, src-aux-loss: 89.89372629, tar-aux-loss: 96.27554184
Epoch: [12 ] train-acc: 0.89892857, dom-acc: 0.52241071, val-acc: 0.89250000, val_loss: 0.29456609
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 293.52013373, sen-loss: 30.19818079, dom-loss: 78.82795775, src-aux-loss: 88.71877998, tar-aux-loss: 95.77521342
Epoch: [13 ] train-acc: 0.90267857, dom-acc: 0.50705357, val-acc: 0.87500000, val_loss: 0.30283695
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 290.60654712, sen-loss: 29.33797558, dom-loss: 78.83783466, src-aux-loss: 87.02828777, tar-aux-loss: 95.40244973
Epoch: [14 ] train-acc: 0.90517857, dom-acc: 0.46357143, val-acc: 0.89250000, val_loss: 0.29440859
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 289.66841626, sen-loss: 28.91494767, dom-loss: 78.53571153, src-aux-loss: 86.01748365, tar-aux-loss: 96.20027554
Epoch: [15 ] train-acc: 0.90696429, dom-acc: 0.46910714, val-acc: 0.89250000, val_loss: 0.28277361
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 286.66992450, sen-loss: 28.43207328, dom-loss: 78.25369281, src-aux-loss: 84.70635423, tar-aux-loss: 95.27780455
Epoch: [16 ] train-acc: 0.90928571, dom-acc: 0.48812500, val-acc: 0.88750000, val_loss: 0.27712792
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 284.89393616, sen-loss: 27.82655773, dom-loss: 77.94725436, src-aux-loss: 83.94255239, tar-aux-loss: 95.17757165
Epoch: [17 ] train-acc: 0.91071429, dom-acc: 0.47267857, val-acc: 0.88750000, val_loss: 0.27572489
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 280.94924879, sen-loss: 27.27358814, dom-loss: 77.73192036, src-aux-loss: 82.53411144, tar-aux-loss: 93.40963042
Epoch: [18 ] train-acc: 0.91232143, dom-acc: 0.48017857, val-acc: 0.89000000, val_loss: 0.27696577
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 280.36424208, sen-loss: 26.91530752, dom-loss: 77.42645216, src-aux-loss: 81.68995565, tar-aux-loss: 94.33252782
Epoch: [19 ] train-acc: 0.91375000, dom-acc: 0.46946429, val-acc: 0.89250000, val_loss: 0.27050671
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 278.62207699, sen-loss: 26.54331547, dom-loss: 77.45836037, src-aux-loss: 80.79088211, tar-aux-loss: 93.82951927
Epoch: [20 ] train-acc: 0.91464286, dom-acc: 0.46937500, val-acc: 0.89000000, val_loss: 0.28130439
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 277.32104564, sen-loss: 26.10808223, dom-loss: 77.41361117, src-aux-loss: 79.77140778, tar-aux-loss: 94.02794492
Epoch: [21 ] train-acc: 0.91535714, dom-acc: 0.48526786, val-acc: 0.88000000, val_loss: 0.27679530
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 275.60535264, sen-loss: 25.87375962, dom-loss: 77.43177956, src-aux-loss: 78.73659739, tar-aux-loss: 93.56321466
Epoch: [22 ] train-acc: 0.91803571, dom-acc: 0.47267857, val-acc: 0.88250000, val_loss: 0.27112937
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 273.92174816, sen-loss: 25.38577567, dom-loss: 77.59275532, src-aux-loss: 78.05291089, tar-aux-loss: 92.89030552
Epoch: [23 ] train-acc: 0.92089286, dom-acc: 0.48883929, val-acc: 0.88500000, val_loss: 0.26815581
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 274.24120569, sen-loss: 25.16687275, dom-loss: 77.70962977, src-aux-loss: 77.00382128, tar-aux-loss: 94.36088067
Epoch: [24 ] train-acc: 0.92160714, dom-acc: 0.47892857, val-acc: 0.88250000, val_loss: 0.26821584
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 271.94022655, sen-loss: 24.80833607, dom-loss: 77.90195024, src-aux-loss: 76.51031631, tar-aux-loss: 92.71962225
Epoch: [25 ] train-acc: 0.92285714, dom-acc: 0.46142857, val-acc: 0.88500000, val_loss: 0.26777554
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 270.63656735, sen-loss: 24.62432628, dom-loss: 78.06945616, src-aux-loss: 75.70035419, tar-aux-loss: 92.24243164
Epoch: [26 ] train-acc: 0.92410714, dom-acc: 0.45151786, val-acc: 0.88000000, val_loss: 0.26886275
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 269.30271649, sen-loss: 24.23579536, dom-loss: 78.14822787, src-aux-loss: 74.86925825, tar-aux-loss: 92.04943484
Epoch: [27 ] train-acc: 0.92500000, dom-acc: 0.43125000, val-acc: 0.88250000, val_loss: 0.26495442
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 267.90451789, sen-loss: 23.91487595, dom-loss: 78.21971226, src-aux-loss: 73.81031057, tar-aux-loss: 91.95961928
Epoch: [28 ] train-acc: 0.92500000, dom-acc: 0.42732143, val-acc: 0.88500000, val_loss: 0.26427186
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 267.27342010, sen-loss: 23.55206001, dom-loss: 78.40242505, src-aux-loss: 73.68958557, tar-aux-loss: 91.62935072
Epoch: [29 ] train-acc: 0.92625000, dom-acc: 0.43812500, val-acc: 0.88750000, val_loss: 0.26416385
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 266.84101605, sen-loss: 23.17779021, dom-loss: 78.60750544, src-aux-loss: 72.53029934, tar-aux-loss: 92.52542216
Epoch: [30 ] train-acc: 0.92714286, dom-acc: 0.41866071, val-acc: 0.88250000, val_loss: 0.26622283
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 265.56658912, sen-loss: 22.98527329, dom-loss: 78.56413561, src-aux-loss: 72.15588629, tar-aux-loss: 91.86129504
Epoch: [31 ] train-acc: 0.93000000, dom-acc: 0.42776786, val-acc: 0.88500000, val_loss: 0.26592049
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 263.79532266, sen-loss: 22.71003865, dom-loss: 78.53513706, src-aux-loss: 71.55433810, tar-aux-loss: 90.99580860
Epoch: [32 ] train-acc: 0.92964286, dom-acc: 0.41312500, val-acc: 0.88750000, val_loss: 0.28009030
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 264.32825494, sen-loss: 22.48841060, dom-loss: 78.38788158, src-aux-loss: 70.67343476, tar-aux-loss: 92.77852786
Epoch: [33 ] train-acc: 0.93053571, dom-acc: 0.42348214, val-acc: 0.89000000, val_loss: 0.27238479
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 260.02850366, sen-loss: 22.16130716, dom-loss: 78.26543719, src-aux-loss: 69.88968095, tar-aux-loss: 89.71207780
Epoch: [34 ] train-acc: 0.93000000, dom-acc: 0.45678571, val-acc: 0.89250000, val_loss: 0.28216282
---------------------------------------------------

Successfully load model from save path: ./work/models/video_electronics_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26416385
Testing accuracy: 0.85850000
./work/attentions/video_electronics_train_HATN.txt
./work/attentions/video_electronics_test_HATN.txt
loading data...
source domain:  video target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 13856
vocab-size:  78115
['best', 'great', 'funny', 'good', 'excellent', 'enjoyable', 'classic', 'favorite', 'amazing', 'love', 'beautifully', 'superb', 'perfect', 'fantastic', 'awesome', 'loved', 'nice', 'brilliant', 'hilarious', 'underrated', 'informative', 'funniest', 'beautiful', 'solid', 'enjoyed', 'fine', 'hard', 'entertaining', 'greatest', 'easy', 'brilliantly', 'fabulous', 'clever', 'wonderful', 'immortal', 'epic', 'un', 'cool', 'terrific', 'sad', 'decent', 'watchable', 'magnificent', 'recommend', 'superbly', 'pleasant', 'perfectly', 'outstanding', 'recommended', 'masterful', 'surprisingly', 'incredible', 'wonderfully', 'comedic', 'lovely', 'riveting', 'poignant', 'happy']
['worst', 'horrible', 'terrible', 'bad', 'boring', 'poor', 'disappointing', 'worse', 'garbled', 'dissapointing', 'bother', 'awful', 'average', 'wasted', 'wrong', 'read', 'dull', 'disappointed', 'unfunny', 'poorly', 'annoying', 'unwatchable', 'dreadful', 'atrocious', 'ok', 'low', 'dissapointed', 'forgettable', 'unnecessary', 'needless', 'uninspired', 'unbelievable', 'disgusting', 'uncooked', 'asleep', 'predictable', 'waste', 'ruined', 'contrived', 'lackluster', 'dumbest', 'laughable', 'stupidest', 'lousy', 'unrealistic', 'unlikeable', 'mediocre', 'amusing', 'sappy', 'miserable', 'pretentious', 'weak', 'unmemorable', 'pointless', 'empty', 'unbearable', 'vapid']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 36180 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 407.65716863, sen-loss: 76.59703660, dom-loss: 78.20302522, src-aux-loss: 141.16539323, tar-aux-loss: 111.69171470
Epoch: [1  ] train-acc: 0.66267857, dom-acc: 0.83098214, val-acc: 0.64500000, val_loss: 0.65266061
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 374.87959909, sen-loss: 69.06998754, dom-loss: 73.76784986, src-aux-loss: 131.02169943, tar-aux-loss: 101.02006137
Epoch: [2  ] train-acc: 0.75714286, dom-acc: 0.89875000, val-acc: 0.72750000, val_loss: 0.58155918
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 352.89300203, sen-loss: 60.57176992, dom-loss: 72.49634236, src-aux-loss: 122.49163485, tar-aux-loss: 97.33325589
Epoch: [3  ] train-acc: 0.79625000, dom-acc: 0.89455357, val-acc: 0.80750000, val_loss: 0.50526619
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 333.21409321, sen-loss: 52.59400901, dom-loss: 72.70099568, src-aux-loss: 114.88093823, tar-aux-loss: 93.03814930
Epoch: [4  ] train-acc: 0.82875000, dom-acc: 0.85714286, val-acc: 0.82000000, val_loss: 0.43715009
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 319.70917392, sen-loss: 46.05080062, dom-loss: 73.57299429, src-aux-loss: 108.58985758, tar-aux-loss: 91.49552202
Epoch: [5  ] train-acc: 0.83892857, dom-acc: 0.85741071, val-acc: 0.84000000, val_loss: 0.38782266
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 310.95988250, sen-loss: 41.50751056, dom-loss: 74.30816215, src-aux-loss: 104.10291719, tar-aux-loss: 91.04129255
Epoch: [6  ] train-acc: 0.86321429, dom-acc: 0.80383929, val-acc: 0.85250000, val_loss: 0.35815069
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 304.27371860, sen-loss: 38.20193990, dom-loss: 75.36603504, src-aux-loss: 100.68177325, tar-aux-loss: 90.02396828
Epoch: [7  ] train-acc: 0.87732143, dom-acc: 0.73919643, val-acc: 0.85750000, val_loss: 0.33863166
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 298.13436460, sen-loss: 35.43477044, dom-loss: 76.27718222, src-aux-loss: 98.28445363, tar-aux-loss: 88.13795787
Epoch: [8  ] train-acc: 0.88607143, dom-acc: 0.69973214, val-acc: 0.87500000, val_loss: 0.31515902
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 294.70155907, sen-loss: 33.50345158, dom-loss: 77.31759471, src-aux-loss: 96.93852949, tar-aux-loss: 86.94198287
Epoch: [9  ] train-acc: 0.89071429, dom-acc: 0.65125000, val-acc: 0.88250000, val_loss: 0.30279839
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 292.93271661, sen-loss: 32.31482556, dom-loss: 78.31664133, src-aux-loss: 94.68954754, tar-aux-loss: 87.61170375
Epoch: [10 ] train-acc: 0.89625000, dom-acc: 0.63437500, val-acc: 0.87000000, val_loss: 0.29705080
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 289.80451918, sen-loss: 31.36401333, dom-loss: 78.70768780, src-aux-loss: 93.35168290, tar-aux-loss: 86.38113654
Epoch: [11 ] train-acc: 0.89910714, dom-acc: 0.60812500, val-acc: 0.87500000, val_loss: 0.29644409
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 286.37708044, sen-loss: 30.40140173, dom-loss: 78.94804353, src-aux-loss: 91.09927267, tar-aux-loss: 85.92836457
Epoch: [12 ] train-acc: 0.90250000, dom-acc: 0.59285714, val-acc: 0.88500000, val_loss: 0.28936255
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 287.45385075, sen-loss: 29.74489372, dom-loss: 79.35757232, src-aux-loss: 89.93251789, tar-aux-loss: 88.41886771
Epoch: [13 ] train-acc: 0.90250000, dom-acc: 0.58044643, val-acc: 0.88000000, val_loss: 0.29398784
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 281.82094121, sen-loss: 29.05340116, dom-loss: 79.29914045, src-aux-loss: 88.65227735, tar-aux-loss: 84.81612164
Epoch: [14 ] train-acc: 0.90571429, dom-acc: 0.57732143, val-acc: 0.88250000, val_loss: 0.29005757
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 279.70980191, sen-loss: 28.50324898, dom-loss: 79.19179517, src-aux-loss: 87.42455834, tar-aux-loss: 84.59019911
Epoch: [15 ] train-acc: 0.91142857, dom-acc: 0.58732143, val-acc: 0.89250000, val_loss: 0.28173375
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 276.89567947, sen-loss: 27.97125541, dom-loss: 78.92833614, src-aux-loss: 86.14384103, tar-aux-loss: 83.85224724
Epoch: [16 ] train-acc: 0.90857143, dom-acc: 0.56705357, val-acc: 0.89250000, val_loss: 0.27694303
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 275.60590124, sen-loss: 27.46561133, dom-loss: 78.65831405, src-aux-loss: 85.13266981, tar-aux-loss: 84.34930617
Epoch: [17 ] train-acc: 0.91357143, dom-acc: 0.58803571, val-acc: 0.89250000, val_loss: 0.27407241
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 274.11431181, sen-loss: 26.92073163, dom-loss: 78.48023206, src-aux-loss: 84.19409007, tar-aux-loss: 84.51925915
Epoch: [18 ] train-acc: 0.91482143, dom-acc: 0.56642857, val-acc: 0.90000000, val_loss: 0.27683949
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 272.40643930, sen-loss: 26.47651623, dom-loss: 78.19707334, src-aux-loss: 83.09327137, tar-aux-loss: 84.63957942
Epoch: [19 ] train-acc: 0.91857143, dom-acc: 0.55035714, val-acc: 0.89750000, val_loss: 0.26975092
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 268.81089139, sen-loss: 26.21634936, dom-loss: 78.02685070, src-aux-loss: 82.35769111, tar-aux-loss: 82.21000034
Epoch: [20 ] train-acc: 0.91982143, dom-acc: 0.57008929, val-acc: 0.89500000, val_loss: 0.27488342
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 267.64144349, sen-loss: 25.69882395, dom-loss: 77.81778747, src-aux-loss: 81.30645281, tar-aux-loss: 82.81837845
Epoch: [21 ] train-acc: 0.92017857, dom-acc: 0.57133929, val-acc: 0.89500000, val_loss: 0.27164581
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 266.76970530, sen-loss: 25.56432842, dom-loss: 77.94432354, src-aux-loss: 80.40284866, tar-aux-loss: 82.85820460
Epoch: [22 ] train-acc: 0.92160714, dom-acc: 0.56714286, val-acc: 0.89750000, val_loss: 0.27218318
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 264.60490966, sen-loss: 25.08112981, dom-loss: 77.88657784, src-aux-loss: 79.58823088, tar-aux-loss: 82.04897076
Epoch: [23 ] train-acc: 0.92196429, dom-acc: 0.56205357, val-acc: 0.89250000, val_loss: 0.26608968
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 264.78374863, sen-loss: 24.82897557, dom-loss: 77.93668890, src-aux-loss: 78.67389268, tar-aux-loss: 83.34419048
Epoch: [24 ] train-acc: 0.92303571, dom-acc: 0.55017857, val-acc: 0.89750000, val_loss: 0.26386145
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 261.91300511, sen-loss: 24.41974947, dom-loss: 78.04484755, src-aux-loss: 77.99035427, tar-aux-loss: 81.45805418
Epoch: [25 ] train-acc: 0.92535714, dom-acc: 0.53125000, val-acc: 0.90000000, val_loss: 0.26340312
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 262.80662227, sen-loss: 24.18324739, dom-loss: 78.14730895, src-aux-loss: 77.14160812, tar-aux-loss: 83.33445662
Epoch: [26 ] train-acc: 0.92607143, dom-acc: 0.52571429, val-acc: 0.89250000, val_loss: 0.26300257
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 260.46553230, sen-loss: 23.75399336, dom-loss: 78.33361590, src-aux-loss: 76.50876659, tar-aux-loss: 81.86915815
Epoch: [27 ] train-acc: 0.92642857, dom-acc: 0.51187500, val-acc: 0.90250000, val_loss: 0.25965527
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 257.92606199, sen-loss: 23.52585140, dom-loss: 78.47388172, src-aux-loss: 75.47045285, tar-aux-loss: 80.45587522
Epoch: [28 ] train-acc: 0.92803571, dom-acc: 0.49223214, val-acc: 0.90000000, val_loss: 0.25907797
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 257.13066697, sen-loss: 23.17514547, dom-loss: 78.52175200, src-aux-loss: 74.83040878, tar-aux-loss: 80.60336143
Epoch: [29 ] train-acc: 0.92982143, dom-acc: 0.47348214, val-acc: 0.90500000, val_loss: 0.25865501
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 258.03797376, sen-loss: 22.81851707, dom-loss: 78.76670200, src-aux-loss: 73.98541349, tar-aux-loss: 82.46733975
Epoch: [30 ] train-acc: 0.93107143, dom-acc: 0.46598214, val-acc: 0.90750000, val_loss: 0.25673240
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 257.26055717, sen-loss: 22.50196512, dom-loss: 78.78998357, src-aux-loss: 73.37849811, tar-aux-loss: 82.59011060
Epoch: [31 ] train-acc: 0.93232143, dom-acc: 0.47767857, val-acc: 0.90250000, val_loss: 0.25637469
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 254.74468911, sen-loss: 22.36152484, dom-loss: 78.79409516, src-aux-loss: 72.40255991, tar-aux-loss: 81.18650961
Epoch: [32 ] train-acc: 0.92964286, dom-acc: 0.42035714, val-acc: 0.89250000, val_loss: 0.26742262
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 253.06209123, sen-loss: 21.94665001, dom-loss: 78.83114463, src-aux-loss: 71.67005026, tar-aux-loss: 80.61424565
Epoch: [33 ] train-acc: 0.93160714, dom-acc: 0.44571429, val-acc: 0.89000000, val_loss: 0.26381841
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 251.05728662, sen-loss: 21.62907221, dom-loss: 78.71910965, src-aux-loss: 70.84113416, tar-aux-loss: 79.86797115
Epoch: [34 ] train-acc: 0.93053571, dom-acc: 0.48660714, val-acc: 0.89500000, val_loss: 0.26962352
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 250.18541920, sen-loss: 21.42769007, dom-loss: 78.66853386, src-aux-loss: 70.06025642, tar-aux-loss: 80.02894062
Epoch: [35 ] train-acc: 0.93839286, dom-acc: 0.46687500, val-acc: 0.90000000, val_loss: 0.25523987
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 249.00100899, sen-loss: 21.16544203, dom-loss: 78.42611974, src-aux-loss: 69.08721498, tar-aux-loss: 80.32223356
Epoch: [36 ] train-acc: 0.93660714, dom-acc: 0.45937500, val-acc: 0.90750000, val_loss: 0.25522316
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 247.64774430, sen-loss: 20.86399165, dom-loss: 78.13889891, src-aux-loss: 68.56240019, tar-aux-loss: 80.08245230
Epoch: [37 ] train-acc: 0.93464286, dom-acc: 0.50080357, val-acc: 0.89250000, val_loss: 0.25553933
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 247.11396539, sen-loss: 20.54636454, dom-loss: 77.89573175, src-aux-loss: 67.78564602, tar-aux-loss: 80.88622308
Epoch: [38 ] train-acc: 0.94071429, dom-acc: 0.49267857, val-acc: 0.90000000, val_loss: 0.25429532
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 246.16419339, sen-loss: 20.27698547, dom-loss: 77.72625649, src-aux-loss: 67.28115702, tar-aux-loss: 80.87979585
Epoch: [39 ] train-acc: 0.94107143, dom-acc: 0.58098214, val-acc: 0.90250000, val_loss: 0.25529057
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 242.48393416, sen-loss: 19.99300407, dom-loss: 77.41853565, src-aux-loss: 66.11200553, tar-aux-loss: 78.96038705
Epoch: [40 ] train-acc: 0.94160714, dom-acc: 0.54589286, val-acc: 0.90000000, val_loss: 0.25855440
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 241.96937406, sen-loss: 19.66236661, dom-loss: 77.37369460, src-aux-loss: 65.56879082, tar-aux-loss: 79.36451936
Epoch: [41 ] train-acc: 0.94446429, dom-acc: 0.58125000, val-acc: 0.89750000, val_loss: 0.25648397
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 240.94301081, sen-loss: 19.39356408, dom-loss: 77.13873011, src-aux-loss: 64.63987747, tar-aux-loss: 79.77083796
Epoch: [42 ] train-acc: 0.94357143, dom-acc: 0.59223214, val-acc: 0.90750000, val_loss: 0.25525782
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 239.34792340, sen-loss: 19.20664163, dom-loss: 76.99694103, src-aux-loss: 64.06456587, tar-aux-loss: 79.07977533
Epoch: [43 ] train-acc: 0.94625000, dom-acc: 0.57464286, val-acc: 0.90250000, val_loss: 0.25641611
---------------------------------------------------

Successfully load model from save path: ./work/models/video_kitchen_HATN.ckpt
Best Epoch: [ 38] best val accuracy: 0.00000000 best val loss: 0.25429532
Testing accuracy: 0.86016667
./work/attentions/video_kitchen_train_HATN.txt
./work/attentions/video_kitchen_test_HATN.txt
loading data...
source domain:  books target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 11843
vocab-size:  100530
['great', 'good', 'excellent', 'best', 'wonderful', 'easy', 'fantastic', 'enjoyable', 'highly', 'entertaining', 'love', 'funny', 'loved', 'brilliant', 'awesome', 'interesting', 'amazing', 'useful', 'incredible', 'classic', 'honest', 'well', 'fascinating', 'favorite', 'important', 'sad', 'nice', 'solid', 'beautiful', 'inspiring', 'fun', 'superb', 'fabulous', 'emotional', 'compelling', 'inspirational', 'valuable', 'loves', 'amusing', 'perfect', 'essential', 'enjoyed', 'impressive', 'familiar', 'liked', 'riveting', 'wonderfully', 'marvelous', 'believable', 'real', 'greatest', 'humorous', 'enlightening', 'true', 'invaluable', 'finest', 'terrific', 'exciting']
['disappointing', 'boring', 'disappointed', 'better', 'poorly', 'horrible', 'useless', 'misleading', 'awful', 'confusing', 'repetitive', 'terrible', 'bad', 'annoying', 'flawed', 'simplistic', 'predictable', 'wasted', 'mediocre', 'difficult', 'tedious', 'worst', 'ridiculous', 'pathetic', 'laughable', 'silly', 'unnecessary', 'lacking', 'frustrating', 'trite', 'waste', 'poor', 'hard', 'outdated', 'lacks', 'biased', 'uninteresting', 'sloppy', 'tired', 'uninspired', 'disjointed', 'slow', 'dull', 'pretentious', 'weak', 'expensive', 'contrived', 'inaccurate', 'tiresome', 'superficial', 'absurd', 'stupid', 'amateurish', 'insulting', 'wrong', 'overrated', 'unlikeable', 'dissapointed', 'unbelievable', 'hackneyed', 'shallow', 'clumsy', 'dangerous', 'lame', 'impossible', 'utterly', 'wastes', 'terribly', 'unreadable']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
100530
5600 400 6000 15750 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 402.62766218, sen-loss: 77.64545494, dom-loss: 77.77058923, src-aux-loss: 131.01164609, tar-aux-loss: 116.19997245
Epoch: [1  ] train-acc: 0.67232143, dom-acc: 0.69080357, val-acc: 0.68500000, val_loss: 0.65552062
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 377.88312387, sen-loss: 70.78244561, dom-loss: 75.69022840, src-aux-loss: 122.78774810, tar-aux-loss: 108.62270206
Epoch: [2  ] train-acc: 0.74410714, dom-acc: 0.71517857, val-acc: 0.76000000, val_loss: 0.59173375
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.36670828, sen-loss: 63.59407195, dom-loss: 74.88282585, src-aux-loss: 118.13072622, tar-aux-loss: 103.75908339
Epoch: [3  ] train-acc: 0.79357143, dom-acc: 0.67598214, val-acc: 0.79500000, val_loss: 0.51892382
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.37696099, sen-loss: 55.74246997, dom-loss: 74.69782281, src-aux-loss: 114.59705794, tar-aux-loss: 102.33960986
Epoch: [4  ] train-acc: 0.82446429, dom-acc: 0.62821429, val-acc: 0.83500000, val_loss: 0.44520748
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 334.25878739, sen-loss: 48.48100427, dom-loss: 74.83273542, src-aux-loss: 111.89021158, tar-aux-loss: 99.05483812
Epoch: [5  ] train-acc: 0.84428571, dom-acc: 0.62267857, val-acc: 0.85750000, val_loss: 0.38670817
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 325.69074035, sen-loss: 42.99867764, dom-loss: 75.00593358, src-aux-loss: 109.71699816, tar-aux-loss: 97.96913302
Epoch: [6  ] train-acc: 0.86464286, dom-acc: 0.62955357, val-acc: 0.86750000, val_loss: 0.34564701
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 317.92072105, sen-loss: 38.67897898, dom-loss: 75.02348584, src-aux-loss: 107.71750009, tar-aux-loss: 96.50075632
Epoch: [7  ] train-acc: 0.87642857, dom-acc: 0.64964286, val-acc: 0.86000000, val_loss: 0.32878560
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 313.57875681, sen-loss: 36.13808660, dom-loss: 75.12586427, src-aux-loss: 106.51343179, tar-aux-loss: 95.80137360
Epoch: [8  ] train-acc: 0.87232143, dom-acc: 0.68348214, val-acc: 0.86750000, val_loss: 0.34513494
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 309.51170993, sen-loss: 34.09556811, dom-loss: 75.29214329, src-aux-loss: 105.35607874, tar-aux-loss: 94.76792067
Epoch: [9  ] train-acc: 0.88714286, dom-acc: 0.69714286, val-acc: 0.87500000, val_loss: 0.32364973
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 307.18629336, sen-loss: 32.87479156, dom-loss: 75.70995224, src-aux-loss: 104.24780625, tar-aux-loss: 94.35374302
Epoch: [10 ] train-acc: 0.89142857, dom-acc: 0.69125000, val-acc: 0.86750000, val_loss: 0.32514766
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 305.64476562, sen-loss: 32.04140425, dom-loss: 75.78011996, src-aux-loss: 103.21801353, tar-aux-loss: 94.60522759
Epoch: [11 ] train-acc: 0.89553571, dom-acc: 0.69392857, val-acc: 0.87750000, val_loss: 0.32488266
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 304.54549336, sen-loss: 31.51615666, dom-loss: 76.46255165, src-aux-loss: 102.74073857, tar-aux-loss: 93.82604617
Epoch: [12 ] train-acc: 0.89428571, dom-acc: 0.68000000, val-acc: 0.87250000, val_loss: 0.32832557
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 302.77408051, sen-loss: 30.84792729, dom-loss: 76.40472901, src-aux-loss: 102.08854753, tar-aux-loss: 93.43287653
Epoch: [13 ] train-acc: 0.89946429, dom-acc: 0.67901786, val-acc: 0.87750000, val_loss: 0.31904098
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 301.50473142, sen-loss: 30.23690430, dom-loss: 76.83833718, src-aux-loss: 101.46774089, tar-aux-loss: 92.96175045
Epoch: [14 ] train-acc: 0.89946429, dom-acc: 0.67982143, val-acc: 0.87250000, val_loss: 0.32459748
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 300.14267588, sen-loss: 29.71190517, dom-loss: 76.95838088, src-aux-loss: 100.58890545, tar-aux-loss: 92.88348472
Epoch: [15 ] train-acc: 0.89928571, dom-acc: 0.68000000, val-acc: 0.87250000, val_loss: 0.32527733
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 299.73187923, sen-loss: 29.33512591, dom-loss: 77.35752046, src-aux-loss: 100.26657850, tar-aux-loss: 92.77265495
Epoch: [16 ] train-acc: 0.90303571, dom-acc: 0.65428571, val-acc: 0.87000000, val_loss: 0.31745028
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 298.56302619, sen-loss: 28.88848599, dom-loss: 77.67785060, src-aux-loss: 99.43922400, tar-aux-loss: 92.55746567
Epoch: [17 ] train-acc: 0.90517857, dom-acc: 0.64133929, val-acc: 0.87750000, val_loss: 0.31310898
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 298.40383768, sen-loss: 28.46745586, dom-loss: 77.69923669, src-aux-loss: 99.04609025, tar-aux-loss: 93.19105387
Epoch: [18 ] train-acc: 0.90571429, dom-acc: 0.64303571, val-acc: 0.88000000, val_loss: 0.31143591
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 296.53360057, sen-loss: 28.05916683, dom-loss: 78.07584912, src-aux-loss: 98.45075077, tar-aux-loss: 91.94783199
Epoch: [19 ] train-acc: 0.90767857, dom-acc: 0.64116071, val-acc: 0.87500000, val_loss: 0.31549102
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 294.21520281, sen-loss: 27.92314038, dom-loss: 78.23697603, src-aux-loss: 98.16089046, tar-aux-loss: 89.89419764
Epoch: [20 ] train-acc: 0.91000000, dom-acc: 0.61205357, val-acc: 0.87750000, val_loss: 0.31028911
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 294.69029093, sen-loss: 27.40916096, dom-loss: 78.21918440, src-aux-loss: 97.50170404, tar-aux-loss: 91.56024170
Epoch: [21 ] train-acc: 0.91017857, dom-acc: 0.63294643, val-acc: 0.88250000, val_loss: 0.31451401
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 293.91218758, sen-loss: 27.13008189, dom-loss: 78.41005707, src-aux-loss: 96.94825572, tar-aux-loss: 91.42379314
Epoch: [22 ] train-acc: 0.91196429, dom-acc: 0.61089286, val-acc: 0.88000000, val_loss: 0.30910477
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 294.02167654, sen-loss: 26.81275276, dom-loss: 78.38122112, src-aux-loss: 96.85655063, tar-aux-loss: 91.97115129
Epoch: [23 ] train-acc: 0.91339286, dom-acc: 0.62455357, val-acc: 0.88000000, val_loss: 0.31880867
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 292.49906421, sen-loss: 26.41482598, dom-loss: 78.49361718, src-aux-loss: 96.15114528, tar-aux-loss: 91.43947607
Epoch: [24 ] train-acc: 0.91339286, dom-acc: 0.61214286, val-acc: 0.88250000, val_loss: 0.31310973
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 292.21889782, sen-loss: 26.11112577, dom-loss: 78.40106779, src-aux-loss: 95.57026500, tar-aux-loss: 92.13643855
Epoch: [25 ] train-acc: 0.91589286, dom-acc: 0.60696429, val-acc: 0.88250000, val_loss: 0.31293803
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 289.17464375, sen-loss: 25.79248568, dom-loss: 78.44222814, src-aux-loss: 95.22544652, tar-aux-loss: 89.71448392
Epoch: [26 ] train-acc: 0.91660714, dom-acc: 0.60580357, val-acc: 0.88250000, val_loss: 0.31249508
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 289.33506632, sen-loss: 25.58875183, dom-loss: 78.45044720, src-aux-loss: 94.90927529, tar-aux-loss: 90.38659030
Epoch: [27 ] train-acc: 0.91750000, dom-acc: 0.60553571, val-acc: 0.88250000, val_loss: 0.31140599
---------------------------------------------------

Successfully load model from save path: ./work/models/books_dvd_HATN.ckpt
Best Epoch: [ 22] best val accuracy: 0.00000000 best val loss: 0.30910477
Testing accuracy: 0.87116667
./work/attentions/books_dvd_train_HATN.txt
./work/attentions/books_dvd_test_HATN.txt
loading data...
source domain:  books target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 17009
vocab-size:  83050
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'fantastic', 'love', 'funny', 'well', 'awesome', 'entertaining', 'classic', 'enjoyed', 'brilliant', 'favorite', 'amazing', 'fascinating', 'interesting', 'loved', 'wonderful', 'nice', 'incredible', 'solid', 'sad', 'essential', 'believable', 'important', 'amusing', 'fun', 'superb', 'riveting', 'useful', 'inspiring', 'inspirational', 'real', 'greatest', 'compelling', 'wonderfully', 'hilarious', 'true', 'liked', 'emotional', 'finest', 'honest', 'loves', 'perfect', 'valuable', 'authentic', 'humorous', 'enlightening', 'invaluable', 'refreshing', 'makes', 'impressive', 'gives']
['disappointing', 'disappointed', 'boring', 'poorly', 'better', 'horrible', 'repetitive', 'confusing', 'useless', 'lacks', 'annoying', 'predictable', 'misleading', 'hard', 'awful', 'worst', 'difficult', 'wasted', 'flawed', 'tedious', 'pathetic', 'simplistic', 'laughable', 'terrible', 'unnecessary', 'slow', 'frustrating', 'ridiculous', 'trite', 'uninteresting', 'mediocre', 'much', 'sloppy', 'waste', 'weak', 'uninspired', 'lacking', 'utterly', 'tiresome', 'superficial', 'biased', 'silly', 'contrived', 'unbelievable', 'disjointed', 'outdated', 'wrong', 'terribly', 'expensive', 'dangerous', 'lame', 'unlikeable', 'inaccurate', 'bad', 'amateurish', 'dissapointed', 'unreadable', 'hackneyed', 'shallow', 'poor', 'deceived', 'ruined', 'fails', 'overrated', 'dull', 'repetitious', 'impossible', 'instead', 'wastes', 'proven', 'stilted', 'irritating', 'clumsy']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
83050
5600 400 6000 15750 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 406.12970400, sen-loss: 77.70533007, dom-loss: 78.43470746, src-aux-loss: 126.90254283, tar-aux-loss: 123.08712423
Epoch: [1  ] train-acc: 0.67017857, dom-acc: 0.75321429, val-acc: 0.66500000, val_loss: 0.65614492
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 377.03626084, sen-loss: 70.95761162, dom-loss: 74.73604381, src-aux-loss: 117.25887102, tar-aux-loss: 114.08373266
Epoch: [2  ] train-acc: 0.74000000, dom-acc: 0.82098214, val-acc: 0.75750000, val_loss: 0.59533972
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.48905849, sen-loss: 63.98845446, dom-loss: 73.13455838, src-aux-loss: 112.44939125, tar-aux-loss: 110.91665620
Epoch: [3  ] train-acc: 0.79410714, dom-acc: 0.80598214, val-acc: 0.79500000, val_loss: 0.52480030
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.49514413, sen-loss: 56.38326460, dom-loss: 73.09021342, src-aux-loss: 109.20752919, tar-aux-loss: 108.81413859
Epoch: [4  ] train-acc: 0.82089286, dom-acc: 0.75160714, val-acc: 0.82250000, val_loss: 0.45146206
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 335.73622727, sen-loss: 48.99176174, dom-loss: 73.60876614, src-aux-loss: 107.01783270, tar-aux-loss: 106.11786723
Epoch: [5  ] train-acc: 0.84089286, dom-acc: 0.73116071, val-acc: 0.84500000, val_loss: 0.39006564
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 327.80735755, sen-loss: 43.12759021, dom-loss: 74.39989251, src-aux-loss: 105.35046405, tar-aux-loss: 104.92940986
Epoch: [6  ] train-acc: 0.86214286, dom-acc: 0.75901786, val-acc: 0.86500000, val_loss: 0.34528571
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 321.10951805, sen-loss: 38.54292998, dom-loss: 74.98142701, src-aux-loss: 103.83353287, tar-aux-loss: 103.75162870
Epoch: [7  ] train-acc: 0.87660714, dom-acc: 0.75901786, val-acc: 0.87000000, val_loss: 0.32533067
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 315.80557728, sen-loss: 35.75956807, dom-loss: 75.62302327, src-aux-loss: 102.30980545, tar-aux-loss: 102.11317998
Epoch: [8  ] train-acc: 0.88017857, dom-acc: 0.70785714, val-acc: 0.87750000, val_loss: 0.32986253
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 313.32587671, sen-loss: 34.00431684, dom-loss: 76.55141848, src-aux-loss: 101.36703664, tar-aux-loss: 101.40310609
Epoch: [9  ] train-acc: 0.88696429, dom-acc: 0.66562500, val-acc: 0.87250000, val_loss: 0.32410794
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 313.33409595, sen-loss: 32.93534933, dom-loss: 77.75256586, src-aux-loss: 100.41886216, tar-aux-loss: 102.22732008
Epoch: [10 ] train-acc: 0.89142857, dom-acc: 0.63223214, val-acc: 0.87250000, val_loss: 0.32385895
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 311.44107056, sen-loss: 32.22120593, dom-loss: 78.26203805, src-aux-loss: 99.85440642, tar-aux-loss: 101.10341865
Epoch: [11 ] train-acc: 0.89410714, dom-acc: 0.61678571, val-acc: 0.87250000, val_loss: 0.32339069
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 310.50057840, sen-loss: 31.52151761, dom-loss: 79.18785977, src-aux-loss: 99.00415367, tar-aux-loss: 100.78704756
Epoch: [12 ] train-acc: 0.89500000, dom-acc: 0.57875000, val-acc: 0.87750000, val_loss: 0.32461527
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 309.00605559, sen-loss: 30.98669209, dom-loss: 79.51888454, src-aux-loss: 98.56434387, tar-aux-loss: 99.93613535
Epoch: [13 ] train-acc: 0.89803571, dom-acc: 0.57008929, val-acc: 0.88000000, val_loss: 0.32154757
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 308.90433812, sen-loss: 30.47266360, dom-loss: 79.84167868, src-aux-loss: 98.16953099, tar-aux-loss: 100.42046624
Epoch: [14 ] train-acc: 0.89964286, dom-acc: 0.55508929, val-acc: 0.88000000, val_loss: 0.32245132
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 305.32195497, sen-loss: 29.84644538, dom-loss: 79.80475676, src-aux-loss: 97.29260898, tar-aux-loss: 98.37814349
Epoch: [15 ] train-acc: 0.89946429, dom-acc: 0.55982143, val-acc: 0.87500000, val_loss: 0.32462117
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 305.07298589, sen-loss: 29.50999054, dom-loss: 79.62336880, src-aux-loss: 97.05678082, tar-aux-loss: 98.88284814
Epoch: [16 ] train-acc: 0.90285714, dom-acc: 0.54178571, val-acc: 0.88000000, val_loss: 0.32076275
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 304.19578338, sen-loss: 29.00376028, dom-loss: 79.42683011, src-aux-loss: 96.33952391, tar-aux-loss: 99.42566901
Epoch: [17 ] train-acc: 0.90500000, dom-acc: 0.53732143, val-acc: 0.87750000, val_loss: 0.31276974
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 303.22274375, sen-loss: 28.62952432, dom-loss: 78.94397324, src-aux-loss: 96.01088327, tar-aux-loss: 99.63836199
Epoch: [18 ] train-acc: 0.90625000, dom-acc: 0.57607143, val-acc: 0.87500000, val_loss: 0.31010470
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 300.18425512, sen-loss: 28.20016080, dom-loss: 78.48191631, src-aux-loss: 95.52781081, tar-aux-loss: 97.97436500
Epoch: [19 ] train-acc: 0.90857143, dom-acc: 0.59303571, val-acc: 0.87750000, val_loss: 0.31088224
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 299.54308414, sen-loss: 27.94814669, dom-loss: 78.34942615, src-aux-loss: 95.33065915, tar-aux-loss: 97.91485220
Epoch: [20 ] train-acc: 0.90875000, dom-acc: 0.60267857, val-acc: 0.87750000, val_loss: 0.30973476
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 297.54166293, sen-loss: 27.48144035, dom-loss: 78.09067237, src-aux-loss: 94.75218666, tar-aux-loss: 97.21736223
Epoch: [21 ] train-acc: 0.91053571, dom-acc: 0.60776786, val-acc: 0.88500000, val_loss: 0.31278050
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 298.55264091, sen-loss: 27.23627336, dom-loss: 77.90526175, src-aux-loss: 94.23205960, tar-aux-loss: 99.17904675
Epoch: [22 ] train-acc: 0.91303571, dom-acc: 0.60955357, val-acc: 0.87500000, val_loss: 0.30573818
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 295.34023023, sen-loss: 26.91575549, dom-loss: 77.68906462, src-aux-loss: 94.17065811, tar-aux-loss: 96.56475109
Epoch: [23 ] train-acc: 0.91071429, dom-acc: 0.61883929, val-acc: 0.87500000, val_loss: 0.31893322
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 295.16854215, sen-loss: 26.53569702, dom-loss: 77.58177662, src-aux-loss: 93.59049213, tar-aux-loss: 97.46057546
Epoch: [24 ] train-acc: 0.91446429, dom-acc: 0.61651786, val-acc: 0.88250000, val_loss: 0.31124657
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.74128246, sen-loss: 26.23804361, dom-loss: 77.67887437, src-aux-loss: 93.20804143, tar-aux-loss: 98.61632174
Epoch: [25 ] train-acc: 0.91517857, dom-acc: 0.61883929, val-acc: 0.88250000, val_loss: 0.30972016
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.16223502, sen-loss: 25.90621155, dom-loss: 77.67985189, src-aux-loss: 92.85372508, tar-aux-loss: 96.72244620
Epoch: [26 ] train-acc: 0.91571429, dom-acc: 0.60473214, val-acc: 0.88250000, val_loss: 0.30859771
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 291.88064694, sen-loss: 25.73939328, dom-loss: 77.65645474, src-aux-loss: 92.55645412, tar-aux-loss: 95.92834479
Epoch: [27 ] train-acc: 0.91964286, dom-acc: 0.60785714, val-acc: 0.87750000, val_loss: 0.30605179
---------------------------------------------------

Successfully load model from save path: ./work/models/books_electronics_HATN.ckpt
Best Epoch: [ 22] best val accuracy: 0.00000000 best val loss: 0.30573818
Testing accuracy: 0.85100000
./work/attentions/books_electronics_train_HATN.txt
./work/attentions/books_electronics_test_HATN.txt
loading data...
source domain:  books target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 13856
vocab-size:  78006
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'love', 'funny', 'fantastic', 'well', 'enjoyed', 'awesome', 'entertaining', 'classic', 'favorite', 'brilliant', 'fascinating', 'wonderful', 'amazing', 'loved', 'interesting', 'fun', 'nice', 'incredible', 'solid', 'inspiring', 'superb', 'believable', 'riveting', 'useful', 'essential', 'important', 'emotional', 'inspirational', 'sad', 'compelling', 'wonderfully', 'real', 'fabulous', 'hilarious', 'greatest', 'liked', 'refreshing', 'honest', 'makes', 'impressive', 'loves', 'valuable', 'authentic', 'humorous', 'enlightening', 'true', 'simple', 'invaluable', 'heartwarming', 'gives']
['disappointing', 'disappointed', 'boring', 'poorly', 'better', 'horrible', 'repetitive', 'confusing', 'useless', 'hard', 'lacks', 'annoying', 'difficult', 'predictable', 'worst', 'awful', 'tedious', 'misleading', 'pathetic', 'flawed', 'wasted', 'simplistic', 'unnecessary', 'terrible', 'slow', 'laughable', 'frustrating', 'uninteresting', 'trite', 'mediocre', 'ridiculous', 'uninspired', 'waste', 'bad', 'contrived', 'sloppy', 'tiresome', 'shallow', 'silly', 'disjointed', 'utterly', 'outdated', 'superficial', 'weak', 'terribly', 'expensive', 'unbelievable', 'lame', 'unlikeable', 'inaccurate', 'impossible', 'wastes', 'dissapointed', 'unreadable', 'seems', 'hackneyed', 'biased', 'poor', 'deceived', 'unconvincing', 'trying', 'ruined', 'lacking', 'fails', 'finest', 'dangerous', 'overrated', 'dull', 'repetitious', 'proven', 'stilted', 'wrong', 'sadly', 'disapointed', 'irritating', 'clumsy']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
78006
5600 400 6000 15750 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.18255711, sen-loss: 77.69759464, dom-loss: 78.09062254, src-aux-loss: 129.30702090, tar-aux-loss: 119.08731717
Epoch: [1  ] train-acc: 0.65875000, dom-acc: 0.75982143, val-acc: 0.64250000, val_loss: 0.65682250
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 375.42541218, sen-loss: 70.96591496, dom-loss: 74.63584620, src-aux-loss: 119.72560710, tar-aux-loss: 110.09804451
Epoch: [2  ] train-acc: 0.74803571, dom-acc: 0.82964286, val-acc: 0.76000000, val_loss: 0.59444654
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 358.09369469, sen-loss: 63.92181635, dom-loss: 73.30609071, src-aux-loss: 114.96406806, tar-aux-loss: 105.90171939
Epoch: [3  ] train-acc: 0.79357143, dom-acc: 0.81026786, val-acc: 0.80000000, val_loss: 0.52444953
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 344.96443319, sen-loss: 56.30929413, dom-loss: 73.39420193, src-aux-loss: 111.34891629, tar-aux-loss: 103.91201979
Epoch: [4  ] train-acc: 0.82410714, dom-acc: 0.77160714, val-acc: 0.82250000, val_loss: 0.45075595
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 334.37554264, sen-loss: 48.93374500, dom-loss: 74.08132792, src-aux-loss: 108.96678883, tar-aux-loss: 102.39368087
Epoch: [5  ] train-acc: 0.84142857, dom-acc: 0.77125000, val-acc: 0.84500000, val_loss: 0.39101857
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 326.08867788, sen-loss: 42.98027426, dom-loss: 74.62159562, src-aux-loss: 107.40927154, tar-aux-loss: 101.07753599
Epoch: [6  ] train-acc: 0.86642857, dom-acc: 0.76044643, val-acc: 0.87000000, val_loss: 0.34465060
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 318.82937860, sen-loss: 38.48364203, dom-loss: 75.22519869, src-aux-loss: 105.49597758, tar-aux-loss: 99.62456059
Epoch: [7  ] train-acc: 0.87625000, dom-acc: 0.71848214, val-acc: 0.86750000, val_loss: 0.32684129
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 316.01750588, sen-loss: 35.77116352, dom-loss: 75.85362899, src-aux-loss: 104.20537281, tar-aux-loss: 100.18734276
Epoch: [8  ] train-acc: 0.87535714, dom-acc: 0.68008929, val-acc: 0.87000000, val_loss: 0.33733892
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 310.59679580, sen-loss: 33.98479258, dom-loss: 76.53113735, src-aux-loss: 103.02752882, tar-aux-loss: 97.05333775
Epoch: [9  ] train-acc: 0.88607143, dom-acc: 0.65258929, val-acc: 0.88000000, val_loss: 0.32574925
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 310.64227390, sen-loss: 32.68557011, dom-loss: 77.66469145, src-aux-loss: 101.89587867, tar-aux-loss: 98.39613247
Epoch: [10 ] train-acc: 0.89107143, dom-acc: 0.63258929, val-acc: 0.87500000, val_loss: 0.32340804
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 309.57431650, sen-loss: 31.97767722, dom-loss: 78.06345785, src-aux-loss: 101.08791178, tar-aux-loss: 98.44526881
Epoch: [11 ] train-acc: 0.89678571, dom-acc: 0.62526786, val-acc: 0.87750000, val_loss: 0.31972483
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 307.97751856, sen-loss: 31.34013049, dom-loss: 78.87198687, src-aux-loss: 100.30896485, tar-aux-loss: 97.45643628
Epoch: [12 ] train-acc: 0.89375000, dom-acc: 0.60651786, val-acc: 0.87000000, val_loss: 0.32685646
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 306.29109097, sen-loss: 30.86468494, dom-loss: 78.74154055, src-aux-loss: 100.04198247, tar-aux-loss: 96.64288247
Epoch: [13 ] train-acc: 0.89982143, dom-acc: 0.60250000, val-acc: 0.87750000, val_loss: 0.31578368
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 304.24671626, sen-loss: 30.30547749, dom-loss: 78.87108719, src-aux-loss: 99.18072873, tar-aux-loss: 95.88942313
Epoch: [14 ] train-acc: 0.89910714, dom-acc: 0.61223214, val-acc: 0.87500000, val_loss: 0.32281235
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 302.65907097, sen-loss: 29.68229769, dom-loss: 78.57267797, src-aux-loss: 98.29034376, tar-aux-loss: 96.11375213
Epoch: [15 ] train-acc: 0.90035714, dom-acc: 0.61714286, val-acc: 0.88000000, val_loss: 0.31827256
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 302.56241155, sen-loss: 29.27344558, dom-loss: 78.59974879, src-aux-loss: 98.06898874, tar-aux-loss: 96.62022799
Epoch: [16 ] train-acc: 0.90303571, dom-acc: 0.60741071, val-acc: 0.87250000, val_loss: 0.31969130
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 299.17105746, sen-loss: 28.76872418, dom-loss: 78.23611671, src-aux-loss: 97.29094803, tar-aux-loss: 94.87526774
Epoch: [17 ] train-acc: 0.90589286, dom-acc: 0.62053571, val-acc: 0.87250000, val_loss: 0.30981719
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 297.55822134, sen-loss: 28.30853848, dom-loss: 77.60238940, src-aux-loss: 96.92293358, tar-aux-loss: 94.72436082
Epoch: [18 ] train-acc: 0.90857143, dom-acc: 0.62366071, val-acc: 0.87500000, val_loss: 0.30700737
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 297.05583358, sen-loss: 27.95821545, dom-loss: 77.36299235, src-aux-loss: 96.34179574, tar-aux-loss: 95.39283115
Epoch: [19 ] train-acc: 0.90821429, dom-acc: 0.64205357, val-acc: 0.87750000, val_loss: 0.30973482
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 295.30112529, sen-loss: 27.75348375, dom-loss: 77.37942982, src-aux-loss: 96.01953286, tar-aux-loss: 94.14867830
Epoch: [20 ] train-acc: 0.90982143, dom-acc: 0.64348214, val-acc: 0.87750000, val_loss: 0.30805615
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 294.06739664, sen-loss: 27.24940923, dom-loss: 77.11559337, src-aux-loss: 95.49517363, tar-aux-loss: 94.20722115
Epoch: [21 ] train-acc: 0.91035714, dom-acc: 0.64651786, val-acc: 0.88000000, val_loss: 0.31228560
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 293.28857899, sen-loss: 26.94585390, dom-loss: 77.08201373, src-aux-loss: 94.95516670, tar-aux-loss: 94.30554295
Epoch: [22 ] train-acc: 0.91678571, dom-acc: 0.64303571, val-acc: 0.87750000, val_loss: 0.30484349
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 292.63366580, sen-loss: 26.72226694, dom-loss: 76.92358762, src-aux-loss: 94.72841054, tar-aux-loss: 94.25940090
Epoch: [23 ] train-acc: 0.91214286, dom-acc: 0.64741071, val-acc: 0.88000000, val_loss: 0.31766501
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 292.34120393, sen-loss: 26.36914935, dom-loss: 77.18343586, src-aux-loss: 94.34156287, tar-aux-loss: 94.44705695
Epoch: [24 ] train-acc: 0.91750000, dom-acc: 0.63812500, val-acc: 0.88250000, val_loss: 0.30583009
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 290.89355946, sen-loss: 26.02118889, dom-loss: 77.08284950, src-aux-loss: 93.61255962, tar-aux-loss: 94.17696136
Epoch: [25 ] train-acc: 0.91607143, dom-acc: 0.63669643, val-acc: 0.87750000, val_loss: 0.31016678
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 290.17223263, sen-loss: 25.66917642, dom-loss: 77.31157672, src-aux-loss: 93.39201587, tar-aux-loss: 93.79946351
Epoch: [26 ] train-acc: 0.91946429, dom-acc: 0.62741071, val-acc: 0.88500000, val_loss: 0.30592570
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 288.97863889, sen-loss: 25.39471459, dom-loss: 77.49304903, src-aux-loss: 93.08562285, tar-aux-loss: 93.00525194
Epoch: [27 ] train-acc: 0.92178571, dom-acc: 0.62080357, val-acc: 0.88250000, val_loss: 0.30415246
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 289.63682461, sen-loss: 25.04949220, dom-loss: 77.58932662, src-aux-loss: 92.77544880, tar-aux-loss: 94.22255790
Epoch: [28 ] train-acc: 0.91839286, dom-acc: 0.62410714, val-acc: 0.88500000, val_loss: 0.31717867
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 288.25021386, sen-loss: 24.79133418, dom-loss: 77.67469609, src-aux-loss: 92.27022815, tar-aux-loss: 93.51395571
Epoch: [29 ] train-acc: 0.92321429, dom-acc: 0.61562500, val-acc: 0.88000000, val_loss: 0.30254701
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 286.60077906, sen-loss: 24.60930211, dom-loss: 77.81827056, src-aux-loss: 91.77649349, tar-aux-loss: 92.39671201
Epoch: [30 ] train-acc: 0.92392857, dom-acc: 0.61491071, val-acc: 0.87750000, val_loss: 0.31184295
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 285.77080774, sen-loss: 24.18257672, dom-loss: 77.80261910, src-aux-loss: 91.61070985, tar-aux-loss: 92.17490298
Epoch: [31 ] train-acc: 0.92750000, dom-acc: 0.61035714, val-acc: 0.88250000, val_loss: 0.30605292
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 286.07745171, sen-loss: 23.94642505, dom-loss: 77.60013795, src-aux-loss: 91.09586489, tar-aux-loss: 93.43502420
Epoch: [32 ] train-acc: 0.92785714, dom-acc: 0.62017857, val-acc: 0.88250000, val_loss: 0.30423295
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 284.64823961, sen-loss: 23.72798268, dom-loss: 77.62286866, src-aux-loss: 90.74366313, tar-aux-loss: 92.55372250
Epoch: [33 ] train-acc: 0.92892857, dom-acc: 0.61955357, val-acc: 0.88500000, val_loss: 0.30529022
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 282.85786462, sen-loss: 23.45468746, dom-loss: 77.81789052, src-aux-loss: 90.25701421, tar-aux-loss: 91.32827318
Epoch: [34 ] train-acc: 0.92982143, dom-acc: 0.62866071, val-acc: 0.88250000, val_loss: 0.30569413
---------------------------------------------------

Successfully load model from save path: ./work/models/books_kitchen_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.30254701
Testing accuracy: 0.86583333
./work/attentions/books_kitchen_train_HATN.txt
./work/attentions/books_kitchen_test_HATN.txt
loading data...
source domain:  books target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 30180
vocab-size:  98084
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'funny', 'fantastic', 'love', 'well', 'entertaining', 'awesome', 'enjoyed', 'brilliant', 'classic', 'favorite', 'fascinating', 'loved', 'wonderful', 'amazing', 'interesting', 'incredible', 'nice', 'inspiring', 'emotional', 'solid', 'fun', 'sad', 'believable', 'essential', 'amusing', 'superb', 'true', 'riveting', 'useful', 'important', 'inspirational', 'compelling', 'real', 'greatest', 'liked', 'invaluable', 'finest', 'honest', 'wonderfully', 'perfect', 'valuable', 'fabulous', 'hilarious', 'authentic', 'humorous', 'enlightening', 'refreshing', 'loves', 'makes', 'impressive', 'marvelous']
['disappointing', 'boring', 'disappointed', 'poorly', 'horrible', 'better', 'repetitive', 'confusing', 'useless', 'worst', 'lacks', 'annoying', 'misleading', 'hard', 'predictable', 'awful', 'wasted', 'pathetic', 'difficult', 'tedious', 'terrible', 'flawed', 'simplistic', 'unnecessary', 'laughable', 'slow', 'frustrating', 'uninteresting', 'ridiculous', 'trite', 'mediocre', 'uninspired', 'sloppy', 'waste', 'weak', 'much', 'lacking', 'wrong', 'utterly', 'tiresome', 'superficial', 'biased', 'silly', 'contrived', 'unbelievable', 'disjointed', 'bad', 'terribly', 'dangerous', 'lame', 'unlikeable', 'inaccurate', 'outdated', 'dissapointed', 'unreadable', 'hackneyed', 'shallow', 'poor', 'deceived', 'unconvincing', 'expensive', 'ruined', 'fails', 'overrated', 'dull', 'repetitious', 'impossible', 'amateurish', 'instead', 'wastes', 'proven', 'stilted', 'irritating', 'clumsy']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
98084
5600 400 6000 15750 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 400.44151306, sen-loss: 77.69962370, dom-loss: 77.73863918, src-aux-loss: 127.55519021, tar-aux-loss: 117.44805795
Epoch: [1  ] train-acc: 0.67035714, dom-acc: 0.70446429, val-acc: 0.67500000, val_loss: 0.65588450
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 372.48016262, sen-loss: 70.89589369, dom-loss: 75.73754787, src-aux-loss: 118.08177090, tar-aux-loss: 107.76495218
Epoch: [2  ] train-acc: 0.73857143, dom-acc: 0.72767857, val-acc: 0.75500000, val_loss: 0.59491736
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 356.19981074, sen-loss: 63.91980529, dom-loss: 74.92425972, src-aux-loss: 113.41904688, tar-aux-loss: 103.93669784
Epoch: [3  ] train-acc: 0.79250000, dom-acc: 0.69973214, val-acc: 0.79750000, val_loss: 0.52398163
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 342.73224235, sen-loss: 56.32255292, dom-loss: 74.68148822, src-aux-loss: 109.94424665, tar-aux-loss: 101.78395474
Epoch: [4  ] train-acc: 0.82125000, dom-acc: 0.67017857, val-acc: 0.82500000, val_loss: 0.45108297
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 329.23050213, sen-loss: 48.98756635, dom-loss: 74.69306690, src-aux-loss: 107.54055738, tar-aux-loss: 98.00931245
Epoch: [5  ] train-acc: 0.84089286, dom-acc: 0.66428571, val-acc: 0.84750000, val_loss: 0.39114913
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 321.42284226, sen-loss: 43.22534707, dom-loss: 74.86485064, src-aux-loss: 105.68797672, tar-aux-loss: 97.64466912
Epoch: [6  ] train-acc: 0.86178571, dom-acc: 0.67955357, val-acc: 0.86000000, val_loss: 0.34853289
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 312.32586479, sen-loss: 38.65695581, dom-loss: 74.81574696, src-aux-loss: 103.84395641, tar-aux-loss: 95.00920469
Epoch: [7  ] train-acc: 0.87500000, dom-acc: 0.69526786, val-acc: 0.86500000, val_loss: 0.32931578
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 309.28616333, sen-loss: 35.97163375, dom-loss: 74.81768215, src-aux-loss: 102.59482241, tar-aux-loss: 95.90202510
Epoch: [8  ] train-acc: 0.87571429, dom-acc: 0.71562500, val-acc: 0.87500000, val_loss: 0.33329767
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 304.70472765, sen-loss: 34.09530343, dom-loss: 75.26579255, src-aux-loss: 101.55628073, tar-aux-loss: 93.78735077
Epoch: [9  ] train-acc: 0.88821429, dom-acc: 0.69803571, val-acc: 0.87750000, val_loss: 0.32304922
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 302.94061184, sen-loss: 32.99955438, dom-loss: 75.65227062, src-aux-loss: 100.57603282, tar-aux-loss: 93.71275401
Epoch: [10 ] train-acc: 0.88892857, dom-acc: 0.69901786, val-acc: 0.87500000, val_loss: 0.32819569
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 301.90925789, sen-loss: 32.25828938, dom-loss: 75.79164118, src-aux-loss: 99.92760658, tar-aux-loss: 93.93172097
Epoch: [11 ] train-acc: 0.89392857, dom-acc: 0.69508929, val-acc: 0.87000000, val_loss: 0.32483739
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 300.57816839, sen-loss: 31.62917753, dom-loss: 76.31214404, src-aux-loss: 99.19562155, tar-aux-loss: 93.44122523
Epoch: [12 ] train-acc: 0.89517857, dom-acc: 0.67973214, val-acc: 0.86500000, val_loss: 0.32602304
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 297.62292790, sen-loss: 31.02333456, dom-loss: 76.25761402, src-aux-loss: 98.57138997, tar-aux-loss: 91.77058917
Epoch: [13 ] train-acc: 0.89625000, dom-acc: 0.67151786, val-acc: 0.87250000, val_loss: 0.32241276
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 297.76240182, sen-loss: 30.58159161, dom-loss: 76.68646514, src-aux-loss: 98.17512327, tar-aux-loss: 92.31922257
Epoch: [14 ] train-acc: 0.89767857, dom-acc: 0.67660714, val-acc: 0.87500000, val_loss: 0.32765672
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 295.42262340, sen-loss: 29.89314349, dom-loss: 77.04957938, src-aux-loss: 97.42504078, tar-aux-loss: 91.05485904
Epoch: [15 ] train-acc: 0.89910714, dom-acc: 0.66830357, val-acc: 0.87000000, val_loss: 0.32650357
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 296.15977693, sen-loss: 29.54879258, dom-loss: 77.35837221, src-aux-loss: 97.19861954, tar-aux-loss: 92.05399227
Epoch: [16 ] train-acc: 0.90303571, dom-acc: 0.64133929, val-acc: 0.88500000, val_loss: 0.31756493
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 293.46597695, sen-loss: 29.07813055, dom-loss: 77.65342516, src-aux-loss: 96.30995446, tar-aux-loss: 90.42446840
Epoch: [17 ] train-acc: 0.90410714, dom-acc: 0.63848214, val-acc: 0.87750000, val_loss: 0.31540889
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 293.23537731, sen-loss: 28.70611276, dom-loss: 77.70798218, src-aux-loss: 96.04028875, tar-aux-loss: 90.78099352
Epoch: [18 ] train-acc: 0.90589286, dom-acc: 0.62750000, val-acc: 0.88000000, val_loss: 0.31188375
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 292.91874456, sen-loss: 28.28820780, dom-loss: 77.93839830, src-aux-loss: 95.66893411, tar-aux-loss: 91.02320564
Epoch: [19 ] train-acc: 0.90839286, dom-acc: 0.63080357, val-acc: 0.88000000, val_loss: 0.31438652
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 291.31925273, sen-loss: 28.04945530, dom-loss: 78.24356985, src-aux-loss: 95.20535386, tar-aux-loss: 89.82087278
Epoch: [20 ] train-acc: 0.90910714, dom-acc: 0.61267857, val-acc: 0.87750000, val_loss: 0.31109554
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 290.11285639, sen-loss: 27.57368371, dom-loss: 78.26050109, src-aux-loss: 94.50670922, tar-aux-loss: 89.77196199
Epoch: [21 ] train-acc: 0.90964286, dom-acc: 0.62151786, val-acc: 0.88250000, val_loss: 0.31287798
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 290.69220924, sen-loss: 27.34460842, dom-loss: 78.39043331, src-aux-loss: 94.11708552, tar-aux-loss: 90.84008253
Epoch: [22 ] train-acc: 0.91142857, dom-acc: 0.61017857, val-acc: 0.88000000, val_loss: 0.30892900
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 288.97629285, sen-loss: 26.99926543, dom-loss: 78.36817896, src-aux-loss: 93.93481380, tar-aux-loss: 89.67403483
Epoch: [23 ] train-acc: 0.91035714, dom-acc: 0.61580357, val-acc: 0.88000000, val_loss: 0.31884477
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 288.58988452, sen-loss: 26.61796892, dom-loss: 78.44435173, src-aux-loss: 93.42504776, tar-aux-loss: 90.10251540
Epoch: [24 ] train-acc: 0.91500000, dom-acc: 0.61366071, val-acc: 0.88250000, val_loss: 0.31099606
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 287.80391526, sen-loss: 26.32601292, dom-loss: 78.33783096, src-aux-loss: 92.85358590, tar-aux-loss: 90.28648543
Epoch: [25 ] train-acc: 0.91625000, dom-acc: 0.60946429, val-acc: 0.88250000, val_loss: 0.31005055
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 285.31528807, sen-loss: 25.94933514, dom-loss: 78.38814908, src-aux-loss: 92.52638352, tar-aux-loss: 88.45141917
Epoch: [26 ] train-acc: 0.91410714, dom-acc: 0.60339286, val-acc: 0.88000000, val_loss: 0.31300655
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 283.86897755, sen-loss: 25.79053376, dom-loss: 78.32022703, src-aux-loss: 92.21586555, tar-aux-loss: 87.54235196
Epoch: [27 ] train-acc: 0.91946429, dom-acc: 0.60437500, val-acc: 0.88250000, val_loss: 0.30851629
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 285.66917944, sen-loss: 25.41196646, dom-loss: 78.23471397, src-aux-loss: 92.10413951, tar-aux-loss: 89.91835862
Epoch: [28 ] train-acc: 0.91232143, dom-acc: 0.62625000, val-acc: 0.87250000, val_loss: 0.32328510
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 284.30091786, sen-loss: 25.10298721, dom-loss: 78.18217963, src-aux-loss: 91.46819770, tar-aux-loss: 89.54755497
Epoch: [29 ] train-acc: 0.92142857, dom-acc: 0.62160714, val-acc: 0.88250000, val_loss: 0.30639389
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 282.42703152, sen-loss: 24.86323953, dom-loss: 78.04316610, src-aux-loss: 91.00171483, tar-aux-loss: 88.51891220
Epoch: [30 ] train-acc: 0.91910714, dom-acc: 0.62375000, val-acc: 0.87500000, val_loss: 0.31637877
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 280.24249125, sen-loss: 24.48545390, dom-loss: 77.96459121, src-aux-loss: 90.78839600, tar-aux-loss: 87.00404936
Epoch: [31 ] train-acc: 0.92357143, dom-acc: 0.62669643, val-acc: 0.88500000, val_loss: 0.31081614
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 281.89583707, sen-loss: 24.20403176, dom-loss: 77.79992509, src-aux-loss: 90.18283993, tar-aux-loss: 89.70903975
Epoch: [32 ] train-acc: 0.92392857, dom-acc: 0.63276786, val-acc: 0.88000000, val_loss: 0.31231534
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 279.98338437, sen-loss: 23.97490730, dom-loss: 77.73214394, src-aux-loss: 89.85272312, tar-aux-loss: 88.42360973
Epoch: [33 ] train-acc: 0.92607143, dom-acc: 0.63312500, val-acc: 0.87750000, val_loss: 0.31264871
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 279.00749803, sen-loss: 23.79580849, dom-loss: 77.79203868, src-aux-loss: 89.45038092, tar-aux-loss: 87.96927053
Epoch: [34 ] train-acc: 0.92767857, dom-acc: 0.64125000, val-acc: 0.88500000, val_loss: 0.30901873
---------------------------------------------------

Successfully load model from save path: ./work/models/books_video_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.30639389
Testing accuracy: 0.88216667
./work/attentions/books_video_train_HATN.txt
./work/attentions/books_video_test_HATN.txt
loading data...
source domain:  dvd target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 9750
vocab-size:  100530
['best', 'good', 'great', 'excellent', 'funny', 'entertaining', 'enjoyable', 'better', 'awesome', 'fantastic', 'love', 'classic', 'wonderful', 'amazing', 'perfect', 'nice', 'brilliant', 'hilarious', 'loved', 'funniest', 'underrated', 'favorite', 'sad', 'interesting', 'superb', 'terrific', 'greatest', 'finest', 'real', 'solid', 'easy', 'incredible', 'memorable', 'fascinating', 'sappy', 'liked', 'spectacular', 'impressive', 'cute']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'wasted', 'dull', 'waste', 'stupid', 'pathetic', 'laughable', 'unwatchable', 'predictable', 'worse', 'annoying', 'lousy', 'forgettable', 'unfunny', 'uninspired', 'ruined', 'decent', 'slow', 'sucked', 'pointless', 'overrated', 'dismal', 'wrong', 'poorly', 'unoriginal', 'ok', 'mediocre', 'lacking', 'lame', 'dreadful', 'defective', 'sucks', 'crappy', 'atrocious', 'disgusting', 'hated', 'cheap', 'embarrassing', 'ridiculous', 'unconvincing', 'depressing', 'horrendous', 'biased', 'weak', 'disjointed', 'disapointed', 'frustrating', 'contrived', 'honest', 'insipid', 'inferior']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
100530
5600 400 6000 17843 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 407.58635521, sen-loss: 78.27328789, dom-loss: 79.59808743, src-aux-loss: 130.71671605, tar-aux-loss: 118.99826449
Epoch: [1  ] train-acc: 0.64482143, dom-acc: 0.60964286, val-acc: 0.67000000, val_loss: 0.66483504
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 384.23787737, sen-loss: 72.68108112, dom-loss: 77.23402297, src-aux-loss: 122.42708904, tar-aux-loss: 111.89568549
Epoch: [2  ] train-acc: 0.71750000, dom-acc: 0.76857143, val-acc: 0.75000000, val_loss: 0.60807455
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 366.40386033, sen-loss: 66.67290431, dom-loss: 76.03789717, src-aux-loss: 117.10250133, tar-aux-loss: 106.59055847
Epoch: [3  ] train-acc: 0.77732143, dom-acc: 0.79616071, val-acc: 0.79750000, val_loss: 0.54283863
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 353.08352971, sen-loss: 59.85875466, dom-loss: 75.27361631, src-aux-loss: 113.66650510, tar-aux-loss: 104.28465360
Epoch: [4  ] train-acc: 0.81035714, dom-acc: 0.79160714, val-acc: 0.81250000, val_loss: 0.47125426
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 340.68212938, sen-loss: 52.58788210, dom-loss: 74.83769590, src-aux-loss: 110.44276243, tar-aux-loss: 102.81378990
Epoch: [5  ] train-acc: 0.83678571, dom-acc: 0.77607143, val-acc: 0.86000000, val_loss: 0.40723157
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 329.30684948, sen-loss: 46.55991757, dom-loss: 74.62638122, src-aux-loss: 107.65803564, tar-aux-loss: 100.46251488
Epoch: [6  ] train-acc: 0.85428571, dom-acc: 0.76276786, val-acc: 0.88750000, val_loss: 0.34752390
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 320.67772627, sen-loss: 41.96770971, dom-loss: 74.50221384, src-aux-loss: 105.87487578, tar-aux-loss: 98.33292747
Epoch: [7  ] train-acc: 0.86839286, dom-acc: 0.75276786, val-acc: 0.90250000, val_loss: 0.31970298
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 316.62185264, sen-loss: 39.24576426, dom-loss: 74.31210363, src-aux-loss: 104.34722120, tar-aux-loss: 98.71676469
Epoch: [8  ] train-acc: 0.87767857, dom-acc: 0.75669643, val-acc: 0.90250000, val_loss: 0.30392820
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 313.48214793, sen-loss: 37.27094541, dom-loss: 74.60239702, src-aux-loss: 102.80055165, tar-aux-loss: 98.80825508
Epoch: [9  ] train-acc: 0.88017857, dom-acc: 0.75696429, val-acc: 0.90000000, val_loss: 0.29584479
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 308.51417470, sen-loss: 35.78135031, dom-loss: 74.62033612, src-aux-loss: 101.82802725, tar-aux-loss: 96.28446114
Epoch: [10 ] train-acc: 0.88517857, dom-acc: 0.75169643, val-acc: 0.90000000, val_loss: 0.28762102
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 307.07775807, sen-loss: 34.89679584, dom-loss: 74.40582204, src-aux-loss: 100.88084495, tar-aux-loss: 96.89429593
Epoch: [11 ] train-acc: 0.88339286, dom-acc: 0.74125000, val-acc: 0.89250000, val_loss: 0.28393173
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 306.54483938, sen-loss: 34.27622744, dom-loss: 74.75647569, src-aux-loss: 100.29300052, tar-aux-loss: 97.21913600
Epoch: [12 ] train-acc: 0.88821429, dom-acc: 0.73285714, val-acc: 0.90250000, val_loss: 0.29224616
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 302.80761766, sen-loss: 33.41924866, dom-loss: 75.02858633, src-aux-loss: 99.45481461, tar-aux-loss: 94.90496570
Epoch: [13 ] train-acc: 0.89232143, dom-acc: 0.72642857, val-acc: 0.90250000, val_loss: 0.28327727
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 304.99580860, sen-loss: 33.00565121, dom-loss: 75.54913670, src-aux-loss: 99.03680164, tar-aux-loss: 97.40421832
Epoch: [14 ] train-acc: 0.89250000, dom-acc: 0.71955357, val-acc: 0.90000000, val_loss: 0.28248793
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 301.23258376, sen-loss: 32.40286984, dom-loss: 75.54745579, src-aux-loss: 98.16572696, tar-aux-loss: 95.11653036
Epoch: [15 ] train-acc: 0.89267857, dom-acc: 0.71330357, val-acc: 0.90000000, val_loss: 0.28356543
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 301.25236797, sen-loss: 32.02064418, dom-loss: 75.94310659, src-aux-loss: 97.65643793, tar-aux-loss: 95.63218135
Epoch: [16 ] train-acc: 0.89625000, dom-acc: 0.70651786, val-acc: 0.89750000, val_loss: 0.27455717
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 298.96913266, sen-loss: 31.49253489, dom-loss: 75.97467244, src-aux-loss: 96.98602015, tar-aux-loss: 94.51590639
Epoch: [17 ] train-acc: 0.89839286, dom-acc: 0.70151786, val-acc: 0.90500000, val_loss: 0.27342880
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 299.95336246, sen-loss: 31.14279465, dom-loss: 76.10441238, src-aux-loss: 96.39607733, tar-aux-loss: 96.31007808
Epoch: [18 ] train-acc: 0.89910714, dom-acc: 0.69812500, val-acc: 0.90250000, val_loss: 0.27130616
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 297.78307056, sen-loss: 30.96269893, dom-loss: 76.42649698, src-aux-loss: 96.00498354, tar-aux-loss: 94.38889235
Epoch: [19 ] train-acc: 0.89696429, dom-acc: 0.69517857, val-acc: 0.90000000, val_loss: 0.27274936
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 297.22510767, sen-loss: 30.52558628, dom-loss: 76.70548332, src-aux-loss: 95.38078326, tar-aux-loss: 94.61325353
Epoch: [20 ] train-acc: 0.90160714, dom-acc: 0.69000000, val-acc: 0.90000000, val_loss: 0.26772606
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 295.26517153, sen-loss: 30.15198076, dom-loss: 76.57784420, src-aux-loss: 94.97746664, tar-aux-loss: 93.55787790
Epoch: [21 ] train-acc: 0.90125000, dom-acc: 0.68214286, val-acc: 0.89500000, val_loss: 0.26655459
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 296.54872680, sen-loss: 29.98568108, dom-loss: 77.13414991, src-aux-loss: 94.48867530, tar-aux-loss: 94.94021893
Epoch: [22 ] train-acc: 0.90339286, dom-acc: 0.68258929, val-acc: 0.90000000, val_loss: 0.26640537
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 293.22457767, sen-loss: 29.63686461, dom-loss: 76.86880565, src-aux-loss: 93.81425208, tar-aux-loss: 92.90465504
Epoch: [23 ] train-acc: 0.90000000, dom-acc: 0.68285714, val-acc: 0.90750000, val_loss: 0.26945359
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 294.39454103, sen-loss: 29.35245600, dom-loss: 77.15231967, src-aux-loss: 93.79906851, tar-aux-loss: 94.09069705
Epoch: [24 ] train-acc: 0.90196429, dom-acc: 0.67705357, val-acc: 0.90500000, val_loss: 0.26622987
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 292.85363221, sen-loss: 28.88514461, dom-loss: 76.95244980, src-aux-loss: 93.08154178, tar-aux-loss: 93.93449610
Epoch: [25 ] train-acc: 0.90410714, dom-acc: 0.67500000, val-acc: 0.90000000, val_loss: 0.26168081
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 291.78110576, sen-loss: 28.61233374, dom-loss: 77.40596080, src-aux-loss: 92.60801870, tar-aux-loss: 93.15479255
Epoch: [26 ] train-acc: 0.90500000, dom-acc: 0.67410714, val-acc: 0.90000000, val_loss: 0.26119977
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 290.06089711, sen-loss: 28.48157503, dom-loss: 77.48543972, src-aux-loss: 92.16362756, tar-aux-loss: 91.93025565
Epoch: [27 ] train-acc: 0.90535714, dom-acc: 0.66928571, val-acc: 0.90250000, val_loss: 0.26433685
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.28372335, sen-loss: 28.15241349, dom-loss: 77.20176589, src-aux-loss: 91.81307018, tar-aux-loss: 94.11647332
Epoch: [28 ] train-acc: 0.90607143, dom-acc: 0.67303571, val-acc: 0.90750000, val_loss: 0.26743364
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 290.33526731, sen-loss: 27.85611560, dom-loss: 77.43204165, src-aux-loss: 91.44363314, tar-aux-loss: 93.60347795
Epoch: [29 ] train-acc: 0.90821429, dom-acc: 0.66767857, val-acc: 0.90500000, val_loss: 0.25949731
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.60953283, sen-loss: 27.62034629, dom-loss: 77.20818597, src-aux-loss: 90.96684903, tar-aux-loss: 93.81415176
Epoch: [30 ] train-acc: 0.90053571, dom-acc: 0.67321429, val-acc: 0.90000000, val_loss: 0.27987736
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.62493181, sen-loss: 27.44866942, dom-loss: 77.46436995, src-aux-loss: 90.75716299, tar-aux-loss: 91.95472956
Epoch: [31 ] train-acc: 0.90964286, dom-acc: 0.66767857, val-acc: 0.90500000, val_loss: 0.25736257
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 287.48216057, sen-loss: 27.17489511, dom-loss: 77.46047312, src-aux-loss: 90.28737819, tar-aux-loss: 92.55941349
Epoch: [32 ] train-acc: 0.91071429, dom-acc: 0.66205357, val-acc: 0.90750000, val_loss: 0.25622612
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 287.28546357, sen-loss: 26.91216527, dom-loss: 77.33894056, src-aux-loss: 89.96464324, tar-aux-loss: 93.06971526
Epoch: [33 ] train-acc: 0.90910714, dom-acc: 0.67357143, val-acc: 0.91000000, val_loss: 0.26079202
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 286.59007502, sen-loss: 26.63663590, dom-loss: 77.49267811, src-aux-loss: 89.36146992, tar-aux-loss: 93.09929115
Epoch: [34 ] train-acc: 0.91285714, dom-acc: 0.66383929, val-acc: 0.90750000, val_loss: 0.25789699
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 283.93292022, sen-loss: 26.54686795, dom-loss: 77.13864452, src-aux-loss: 89.18695569, tar-aux-loss: 91.06045151
Epoch: [35 ] train-acc: 0.91321429, dom-acc: 0.67348214, val-acc: 0.90750000, val_loss: 0.26072666
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 284.94345498, sen-loss: 26.29446826, dom-loss: 77.36509758, src-aux-loss: 88.75470579, tar-aux-loss: 92.52918321
Epoch: [36 ] train-acc: 0.91500000, dom-acc: 0.66866071, val-acc: 0.90750000, val_loss: 0.25674894
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 282.73031878, sen-loss: 25.94438746, dom-loss: 76.89417076, src-aux-loss: 87.97731799, tar-aux-loss: 91.91444123
Epoch: [37 ] train-acc: 0.91625000, dom-acc: 0.66848214, val-acc: 0.90500000, val_loss: 0.25325572
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 282.77427077, sen-loss: 25.77920348, dom-loss: 77.22692925, src-aux-loss: 87.73589915, tar-aux-loss: 92.03223759
Epoch: [38 ] train-acc: 0.91607143, dom-acc: 0.67455357, val-acc: 0.91250000, val_loss: 0.25701135
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 281.18623137, sen-loss: 25.44172791, dom-loss: 77.28760958, src-aux-loss: 86.96578979, tar-aux-loss: 91.49110454
Epoch: [39 ] train-acc: 0.91696429, dom-acc: 0.67651786, val-acc: 0.90750000, val_loss: 0.25456411
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 280.12897134, sen-loss: 25.24467379, dom-loss: 77.03077871, src-aux-loss: 86.70563126, tar-aux-loss: 91.14788759
Epoch: [40 ] train-acc: 0.92000000, dom-acc: 0.67982143, val-acc: 0.90750000, val_loss: 0.25548282
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 280.18256664, sen-loss: 24.94585892, dom-loss: 76.76452470, src-aux-loss: 86.39954561, tar-aux-loss: 92.07263547
Epoch: [41 ] train-acc: 0.91946429, dom-acc: 0.68169643, val-acc: 0.91250000, val_loss: 0.25382710
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 279.53171563, sen-loss: 24.69787894, dom-loss: 77.06631714, src-aux-loss: 85.61008030, tar-aux-loss: 92.15743780
Epoch: [42 ] train-acc: 0.92178571, dom-acc: 0.68232143, val-acc: 0.91000000, val_loss: 0.25126088
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 278.35117579, sen-loss: 24.50405534, dom-loss: 76.93338990, src-aux-loss: 85.52067101, tar-aux-loss: 91.39305896
Epoch: [43 ] train-acc: 0.92285714, dom-acc: 0.68446429, val-acc: 0.90750000, val_loss: 0.25778839
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 278.37569880, sen-loss: 24.38034788, dom-loss: 76.57754344, src-aux-loss: 85.27318364, tar-aux-loss: 92.14462298
Epoch: [44 ] train-acc: 0.92392857, dom-acc: 0.68616071, val-acc: 0.91250000, val_loss: 0.25084779
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 275.24696732, sen-loss: 24.02216484, dom-loss: 76.66951352, src-aux-loss: 84.68003488, tar-aux-loss: 89.87525427
Epoch: [45 ] train-acc: 0.92196429, dom-acc: 0.68741071, val-acc: 0.90750000, val_loss: 0.26451746
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 276.44572353, sen-loss: 23.89672244, dom-loss: 76.83982098, src-aux-loss: 84.32371122, tar-aux-loss: 91.38546813
Epoch: [46 ] train-acc: 0.92446429, dom-acc: 0.68794643, val-acc: 0.90750000, val_loss: 0.24958488
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 275.20587134, sen-loss: 23.50254314, dom-loss: 76.69523746, src-aux-loss: 83.51404208, tar-aux-loss: 91.49404895
Epoch: [47 ] train-acc: 0.91946429, dom-acc: 0.67892857, val-acc: 0.90500000, val_loss: 0.25143248
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 273.76377416, sen-loss: 23.32001580, dom-loss: 76.62886953, src-aux-loss: 83.08460599, tar-aux-loss: 90.73028219
Epoch: [48 ] train-acc: 0.92785714, dom-acc: 0.68223214, val-acc: 0.91000000, val_loss: 0.25043970
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 274.50718403, sen-loss: 23.06383473, dom-loss: 76.78784573, src-aux-loss: 82.86282659, tar-aux-loss: 91.79267830
Epoch: [49 ] train-acc: 0.92982143, dom-acc: 0.68776786, val-acc: 0.91000000, val_loss: 0.25175825
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 271.67035866, sen-loss: 23.02915297, dom-loss: 76.64206338, src-aux-loss: 82.14830446, tar-aux-loss: 89.85083675
Epoch: [50 ] train-acc: 0.93053571, dom-acc: 0.68964286, val-acc: 0.91000000, val_loss: 0.25673580
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 271.99237895, sen-loss: 22.65593141, dom-loss: 76.70623767, src-aux-loss: 81.69120300, tar-aux-loss: 90.93900639
Epoch: [51 ] train-acc: 0.93125000, dom-acc: 0.69000000, val-acc: 0.91250000, val_loss: 0.25621650
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_books_HATN.ckpt
Best Epoch: [ 46] best val accuracy: 0.00000000 best val loss: 0.24958488
Testing accuracy: 0.88283333
./work/attentions/dvd_books_train_HATN.txt
./work/attentions/dvd_books_test_HATN.txt
loading data...
source domain:  dvd target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 17009
vocab-size:  85442
['good', 'best', 'great', 'excellent', 'funny', 'enjoyable', 'entertaining', 'better', 'awesome', 'fantastic', 'love', 'classic', 'nice', 'amazing', 'wonderful', 'perfect', 'hilarious', 'brilliant', 'funniest', 'underrated', 'loved', 'favorite', 'superb', 'interesting', 'sad', 'greatest', 'terrific', 'finest', 'real', 'solid', 'easy', 'incredible', 'memorable', 'fine', 'fascinating', 'liked', 'spectacular', 'impressive', 'cute']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'wasted', 'dull', 'waste', 'stupid', 'pathetic', 'unwatchable', 'annoying', 'unfunny', 'worse', 'laughable', 'forgettable', 'predictable', 'decent', 'lousy', 'ruined', 'uninspired', 'pointless', 'slow', 'wrong', 'overrated', 'dismal', 'sucks', 'sucked', 'silly', 'poorly', 'cheap', 'unoriginal', 'ok', 'crappy', 'mediocre', 'lacking', 'atrocious', 'lame', 'defective', 'disgusting', 'frustrating', 'okay', 'ridiculous', 'unconvincing', 'depressing', 'horrendous', 'biased', 'weak', 'disapointed', 'dumbest', 'hated', 'contrived', 'lackluster', 'insipid', 'inferior']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
85442
5600 400 6000 17843 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.81441712, sen-loss: 78.28796816, dom-loss: 79.10348880, src-aux-loss: 129.90403193, tar-aux-loss: 117.51892787
Epoch: [1  ] train-acc: 0.64071429, dom-acc: 0.74455357, val-acc: 0.66000000, val_loss: 0.66586071
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 381.12674189, sen-loss: 72.88289624, dom-loss: 75.11709994, src-aux-loss: 121.87645143, tar-aux-loss: 111.25029188
Epoch: [2  ] train-acc: 0.73160714, dom-acc: 0.85973214, val-acc: 0.75000000, val_loss: 0.61132246
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 364.28562975, sen-loss: 67.05809486, dom-loss: 73.36450124, src-aux-loss: 117.16165864, tar-aux-loss: 106.70137465
Epoch: [3  ] train-acc: 0.77714286, dom-acc: 0.85526786, val-acc: 0.80000000, val_loss: 0.54696256
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 351.53580475, sen-loss: 60.17582697, dom-loss: 72.97274059, src-aux-loss: 113.76706070, tar-aux-loss: 104.62017572
Epoch: [4  ] train-acc: 0.80482143, dom-acc: 0.84455357, val-acc: 0.81000000, val_loss: 0.47132710
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 338.83957314, sen-loss: 52.63471857, dom-loss: 73.18320811, src-aux-loss: 110.61331689, tar-aux-loss: 102.40832961
Epoch: [5  ] train-acc: 0.83750000, dom-acc: 0.82901786, val-acc: 0.85500000, val_loss: 0.40501773
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 328.23775029, sen-loss: 46.51665926, dom-loss: 73.93977261, src-aux-loss: 107.66288799, tar-aux-loss: 100.11843151
Epoch: [6  ] train-acc: 0.85500000, dom-acc: 0.81848214, val-acc: 0.88250000, val_loss: 0.34431145
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 322.01916504, sen-loss: 41.70730495, dom-loss: 74.87298161, src-aux-loss: 105.99062407, tar-aux-loss: 99.44825739
Epoch: [7  ] train-acc: 0.86857143, dom-acc: 0.76285714, val-acc: 0.90250000, val_loss: 0.31644577
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 318.24422550, sen-loss: 38.82272424, dom-loss: 75.94101346, src-aux-loss: 104.32667500, tar-aux-loss: 99.15381384
Epoch: [8  ] train-acc: 0.87892857, dom-acc: 0.69312500, val-acc: 0.90750000, val_loss: 0.30052069
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 315.94471192, sen-loss: 37.12735406, dom-loss: 77.33691692, src-aux-loss: 103.03212124, tar-aux-loss: 98.44831848
Epoch: [9  ] train-acc: 0.88142857, dom-acc: 0.64375000, val-acc: 0.90250000, val_loss: 0.29368410
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 314.44756055, sen-loss: 35.72645895, dom-loss: 78.73567575, src-aux-loss: 101.92467362, tar-aux-loss: 98.06074965
Epoch: [10 ] train-acc: 0.88500000, dom-acc: 0.57455357, val-acc: 0.89250000, val_loss: 0.28688875
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 311.39768744, sen-loss: 34.76146270, dom-loss: 79.12691534, src-aux-loss: 100.84221947, tar-aux-loss: 96.66709000
Epoch: [11 ] train-acc: 0.88500000, dom-acc: 0.56142857, val-acc: 0.89250000, val_loss: 0.28370747
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 310.98099351, sen-loss: 34.17185830, dom-loss: 79.71673214, src-aux-loss: 100.21598178, tar-aux-loss: 96.87642318
Epoch: [12 ] train-acc: 0.88714286, dom-acc: 0.55241071, val-acc: 0.90250000, val_loss: 0.29109785
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 309.11963105, sen-loss: 33.41510193, dom-loss: 79.90949547, src-aux-loss: 99.62431473, tar-aux-loss: 96.17071807
Epoch: [13 ] train-acc: 0.89053571, dom-acc: 0.53901786, val-acc: 0.90250000, val_loss: 0.28253040
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 308.41496515, sen-loss: 32.92896913, dom-loss: 79.73200464, src-aux-loss: 98.81366754, tar-aux-loss: 96.94032627
Epoch: [14 ] train-acc: 0.89125000, dom-acc: 0.53866071, val-acc: 0.90000000, val_loss: 0.27795473
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 305.91669512, sen-loss: 32.38161916, dom-loss: 79.27181572, src-aux-loss: 98.29728085, tar-aux-loss: 95.96597880
Epoch: [15 ] train-acc: 0.89071429, dom-acc: 0.55973214, val-acc: 0.90000000, val_loss: 0.28223598
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 304.47603512, sen-loss: 32.07673563, dom-loss: 78.86310256, src-aux-loss: 97.78272510, tar-aux-loss: 95.75347006
Epoch: [16 ] train-acc: 0.89714286, dom-acc: 0.57500000, val-acc: 0.90250000, val_loss: 0.27241287
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 301.92703986, sen-loss: 31.50194658, dom-loss: 78.47566855, src-aux-loss: 96.89763862, tar-aux-loss: 95.05178583
Epoch: [17 ] train-acc: 0.89517857, dom-acc: 0.59437500, val-acc: 0.90000000, val_loss: 0.27331147
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 301.56823635, sen-loss: 31.14238808, dom-loss: 77.96062779, src-aux-loss: 96.49197811, tar-aux-loss: 95.97324264
Epoch: [18 ] train-acc: 0.89910714, dom-acc: 0.61794643, val-acc: 0.90000000, val_loss: 0.27031994
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 299.79164433, sen-loss: 30.98369051, dom-loss: 77.63174325, src-aux-loss: 96.05491161, tar-aux-loss: 95.12129843
Epoch: [19 ] train-acc: 0.89642857, dom-acc: 0.61955357, val-acc: 0.90500000, val_loss: 0.27272832
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 298.14606380, sen-loss: 30.60361092, dom-loss: 77.56735504, src-aux-loss: 95.48991394, tar-aux-loss: 94.48518455
Epoch: [20 ] train-acc: 0.90232143, dom-acc: 0.62401786, val-acc: 0.89500000, val_loss: 0.26577383
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 296.00710177, sen-loss: 30.22377634, dom-loss: 77.51454496, src-aux-loss: 94.97137451, tar-aux-loss: 93.29740661
Epoch: [21 ] train-acc: 0.90160714, dom-acc: 0.63258929, val-acc: 0.89500000, val_loss: 0.26452184
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 297.03718352, sen-loss: 30.04260295, dom-loss: 77.51133913, src-aux-loss: 94.62554979, tar-aux-loss: 94.85769033
Epoch: [22 ] train-acc: 0.90196429, dom-acc: 0.62294643, val-acc: 0.89750000, val_loss: 0.26520285
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 294.80748916, sen-loss: 29.71093484, dom-loss: 77.51111722, src-aux-loss: 93.87835515, tar-aux-loss: 93.70708239
Epoch: [23 ] train-acc: 0.89946429, dom-acc: 0.63267857, val-acc: 0.90750000, val_loss: 0.27001122
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 295.00503182, sen-loss: 29.38707814, dom-loss: 77.58496952, src-aux-loss: 93.84790152, tar-aux-loss: 94.18508142
Epoch: [24 ] train-acc: 0.90410714, dom-acc: 0.61580357, val-acc: 0.90000000, val_loss: 0.26235265
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 294.57289386, sen-loss: 29.00667316, dom-loss: 78.00254285, src-aux-loss: 93.18542194, tar-aux-loss: 94.37825620
Epoch: [25 ] train-acc: 0.90285714, dom-acc: 0.60616071, val-acc: 0.89250000, val_loss: 0.25895527
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.61849332, sen-loss: 28.70890465, dom-loss: 78.15189505, src-aux-loss: 92.72181082, tar-aux-loss: 94.03588247
Epoch: [26 ] train-acc: 0.90500000, dom-acc: 0.59214286, val-acc: 0.90000000, val_loss: 0.25914103
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 291.50191975, sen-loss: 28.58227937, dom-loss: 78.23720002, src-aux-loss: 92.29960108, tar-aux-loss: 92.38283902
Epoch: [27 ] train-acc: 0.90392857, dom-acc: 0.57053571, val-acc: 0.90500000, val_loss: 0.26208037
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.41761827, sen-loss: 28.18652350, dom-loss: 78.46683210, src-aux-loss: 91.85063499, tar-aux-loss: 92.91362846
Epoch: [28 ] train-acc: 0.90071429, dom-acc: 0.57312500, val-acc: 0.91000000, val_loss: 0.27140680
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 292.12909532, sen-loss: 27.97040488, dom-loss: 78.50872868, src-aux-loss: 91.43295604, tar-aux-loss: 94.21700609
Epoch: [29 ] train-acc: 0.90767857, dom-acc: 0.56669643, val-acc: 0.90250000, val_loss: 0.25628620
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.93293548, sen-loss: 27.74689402, dom-loss: 78.51899272, src-aux-loss: 91.20904309, tar-aux-loss: 93.45800567
Epoch: [30 ] train-acc: 0.90267857, dom-acc: 0.58035714, val-acc: 0.91250000, val_loss: 0.27267033
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 288.94591475, sen-loss: 27.49197336, dom-loss: 78.59849662, src-aux-loss: 90.78262156, tar-aux-loss: 92.07282472
Epoch: [31 ] train-acc: 0.90803571, dom-acc: 0.56857143, val-acc: 0.90500000, val_loss: 0.25777835
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.75503778, sen-loss: 27.30139400, dom-loss: 78.43094993, src-aux-loss: 90.42292142, tar-aux-loss: 92.59977204
Epoch: [32 ] train-acc: 0.90964286, dom-acc: 0.57410714, val-acc: 0.89250000, val_loss: 0.25329873
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 288.23197031, sen-loss: 26.98637988, dom-loss: 78.38034844, src-aux-loss: 90.19342744, tar-aux-loss: 92.67181438
Epoch: [33 ] train-acc: 0.90821429, dom-acc: 0.59330357, val-acc: 0.90750000, val_loss: 0.25976095
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 288.25296974, sen-loss: 26.71486524, dom-loss: 78.29243380, src-aux-loss: 89.56129271, tar-aux-loss: 93.68437666
Epoch: [34 ] train-acc: 0.91196429, dom-acc: 0.56473214, val-acc: 0.89750000, val_loss: 0.25339314
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 286.09019542, sen-loss: 26.53181016, dom-loss: 78.00155467, src-aux-loss: 89.34548301, tar-aux-loss: 92.21134752
Epoch: [35 ] train-acc: 0.91196429, dom-acc: 0.60901786, val-acc: 0.90500000, val_loss: 0.25923082
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 285.47707248, sen-loss: 26.27621736, dom-loss: 77.90124977, src-aux-loss: 88.82271624, tar-aux-loss: 92.47688985
Epoch: [36 ] train-acc: 0.91446429, dom-acc: 0.63508929, val-acc: 0.90250000, val_loss: 0.25201565
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 282.98246765, sen-loss: 26.04695765, dom-loss: 77.65819561, src-aux-loss: 88.20922345, tar-aux-loss: 91.06809294
Epoch: [37 ] train-acc: 0.91375000, dom-acc: 0.64375000, val-acc: 0.89750000, val_loss: 0.25067085
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 284.29622817, sen-loss: 25.87063608, dom-loss: 77.40959316, src-aux-loss: 88.11900598, tar-aux-loss: 92.89699256
Epoch: [38 ] train-acc: 0.91660714, dom-acc: 0.64125000, val-acc: 0.90500000, val_loss: 0.25499320
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 282.67217350, sen-loss: 25.50689679, dom-loss: 77.39105088, src-aux-loss: 87.45072502, tar-aux-loss: 92.32350087
Epoch: [39 ] train-acc: 0.91642857, dom-acc: 0.65723214, val-acc: 0.90250000, val_loss: 0.24926120
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 280.13893390, sen-loss: 25.32918222, dom-loss: 77.31795263, src-aux-loss: 87.10350114, tar-aux-loss: 90.38829952
Epoch: [40 ] train-acc: 0.91696429, dom-acc: 0.64660714, val-acc: 0.90000000, val_loss: 0.24997485
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 280.63685584, sen-loss: 25.05067082, dom-loss: 77.22457230, src-aux-loss: 86.56319571, tar-aux-loss: 91.79841834
Epoch: [41 ] train-acc: 0.92053571, dom-acc: 0.67035714, val-acc: 0.90500000, val_loss: 0.25260597
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 280.03471065, sen-loss: 24.77058507, dom-loss: 77.33237076, src-aux-loss: 86.06191307, tar-aux-loss: 91.86984199
Epoch: [42 ] train-acc: 0.92000000, dom-acc: 0.65196429, val-acc: 0.90250000, val_loss: 0.24873282
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 279.06297207, sen-loss: 24.62727471, dom-loss: 77.39456129, src-aux-loss: 85.81549704, tar-aux-loss: 91.22563863
Epoch: [43 ] train-acc: 0.92303571, dom-acc: 0.62160714, val-acc: 0.90500000, val_loss: 0.25209451
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 279.50115299, sen-loss: 24.55053929, dom-loss: 77.48960012, src-aux-loss: 85.73507553, tar-aux-loss: 91.72593790
Epoch: [44 ] train-acc: 0.92392857, dom-acc: 0.62285714, val-acc: 0.90750000, val_loss: 0.24660546
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 277.69793367, sen-loss: 24.09576713, dom-loss: 77.59072244, src-aux-loss: 85.05973607, tar-aux-loss: 90.95170844
Epoch: [45 ] train-acc: 0.92428571, dom-acc: 0.62339286, val-acc: 0.91250000, val_loss: 0.25701037
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 278.84349358, sen-loss: 24.00394254, dom-loss: 77.69042099, src-aux-loss: 84.58018976, tar-aux-loss: 92.56893963
Epoch: [46 ] train-acc: 0.92535714, dom-acc: 0.58312500, val-acc: 0.91000000, val_loss: 0.24729955
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 275.50524712, sen-loss: 23.60626867, dom-loss: 77.73273933, src-aux-loss: 84.04626089, tar-aux-loss: 90.11997813
Epoch: [47 ] train-acc: 0.91982143, dom-acc: 0.59616071, val-acc: 0.91000000, val_loss: 0.24712712
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 276.46947336, sen-loss: 23.47725294, dom-loss: 77.80959851, src-aux-loss: 83.78695208, tar-aux-loss: 91.39567155
Epoch: [48 ] train-acc: 0.92589286, dom-acc: 0.58026786, val-acc: 0.90500000, val_loss: 0.24667136
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 276.68145943, sen-loss: 23.13573460, dom-loss: 77.89328599, src-aux-loss: 83.37885582, tar-aux-loss: 92.27358377
Epoch: [49 ] train-acc: 0.92803571, dom-acc: 0.57946429, val-acc: 0.90750000, val_loss: 0.24881080
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_electronics_HATN.ckpt
Best Epoch: [ 44] best val accuracy: 0.00000000 best val loss: 0.24660546
Testing accuracy: 0.86700000
./work/attentions/dvd_electronics_train_HATN.txt
./work/attentions/dvd_electronics_test_HATN.txt
loading data...
source domain:  dvd target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 13856
vocab-size:  80685
['good', 'best', 'great', 'excellent', 'funny', 'enjoyable', 'entertaining', 'better', 'love', 'awesome', 'fantastic', 'classic', 'nice', 'wonderful', 'amazing', 'perfect', 'hilarious', 'brilliant', 'funniest', 'loved', 'underrated', 'favorite', 'superb', 'sad', 'greatest', 'interesting', 'terrific', 'finest', 'real', 'solid', 'easy', 'incredible', 'memorable', 'fine', 'simplistic', 'fascinating', 'liked', 'impressive', 'cute']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'wasted', 'dull', 'waste', 'stupid', 'pathetic', 'worse', 'unwatchable', 'unfunny', 'forgettable', 'predictable', 'annoying', 'lousy', 'laughable', 'decent', 'ruined', 'uninspired', 'slow', 'sucked', 'wrong', 'pointless', 'overrated', 'dismal', 'sucks', 'poorly', 'cheap', 'unoriginal', 'ok', 'silly', 'mediocre', 'crappy', 'defective', 'lacking', 'atrocious', 'lame', 'disgusting', 'hated', 'frustrating', 'contrived', 'unconvincing', 'dreadful', 'depressing', 'horrendous', 'disjointed', 'disapointed', 'dumbest', 'okay', 'ridiculous', 'insipid', 'inferior']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
80685
5600 400 6000 17843 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 401.10386777, sen-loss: 78.28296798, dom-loss: 78.90460938, src-aux-loss: 130.80247205, tar-aux-loss: 113.11381888
Epoch: [1  ] train-acc: 0.63821429, dom-acc: 0.73133929, val-acc: 0.65500000, val_loss: 0.66621780
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 376.17240167, sen-loss: 72.88192540, dom-loss: 75.33203387, src-aux-loss: 122.60083228, tar-aux-loss: 105.35761303
Epoch: [2  ] train-acc: 0.72357143, dom-acc: 0.84250000, val-acc: 0.74750000, val_loss: 0.61156309
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 358.58519006, sen-loss: 67.04686016, dom-loss: 73.70986384, src-aux-loss: 117.56978834, tar-aux-loss: 100.25867677
Epoch: [3  ] train-acc: 0.77821429, dom-acc: 0.84473214, val-acc: 0.79250000, val_loss: 0.54652512
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 345.43244171, sen-loss: 60.13321477, dom-loss: 73.30757201, src-aux-loss: 113.96983588, tar-aux-loss: 98.02181882
Epoch: [4  ] train-acc: 0.80553571, dom-acc: 0.81892857, val-acc: 0.80500000, val_loss: 0.47245347
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 333.57166910, sen-loss: 52.64600673, dom-loss: 73.63186270, src-aux-loss: 110.79775578, tar-aux-loss: 96.49604189
Epoch: [5  ] train-acc: 0.83803571, dom-acc: 0.78232143, val-acc: 0.85250000, val_loss: 0.40264145
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 323.13513541, sen-loss: 46.47677860, dom-loss: 73.89211094, src-aux-loss: 107.88353807, tar-aux-loss: 94.88270879
Epoch: [6  ] train-acc: 0.85375000, dom-acc: 0.73250000, val-acc: 0.88750000, val_loss: 0.34624600
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 314.79634047, sen-loss: 41.71526997, dom-loss: 74.70648438, src-aux-loss: 105.96865290, tar-aux-loss: 92.40593231
Epoch: [7  ] train-acc: 0.86982143, dom-acc: 0.69419643, val-acc: 0.90000000, val_loss: 0.31752813
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 313.06037641, sen-loss: 39.04943889, dom-loss: 75.83111477, src-aux-loss: 104.45763010, tar-aux-loss: 93.72219330
Epoch: [8  ] train-acc: 0.87571429, dom-acc: 0.66580357, val-acc: 0.90000000, val_loss: 0.30556962
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 308.58378267, sen-loss: 37.10770182, dom-loss: 76.54334575, src-aux-loss: 102.83314753, tar-aux-loss: 92.09958696
Epoch: [9  ] train-acc: 0.88214286, dom-acc: 0.64723214, val-acc: 0.90250000, val_loss: 0.29583690
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 307.02728605, sen-loss: 35.70347969, dom-loss: 77.95628387, src-aux-loss: 101.87128514, tar-aux-loss: 91.49623692
Epoch: [10 ] train-acc: 0.88571429, dom-acc: 0.62383929, val-acc: 0.89500000, val_loss: 0.28649560
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 304.95682383, sen-loss: 34.79445319, dom-loss: 77.90451258, src-aux-loss: 100.70076734, tar-aux-loss: 91.55709171
Epoch: [11 ] train-acc: 0.88464286, dom-acc: 0.61660714, val-acc: 0.89000000, val_loss: 0.28312504
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 303.40704584, sen-loss: 34.16513693, dom-loss: 78.24351895, src-aux-loss: 100.02560967, tar-aux-loss: 90.97278041
Epoch: [12 ] train-acc: 0.88321429, dom-acc: 0.62142857, val-acc: 0.89250000, val_loss: 0.29455328
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 301.94916224, sen-loss: 33.46336985, dom-loss: 78.37878203, src-aux-loss: 99.41721398, tar-aux-loss: 90.68979859
Epoch: [13 ] train-acc: 0.89071429, dom-acc: 0.61651786, val-acc: 0.90750000, val_loss: 0.27902609
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 299.95083189, sen-loss: 32.93493864, dom-loss: 78.27849501, src-aux-loss: 98.63889641, tar-aux-loss: 90.09850287
Epoch: [14 ] train-acc: 0.89125000, dom-acc: 0.62553571, val-acc: 0.90000000, val_loss: 0.27885154
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 298.56094337, sen-loss: 32.37832437, dom-loss: 77.81818312, src-aux-loss: 97.79839951, tar-aux-loss: 90.56603789
Epoch: [15 ] train-acc: 0.89107143, dom-acc: 0.63848214, val-acc: 0.90000000, val_loss: 0.27768388
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 297.68106675, sen-loss: 32.06215881, dom-loss: 77.74814099, src-aux-loss: 97.41105014, tar-aux-loss: 90.45971572
Epoch: [16 ] train-acc: 0.89714286, dom-acc: 0.64125000, val-acc: 0.90750000, val_loss: 0.27149883
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 293.68915844, sen-loss: 31.45552790, dom-loss: 77.30324745, src-aux-loss: 96.48636347, tar-aux-loss: 88.44401878
Epoch: [17 ] train-acc: 0.89696429, dom-acc: 0.65419643, val-acc: 0.90500000, val_loss: 0.27086410
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 293.55556250, sen-loss: 31.10161577, dom-loss: 76.75531203, src-aux-loss: 95.94200748, tar-aux-loss: 89.75662744
Epoch: [18 ] train-acc: 0.89928571, dom-acc: 0.65178571, val-acc: 0.90250000, val_loss: 0.26760724
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 291.95161295, sen-loss: 30.99830914, dom-loss: 76.88332319, src-aux-loss: 95.68213254, tar-aux-loss: 88.38784838
Epoch: [19 ] train-acc: 0.89571429, dom-acc: 0.65758929, val-acc: 0.90500000, val_loss: 0.27043238
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 292.10010386, sen-loss: 30.58800316, dom-loss: 76.98144400, src-aux-loss: 95.06154752, tar-aux-loss: 89.46911061
Epoch: [20 ] train-acc: 0.90232143, dom-acc: 0.65919643, val-acc: 0.90000000, val_loss: 0.26300642
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 289.66237211, sen-loss: 30.18116367, dom-loss: 76.79798639, src-aux-loss: 94.53391820, tar-aux-loss: 88.14930475
Epoch: [21 ] train-acc: 0.90250000, dom-acc: 0.65687500, val-acc: 0.90000000, val_loss: 0.26228699
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 289.03085923, sen-loss: 29.94155635, dom-loss: 77.09513539, src-aux-loss: 94.11812371, tar-aux-loss: 87.87604374
Epoch: [22 ] train-acc: 0.90339286, dom-acc: 0.65044643, val-acc: 0.90000000, val_loss: 0.26192760
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 288.13803339, sen-loss: 29.66762933, dom-loss: 76.95324409, src-aux-loss: 93.51843178, tar-aux-loss: 87.99872768
Epoch: [23 ] train-acc: 0.89625000, dom-acc: 0.64821429, val-acc: 0.90750000, val_loss: 0.27110851
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 288.87589765, sen-loss: 29.34713113, dom-loss: 77.47432053, src-aux-loss: 93.27940190, tar-aux-loss: 88.77504307
Epoch: [24 ] train-acc: 0.90392857, dom-acc: 0.63544643, val-acc: 0.90250000, val_loss: 0.25988933
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 285.96985126, sen-loss: 28.88411854, dom-loss: 77.60976130, src-aux-loss: 92.66045034, tar-aux-loss: 86.81552112
Epoch: [25 ] train-acc: 0.90375000, dom-acc: 0.62517857, val-acc: 0.90000000, val_loss: 0.25647399
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 286.60911512, sen-loss: 28.57622901, dom-loss: 77.86231101, src-aux-loss: 92.19847918, tar-aux-loss: 87.97209597
Epoch: [26 ] train-acc: 0.90571429, dom-acc: 0.62008929, val-acc: 0.90500000, val_loss: 0.25621700
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 284.67678952, sen-loss: 28.55400697, dom-loss: 78.22046858, src-aux-loss: 91.65529364, tar-aux-loss: 86.24702132
Epoch: [27 ] train-acc: 0.90285714, dom-acc: 0.61446429, val-acc: 0.90500000, val_loss: 0.26244640
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 286.43470216, sen-loss: 28.16949262, dom-loss: 78.37905067, src-aux-loss: 91.27734828, tar-aux-loss: 88.60881072
Epoch: [28 ] train-acc: 0.90142857, dom-acc: 0.60892857, val-acc: 0.90500000, val_loss: 0.26907608
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 284.05846000, sen-loss: 27.84696272, dom-loss: 78.58078289, src-aux-loss: 90.86788535, tar-aux-loss: 86.76282924
Epoch: [29 ] train-acc: 0.90714286, dom-acc: 0.59607143, val-acc: 0.90750000, val_loss: 0.25436074
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 285.22770500, sen-loss: 27.63764696, dom-loss: 78.45461774, src-aux-loss: 90.49098808, tar-aux-loss: 88.64445293
Epoch: [30 ] train-acc: 0.90178571, dom-acc: 0.60973214, val-acc: 0.90750000, val_loss: 0.27299830
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 282.50590110, sen-loss: 27.47300667, dom-loss: 78.59506255, src-aux-loss: 90.06282949, tar-aux-loss: 86.37500221
Epoch: [31 ] train-acc: 0.90982143, dom-acc: 0.60312500, val-acc: 0.90500000, val_loss: 0.25310743
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 281.47851467, sen-loss: 27.22476260, dom-loss: 78.36769938, src-aux-loss: 89.91451079, tar-aux-loss: 85.97154033
Epoch: [32 ] train-acc: 0.91125000, dom-acc: 0.60312500, val-acc: 0.90250000, val_loss: 0.24994582
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 280.89456201, sen-loss: 26.89470945, dom-loss: 78.22100562, src-aux-loss: 89.37415528, tar-aux-loss: 86.40469235
Epoch: [33 ] train-acc: 0.90892857, dom-acc: 0.62160714, val-acc: 0.90750000, val_loss: 0.25319746
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 280.40171242, sen-loss: 26.65407479, dom-loss: 78.17416430, src-aux-loss: 88.85365564, tar-aux-loss: 86.71981764
Epoch: [34 ] train-acc: 0.91232143, dom-acc: 0.61821429, val-acc: 0.90500000, val_loss: 0.24959928
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 279.65349436, sen-loss: 26.47781318, dom-loss: 77.78172350, src-aux-loss: 88.82566392, tar-aux-loss: 86.56829214
Epoch: [35 ] train-acc: 0.91267857, dom-acc: 0.64196429, val-acc: 0.91000000, val_loss: 0.25269705
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 278.16850710, sen-loss: 26.25150999, dom-loss: 77.45178157, src-aux-loss: 88.07638443, tar-aux-loss: 86.38883138
Epoch: [36 ] train-acc: 0.91589286, dom-acc: 0.64750000, val-acc: 0.90750000, val_loss: 0.24984595
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 276.73645091, sen-loss: 25.94663143, dom-loss: 77.13900167, src-aux-loss: 87.49580538, tar-aux-loss: 86.15501171
Epoch: [37 ] train-acc: 0.91410714, dom-acc: 0.65517857, val-acc: 0.90000000, val_loss: 0.24707735
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 275.92024422, sen-loss: 25.71763933, dom-loss: 76.81575662, src-aux-loss: 87.31081945, tar-aux-loss: 86.07602841
Epoch: [38 ] train-acc: 0.91660714, dom-acc: 0.67357143, val-acc: 0.90750000, val_loss: 0.25229409
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 275.40934896, sen-loss: 25.41374367, dom-loss: 76.83182442, src-aux-loss: 86.80266440, tar-aux-loss: 86.36111641
Epoch: [39 ] train-acc: 0.91732143, dom-acc: 0.68473214, val-acc: 0.91000000, val_loss: 0.24570960
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 272.66859937, sen-loss: 25.24457514, dom-loss: 76.60200816, src-aux-loss: 86.27647001, tar-aux-loss: 84.54554701
Epoch: [40 ] train-acc: 0.91892857, dom-acc: 0.68910714, val-acc: 0.90500000, val_loss: 0.24470539
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 272.14135075, sen-loss: 25.10639600, dom-loss: 76.47853827, src-aux-loss: 85.98011535, tar-aux-loss: 84.57630122
Epoch: [41 ] train-acc: 0.92000000, dom-acc: 0.67857143, val-acc: 0.91250000, val_loss: 0.25242803
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 273.31427717, sen-loss: 24.69981010, dom-loss: 76.66187686, src-aux-loss: 85.22190988, tar-aux-loss: 86.73068011
Epoch: [42 ] train-acc: 0.91928571, dom-acc: 0.67928571, val-acc: 0.91000000, val_loss: 0.24509472
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 270.90229225, sen-loss: 24.51051948, dom-loss: 76.79666460, src-aux-loss: 85.10383570, tar-aux-loss: 84.49127293
Epoch: [43 ] train-acc: 0.92053571, dom-acc: 0.66955357, val-acc: 0.90750000, val_loss: 0.24852583
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 273.57779551, sen-loss: 24.39662720, dom-loss: 77.06683654, src-aux-loss: 84.97749794, tar-aux-loss: 87.13683468
Epoch: [44 ] train-acc: 0.92232143, dom-acc: 0.65812500, val-acc: 0.90750000, val_loss: 0.24382697
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 269.41838479, sen-loss: 24.04437606, dom-loss: 77.12028253, src-aux-loss: 84.08955163, tar-aux-loss: 84.16417396
Epoch: [45 ] train-acc: 0.92250000, dom-acc: 0.64562500, val-acc: 0.91250000, val_loss: 0.25535876
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 270.66236830, sen-loss: 23.91002061, dom-loss: 77.56559223, src-aux-loss: 83.76769346, tar-aux-loss: 85.41906244
Epoch: [46 ] train-acc: 0.92303571, dom-acc: 0.61767857, val-acc: 0.91000000, val_loss: 0.24342589
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 270.16108918, sen-loss: 23.52168806, dom-loss: 77.76171565, src-aux-loss: 83.19291663, tar-aux-loss: 85.68476921
Epoch: [47 ] train-acc: 0.91821429, dom-acc: 0.58375000, val-acc: 0.91000000, val_loss: 0.24580146
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 268.78984523, sen-loss: 23.41386138, dom-loss: 78.02556407, src-aux-loss: 82.83831149, tar-aux-loss: 84.51210910
Epoch: [48 ] train-acc: 0.92660714, dom-acc: 0.57651786, val-acc: 0.91500000, val_loss: 0.24315944
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 270.05750895, sen-loss: 23.04364432, dom-loss: 78.41378003, src-aux-loss: 82.43770015, tar-aux-loss: 86.16238469
Epoch: [49 ] train-acc: 0.92767857, dom-acc: 0.54848214, val-acc: 0.90750000, val_loss: 0.24752408
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 266.83253646, sen-loss: 22.98919208, dom-loss: 78.52037746, src-aux-loss: 81.81513840, tar-aux-loss: 83.50782859
Epoch: [50 ] train-acc: 0.92946429, dom-acc: 0.53125000, val-acc: 0.91000000, val_loss: 0.25104246
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 268.67977977, sen-loss: 22.72044683, dom-loss: 78.49746519, src-aux-loss: 81.42707074, tar-aux-loss: 86.03479677
Epoch: [51 ] train-acc: 0.93035714, dom-acc: 0.54107143, val-acc: 0.91250000, val_loss: 0.24921292
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 265.83236575, sen-loss: 22.42766223, dom-loss: 78.40622467, src-aux-loss: 81.04116645, tar-aux-loss: 83.95731229
Epoch: [52 ] train-acc: 0.93071429, dom-acc: 0.58767857, val-acc: 0.91250000, val_loss: 0.24717911
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 265.45960701, sen-loss: 22.34738883, dom-loss: 78.45939535, src-aux-loss: 80.27443936, tar-aux-loss: 84.37838340
Epoch: [53 ] train-acc: 0.93178571, dom-acc: 0.55410714, val-acc: 0.91250000, val_loss: 0.24485113
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_kitchen_HATN.ckpt
Best Epoch: [ 48] best val accuracy: 0.00000000 best val loss: 0.24315944
Testing accuracy: 0.86733333
./work/attentions/dvd_kitchen_train_HATN.txt
./work/attentions/dvd_kitchen_test_HATN.txt
loading data...
source domain:  dvd target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 30180
vocab-size:  91852
['best', 'good', 'great', 'excellent', 'funny', 'enjoyable', 'entertaining', 'love', 'awesome', 'fantastic', 'better', 'classic', 'nice', 'wonderful', 'amazing', 'hilarious', 'brilliant', 'funniest', 'perfect', 'loved', 'underrated', 'favorite', 'interesting', 'superb', 'sad', 'terrific', 'greatest', 'finest', 'real', 'solid', 'incredible', 'easy', 'memorable', 'cute', 'believable', 'fascinating', 'liked']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'dull', 'waste', 'wasted', 'stupid', 'pathetic', 'worse', 'unwatchable', 'forgettable', 'unfunny', 'predictable', 'annoying', 'laughable', 'lousy', 'ruined', 'uninspired', 'decent', 'slow', 'sucked', 'pointless', 'overrated', 'dismal', 'wrong', 'ok', 'silly', 'poorly', 'unoriginal', 'mediocre', 'lacking', 'sucks', 'crappy', 'lame', 'cheap', 'atrocious', 'disgusting', 'hated', 'ridiculous', 'contrived', 'defective', 'unconvincing', 'depressing', 'horrendous', 'biased', 'disjointed', 'disapointed', 'dumbest', 'frustrating', 'wasting', 'insipid', 'dreadful']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
91852
5600 400 6000 17843 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 403.92463803, sen-loss: 78.26223034, dom-loss: 79.40033561, src-aux-loss: 130.80662745, tar-aux-loss: 115.45544440
Epoch: [1  ] train-acc: 0.65375000, dom-acc: 0.46366071, val-acc: 0.67250000, val_loss: 0.66452223
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 384.35798192, sen-loss: 72.64165008, dom-loss: 78.79194611, src-aux-loss: 123.19187194, tar-aux-loss: 109.73251438
Epoch: [2  ] train-acc: 0.72946429, dom-acc: 0.48794643, val-acc: 0.74000000, val_loss: 0.60762227
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 368.77901411, sen-loss: 66.62129653, dom-loss: 78.62580454, src-aux-loss: 118.38119328, tar-aux-loss: 105.15072072
Epoch: [3  ] train-acc: 0.77821429, dom-acc: 0.52803571, val-acc: 0.80000000, val_loss: 0.54130453
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 355.26536155, sen-loss: 59.66447985, dom-loss: 78.41169512, src-aux-loss: 114.84796864, tar-aux-loss: 102.34121561
Epoch: [4  ] train-acc: 0.80732143, dom-acc: 0.58517857, val-acc: 0.80500000, val_loss: 0.46807608
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 341.22735214, sen-loss: 52.31541049, dom-loss: 78.05549884, src-aux-loss: 111.38333899, tar-aux-loss: 99.47310334
Epoch: [5  ] train-acc: 0.84053571, dom-acc: 0.62258929, val-acc: 0.85500000, val_loss: 0.39977941
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 330.54144979, sen-loss: 46.48837826, dom-loss: 77.80139625, src-aux-loss: 108.21761602, tar-aux-loss: 98.03405839
Epoch: [6  ] train-acc: 0.84982143, dom-acc: 0.62776786, val-acc: 0.87750000, val_loss: 0.34736767
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 321.86397624, sen-loss: 42.02666502, dom-loss: 77.72249830, src-aux-loss: 106.35214037, tar-aux-loss: 95.76267147
Epoch: [7  ] train-acc: 0.86928571, dom-acc: 0.62892857, val-acc: 0.90500000, val_loss: 0.32122508
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 317.25023866, sen-loss: 39.33445500, dom-loss: 77.44889039, src-aux-loss: 104.69133681, tar-aux-loss: 95.77555680
Epoch: [8  ] train-acc: 0.87642857, dom-acc: 0.63276786, val-acc: 0.89750000, val_loss: 0.30304968
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 313.24282241, sen-loss: 37.27005681, dom-loss: 77.45090824, src-aux-loss: 103.21729934, tar-aux-loss: 95.30455703
Epoch: [9  ] train-acc: 0.88125000, dom-acc: 0.64142857, val-acc: 0.90250000, val_loss: 0.29314139
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 310.15962887, sen-loss: 35.82341313, dom-loss: 77.58320659, src-aux-loss: 102.47770751, tar-aux-loss: 94.27530074
Epoch: [10 ] train-acc: 0.88357143, dom-acc: 0.64357143, val-acc: 0.90000000, val_loss: 0.28646770
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 306.57850337, sen-loss: 34.85781305, dom-loss: 76.97732937, src-aux-loss: 101.21233737, tar-aux-loss: 93.53102320
Epoch: [11 ] train-acc: 0.88428571, dom-acc: 0.64830357, val-acc: 0.89250000, val_loss: 0.28324294
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 305.29966950, sen-loss: 34.20277603, dom-loss: 76.93441051, src-aux-loss: 100.53921193, tar-aux-loss: 93.62327194
Epoch: [12 ] train-acc: 0.88589286, dom-acc: 0.67026786, val-acc: 0.90500000, val_loss: 0.29112655
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 302.62267661, sen-loss: 33.44202901, dom-loss: 76.83557445, src-aux-loss: 99.92418283, tar-aux-loss: 92.42089254
Epoch: [13 ] train-acc: 0.89178571, dom-acc: 0.66419643, val-acc: 0.90750000, val_loss: 0.27844185
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 302.65134883, sen-loss: 33.08052582, dom-loss: 76.85282534, src-aux-loss: 99.20988441, tar-aux-loss: 93.50811386
Epoch: [14 ] train-acc: 0.89250000, dom-acc: 0.66607143, val-acc: 0.90750000, val_loss: 0.27855089
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 299.87584615, sen-loss: 32.31684849, dom-loss: 76.92825055, src-aux-loss: 98.40976340, tar-aux-loss: 92.22098380
Epoch: [15 ] train-acc: 0.89410714, dom-acc: 0.67205357, val-acc: 0.90500000, val_loss: 0.27727851
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 298.73325014, sen-loss: 31.97063398, dom-loss: 76.92973745, src-aux-loss: 98.00025469, tar-aux-loss: 91.83262300
Epoch: [16 ] train-acc: 0.89678571, dom-acc: 0.66901786, val-acc: 0.90500000, val_loss: 0.27175921
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 296.03952789, sen-loss: 31.46254918, dom-loss: 76.64813632, src-aux-loss: 97.22848451, tar-aux-loss: 90.70035732
Epoch: [17 ] train-acc: 0.89625000, dom-acc: 0.67133929, val-acc: 0.90500000, val_loss: 0.27242532
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 296.53221655, sen-loss: 31.12763369, dom-loss: 76.58429962, src-aux-loss: 96.78569287, tar-aux-loss: 92.03459138
Epoch: [18 ] train-acc: 0.89946429, dom-acc: 0.66794643, val-acc: 0.90500000, val_loss: 0.26899472
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 295.31762338, sen-loss: 31.02401280, dom-loss: 76.71306068, src-aux-loss: 96.32621390, tar-aux-loss: 91.25433648
Epoch: [19 ] train-acc: 0.89767857, dom-acc: 0.66857143, val-acc: 0.90750000, val_loss: 0.27014089
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 294.48503256, sen-loss: 30.59937540, dom-loss: 76.81344521, src-aux-loss: 95.72882265, tar-aux-loss: 91.34338945
Epoch: [20 ] train-acc: 0.90232143, dom-acc: 0.66928571, val-acc: 0.90250000, val_loss: 0.26512709
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 292.14455891, sen-loss: 30.13496979, dom-loss: 76.74502796, src-aux-loss: 95.27912736, tar-aux-loss: 89.98543364
Epoch: [21 ] train-acc: 0.90089286, dom-acc: 0.66928571, val-acc: 0.90250000, val_loss: 0.26460752
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 291.98054743, sen-loss: 29.93020873, dom-loss: 76.71611303, src-aux-loss: 94.63533026, tar-aux-loss: 90.69889647
Epoch: [22 ] train-acc: 0.90375000, dom-acc: 0.67142857, val-acc: 0.90250000, val_loss: 0.26400581
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 289.28155684, sen-loss: 29.59415986, dom-loss: 76.40626657, src-aux-loss: 94.06909853, tar-aux-loss: 89.21203166
Epoch: [23 ] train-acc: 0.89892857, dom-acc: 0.67901786, val-acc: 0.90250000, val_loss: 0.26854813
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 290.79453850, sen-loss: 29.38505939, dom-loss: 76.69755816, src-aux-loss: 93.85101038, tar-aux-loss: 90.86091059
Epoch: [24 ] train-acc: 0.90482143, dom-acc: 0.67017857, val-acc: 0.90250000, val_loss: 0.26220438
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 288.65250707, sen-loss: 28.92368855, dom-loss: 76.62727469, src-aux-loss: 93.14026564, tar-aux-loss: 89.96127766
Epoch: [25 ] train-acc: 0.90464286, dom-acc: 0.66741071, val-acc: 0.90250000, val_loss: 0.25974548
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 287.79150629, sen-loss: 28.63366140, dom-loss: 76.76215160, src-aux-loss: 92.69845498, tar-aux-loss: 89.69723809
Epoch: [26 ] train-acc: 0.90535714, dom-acc: 0.67169643, val-acc: 0.90000000, val_loss: 0.26140985
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 284.73376060, sen-loss: 28.54654658, dom-loss: 76.48017234, src-aux-loss: 92.22475952, tar-aux-loss: 87.48228276
Epoch: [27 ] train-acc: 0.90321429, dom-acc: 0.67142857, val-acc: 0.90250000, val_loss: 0.26652288
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 285.39834523, sen-loss: 28.23595811, dom-loss: 76.46407264, src-aux-loss: 91.76090693, tar-aux-loss: 88.93740666
Epoch: [28 ] train-acc: 0.90107143, dom-acc: 0.68044643, val-acc: 0.90250000, val_loss: 0.27573308
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 285.39714956, sen-loss: 27.92139449, dom-loss: 76.67227948, src-aux-loss: 91.34118748, tar-aux-loss: 89.46228975
Epoch: [29 ] train-acc: 0.90821429, dom-acc: 0.66866071, val-acc: 0.90250000, val_loss: 0.25597161
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 284.77748919, sen-loss: 27.66983139, dom-loss: 76.32795411, src-aux-loss: 90.84488374, tar-aux-loss: 89.93481976
Epoch: [30 ] train-acc: 0.90125000, dom-acc: 0.68321429, val-acc: 0.90500000, val_loss: 0.27419350
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 282.15395141, sen-loss: 27.41609658, dom-loss: 76.61511618, src-aux-loss: 90.59278488, tar-aux-loss: 87.52995235
Epoch: [31 ] train-acc: 0.91071429, dom-acc: 0.67214286, val-acc: 0.90750000, val_loss: 0.25550783
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 282.23883295, sen-loss: 27.25431248, dom-loss: 76.51056248, src-aux-loss: 90.27046126, tar-aux-loss: 88.20349699
Epoch: [32 ] train-acc: 0.91000000, dom-acc: 0.66750000, val-acc: 0.90750000, val_loss: 0.25444686
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 281.88592625, sen-loss: 26.96148035, dom-loss: 76.20858765, src-aux-loss: 89.92191255, tar-aux-loss: 88.79394549
Epoch: [33 ] train-acc: 0.91160714, dom-acc: 0.67526786, val-acc: 0.90500000, val_loss: 0.25518882
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 281.14574242, sen-loss: 26.80323543, dom-loss: 76.37761664, src-aux-loss: 89.22747415, tar-aux-loss: 88.73741579
Epoch: [34 ] train-acc: 0.91196429, dom-acc: 0.66839286, val-acc: 0.90250000, val_loss: 0.25465709
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 279.07610822, sen-loss: 26.52891981, dom-loss: 76.57717997, src-aux-loss: 88.80241024, tar-aux-loss: 87.16759855
Epoch: [35 ] train-acc: 0.91321429, dom-acc: 0.68000000, val-acc: 0.90750000, val_loss: 0.25794804
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 278.69873381, sen-loss: 26.32335842, dom-loss: 76.52470517, src-aux-loss: 88.14633781, tar-aux-loss: 87.70433319
Epoch: [36 ] train-acc: 0.91464286, dom-acc: 0.67544643, val-acc: 0.90250000, val_loss: 0.25392166
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 276.07919312, sen-loss: 26.03415851, dom-loss: 76.30442280, src-aux-loss: 87.48980063, tar-aux-loss: 86.25081074
Epoch: [37 ] train-acc: 0.91482143, dom-acc: 0.66294643, val-acc: 0.90250000, val_loss: 0.25076866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 277.99161839, sen-loss: 25.82272976, dom-loss: 76.55176377, src-aux-loss: 87.42393839, tar-aux-loss: 88.19318718
Epoch: [38 ] train-acc: 0.91607143, dom-acc: 0.66821429, val-acc: 0.91250000, val_loss: 0.25432029
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 275.55758619, sen-loss: 25.45982622, dom-loss: 76.61023444, src-aux-loss: 86.65393031, tar-aux-loss: 86.83359510
Epoch: [39 ] train-acc: 0.91678571, dom-acc: 0.66794643, val-acc: 0.90500000, val_loss: 0.25008097
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 273.77965593, sen-loss: 25.26263259, dom-loss: 76.44521469, src-aux-loss: 86.18194389, tar-aux-loss: 85.88986450
Epoch: [40 ] train-acc: 0.91803571, dom-acc: 0.66803571, val-acc: 0.90250000, val_loss: 0.24992907
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 273.35787940, sen-loss: 25.02378361, dom-loss: 76.06906503, src-aux-loss: 85.55394310, tar-aux-loss: 86.71108896
Epoch: [41 ] train-acc: 0.91928571, dom-acc: 0.66955357, val-acc: 0.91250000, val_loss: 0.25274009
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 272.87717938, sen-loss: 24.79646306, dom-loss: 76.63512224, src-aux-loss: 84.94616336, tar-aux-loss: 86.49943036
Epoch: [42 ] train-acc: 0.91982143, dom-acc: 0.66830357, val-acc: 0.90500000, val_loss: 0.25009176
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 271.98254323, sen-loss: 24.66199697, dom-loss: 76.32977962, src-aux-loss: 84.66411543, tar-aux-loss: 86.32665128
Epoch: [43 ] train-acc: 0.92178571, dom-acc: 0.66687500, val-acc: 0.90500000, val_loss: 0.25288540
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 273.15025616, sen-loss: 24.51371059, dom-loss: 76.51255429, src-aux-loss: 84.55308729, tar-aux-loss: 87.57090396
Epoch: [44 ] train-acc: 0.92267857, dom-acc: 0.66776786, val-acc: 0.90250000, val_loss: 0.24716912
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 269.27399635, sen-loss: 24.04586662, dom-loss: 76.25535429, src-aux-loss: 83.52282637, tar-aux-loss: 85.44994986
Epoch: [45 ] train-acc: 0.92142857, dom-acc: 0.67598214, val-acc: 0.91000000, val_loss: 0.25583437
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 269.50609803, sen-loss: 23.98521940, dom-loss: 76.34731936, src-aux-loss: 83.15996867, tar-aux-loss: 86.01358968
Epoch: [46 ] train-acc: 0.92410714, dom-acc: 0.66428571, val-acc: 0.90500000, val_loss: 0.24790719
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 268.56918478, sen-loss: 23.56818629, dom-loss: 76.41229475, src-aux-loss: 82.52371746, tar-aux-loss: 86.06498551
Epoch: [47 ] train-acc: 0.92232143, dom-acc: 0.65955357, val-acc: 0.90000000, val_loss: 0.24818882
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 266.66067278, sen-loss: 23.36220347, dom-loss: 76.24633300, src-aux-loss: 82.02820560, tar-aux-loss: 85.02392852
Epoch: [48 ] train-acc: 0.92482143, dom-acc: 0.66214286, val-acc: 0.90500000, val_loss: 0.24724412
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 266.69698286, sen-loss: 23.07177451, dom-loss: 76.38376522, src-aux-loss: 81.48824078, tar-aux-loss: 85.75320375
Epoch: [49 ] train-acc: 0.92946429, dom-acc: 0.67017857, val-acc: 0.90250000, val_loss: 0.24981664
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_video_HATN.ckpt
Best Epoch: [ 44] best val accuracy: 0.00000000 best val loss: 0.24716912
Testing accuracy: 0.89283333
./work/attentions/dvd_video_train_HATN.txt
./work/attentions/dvd_video_test_HATN.txt
loading data...
source domain:  electronics target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 9750
vocab-size:  83050
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'happy', 'fantastic', 'awesome', 'solid', 'amazing', 'highly', 'satisfied', 'reliable', 'love', 'quick', 'worth', 'durable', 'recommend', 'outstanding', 'perfectly', 'impressive', 'fast', 'pleased', 'simple', 'exactly', 'old', 'recommended', 'nice', 'decent', 'impressed', 'useful', 'arrived', 'cheaper', 'inexpensive', 'awsome', 'far', 'slick', 'effective', 'expected', 'loves', 'advertised']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'useless', 'frustrating', 'bad', 'worst', 'awful', 'failed', 'unreliable', 'cheap', 'wrong', 'horrible', 'died', 'hard', 'broke', 'impossible', 'expensive', 'poorly', 'defective', 'overpriced', 'returning', 'unacceptable', 'misleading', 'quit', 'broken', 'short', 'terrible', 'worthless', 'flimsy', 'ridiculous', 'beware', 'missing', 'slow', 'save', 'lousy', 'lasted', 'much', 'uncomfortable', 'difficult', 'unhappy', 'trying', 'low', 'cannot', 'flawed', 'incompatible', 'waste', 'acceptable', 'average', 'sad', 'lose', 'nowhere', 'waiting', 'weak', 'tried', 'mediocre', 'inferior']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
83050
5600 400 6000 23009 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 414.91446424, sen-loss: 77.46293360, dom-loss: 77.51454759, src-aux-loss: 133.20157480, tar-aux-loss: 126.73540914
Epoch: [1  ] train-acc: 0.71071429, dom-acc: 0.83562500, val-acc: 0.71000000, val_loss: 0.65236551
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 384.94170141, sen-loss: 68.91807097, dom-loss: 74.10301352, src-aux-loss: 123.89481312, tar-aux-loss: 118.02580345
Epoch: [2  ] train-acc: 0.75214286, dom-acc: 0.85178571, val-acc: 0.76500000, val_loss: 0.57043421
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 364.91943145, sen-loss: 58.41251451, dom-loss: 72.90701121, src-aux-loss: 119.10657191, tar-aux-loss: 114.49333304
Epoch: [3  ] train-acc: 0.80303571, dom-acc: 0.70741071, val-acc: 0.84000000, val_loss: 0.45176813
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 348.28736711, sen-loss: 48.53097177, dom-loss: 72.66872627, src-aux-loss: 115.53059679, tar-aux-loss: 111.55707234
Epoch: [4  ] train-acc: 0.82678571, dom-acc: 0.59544643, val-acc: 0.86000000, val_loss: 0.38168418
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 340.96017814, sen-loss: 44.18769591, dom-loss: 73.27385432, src-aux-loss: 113.34903377, tar-aux-loss: 110.14959490
Epoch: [5  ] train-acc: 0.84571429, dom-acc: 0.57750000, val-acc: 0.87000000, val_loss: 0.34466961
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 335.35157228, sen-loss: 40.74960878, dom-loss: 74.32887203, src-aux-loss: 111.38853002, tar-aux-loss: 108.88456154
Epoch: [6  ] train-acc: 0.85964286, dom-acc: 0.56821429, val-acc: 0.88250000, val_loss: 0.31639695
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 330.77779889, sen-loss: 38.08081642, dom-loss: 75.45481324, src-aux-loss: 109.58559108, tar-aux-loss: 107.65657967
Epoch: [7  ] train-acc: 0.86553571, dom-acc: 0.53633929, val-acc: 0.89000000, val_loss: 0.29737884
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 326.67446470, sen-loss: 36.22504523, dom-loss: 76.23204458, src-aux-loss: 108.29694802, tar-aux-loss: 105.92042798
Epoch: [8  ] train-acc: 0.87767857, dom-acc: 0.50392857, val-acc: 0.90000000, val_loss: 0.28158504
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 325.03582835, sen-loss: 34.85550873, dom-loss: 77.06382418, src-aux-loss: 107.06962609, tar-aux-loss: 106.04687184
Epoch: [9  ] train-acc: 0.88357143, dom-acc: 0.47848214, val-acc: 0.90250000, val_loss: 0.26649335
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 323.30563211, sen-loss: 33.55825664, dom-loss: 77.97362483, src-aux-loss: 106.00801659, tar-aux-loss: 105.76573426
Epoch: [10 ] train-acc: 0.88785714, dom-acc: 0.45830357, val-acc: 0.91250000, val_loss: 0.25683799
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 321.18741035, sen-loss: 32.50252032, dom-loss: 78.32132846, src-aux-loss: 105.57146710, tar-aux-loss: 104.79209512
Epoch: [11 ] train-acc: 0.88750000, dom-acc: 0.43776786, val-acc: 0.92000000, val_loss: 0.24740976
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 318.91193414, sen-loss: 31.78903799, dom-loss: 78.65305191, src-aux-loss: 104.35876882, tar-aux-loss: 104.11107475
Epoch: [12 ] train-acc: 0.89696429, dom-acc: 0.41973214, val-acc: 0.91750000, val_loss: 0.23838530
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 317.69563365, sen-loss: 30.93742894, dom-loss: 78.83996290, src-aux-loss: 103.69713438, tar-aux-loss: 104.22110963
Epoch: [13 ] train-acc: 0.89500000, dom-acc: 0.43169643, val-acc: 0.91500000, val_loss: 0.23537725
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 316.34139681, sen-loss: 30.41170466, dom-loss: 79.07614887, src-aux-loss: 102.99344319, tar-aux-loss: 103.86010039
Epoch: [14 ] train-acc: 0.90178571, dom-acc: 0.42866071, val-acc: 0.91750000, val_loss: 0.23195019
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 313.87299514, sen-loss: 29.71751064, dom-loss: 79.02869135, src-aux-loss: 102.38238448, tar-aux-loss: 102.74440825
Epoch: [15 ] train-acc: 0.90339286, dom-acc: 0.43937500, val-acc: 0.91750000, val_loss: 0.22506738
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 312.51995540, sen-loss: 29.08096586, dom-loss: 78.86398917, src-aux-loss: 102.16287762, tar-aux-loss: 102.41212159
Epoch: [16 ] train-acc: 0.90696429, dom-acc: 0.44366071, val-acc: 0.91750000, val_loss: 0.22199255
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 310.55027223, sen-loss: 28.56083179, dom-loss: 78.77486211, src-aux-loss: 101.43273759, tar-aux-loss: 101.78184187
Epoch: [17 ] train-acc: 0.90767857, dom-acc: 0.45312500, val-acc: 0.92250000, val_loss: 0.22091457
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 310.53998423, sen-loss: 28.33433204, dom-loss: 78.73268783, src-aux-loss: 100.83265829, tar-aux-loss: 102.64030689
Epoch: [18 ] train-acc: 0.90767857, dom-acc: 0.47919643, val-acc: 0.92750000, val_loss: 0.22275405
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 308.42253256, sen-loss: 27.68697432, dom-loss: 78.60022295, src-aux-loss: 100.37110120, tar-aux-loss: 101.76423585
Epoch: [19 ] train-acc: 0.90982143, dom-acc: 0.49660714, val-acc: 0.92250000, val_loss: 0.21122096
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 307.07026076, sen-loss: 27.34376413, dom-loss: 78.39649957, src-aux-loss: 99.75516468, tar-aux-loss: 101.57483250
Epoch: [20 ] train-acc: 0.91232143, dom-acc: 0.51928571, val-acc: 0.92750000, val_loss: 0.21589264
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 307.06058741, sen-loss: 27.04906225, dom-loss: 78.02558255, src-aux-loss: 99.69565302, tar-aux-loss: 102.29029012
Epoch: [21 ] train-acc: 0.91392857, dom-acc: 0.50169643, val-acc: 0.92250000, val_loss: 0.20690100
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 304.84719419, sen-loss: 26.63414381, dom-loss: 77.96397781, src-aux-loss: 98.99543077, tar-aux-loss: 101.25364131
Epoch: [22 ] train-acc: 0.91303571, dom-acc: 0.51392857, val-acc: 0.92250000, val_loss: 0.20530246
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 303.34833407, sen-loss: 26.16830012, dom-loss: 77.68751341, src-aux-loss: 98.39043134, tar-aux-loss: 101.10208946
Epoch: [23 ] train-acc: 0.91410714, dom-acc: 0.53125000, val-acc: 0.92750000, val_loss: 0.20493789
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 302.95930958, sen-loss: 25.78090342, dom-loss: 77.51136708, src-aux-loss: 98.46377516, tar-aux-loss: 101.20326346
Epoch: [24 ] train-acc: 0.92035714, dom-acc: 0.52821429, val-acc: 0.92750000, val_loss: 0.20563261
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 303.20326281, sen-loss: 25.54735221, dom-loss: 77.62821651, src-aux-loss: 97.72359371, tar-aux-loss: 102.30410129
Epoch: [25 ] train-acc: 0.91910714, dom-acc: 0.54794643, val-acc: 0.93000000, val_loss: 0.20251937
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 299.87754655, sen-loss: 25.30568791, dom-loss: 77.57096767, src-aux-loss: 97.34092504, tar-aux-loss: 99.65996420
Epoch: [26 ] train-acc: 0.92178571, dom-acc: 0.52723214, val-acc: 0.92750000, val_loss: 0.20304008
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 300.68050981, sen-loss: 24.92601562, dom-loss: 77.37631238, src-aux-loss: 96.84695482, tar-aux-loss: 101.53122538
Epoch: [27 ] train-acc: 0.92107143, dom-acc: 0.52303571, val-acc: 0.93000000, val_loss: 0.19858281
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 299.40180898, sen-loss: 24.71089883, dom-loss: 77.51055396, src-aux-loss: 96.55391949, tar-aux-loss: 100.62643665
Epoch: [28 ] train-acc: 0.92321429, dom-acc: 0.52035714, val-acc: 0.93000000, val_loss: 0.19916081
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 298.78273749, sen-loss: 24.52436759, dom-loss: 77.52599478, src-aux-loss: 96.15631068, tar-aux-loss: 100.57606351
Epoch: [29 ] train-acc: 0.92089286, dom-acc: 0.53883929, val-acc: 0.93000000, val_loss: 0.19793698
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 297.50271535, sen-loss: 24.08938237, dom-loss: 77.46388400, src-aux-loss: 95.74640936, tar-aux-loss: 100.20304096
Epoch: [30 ] train-acc: 0.92660714, dom-acc: 0.52357143, val-acc: 0.93250000, val_loss: 0.20057303
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 297.07931638, sen-loss: 23.87202184, dom-loss: 77.65145904, src-aux-loss: 95.24417180, tar-aux-loss: 100.31166315
Epoch: [31 ] train-acc: 0.92607143, dom-acc: 0.49348214, val-acc: 0.93750000, val_loss: 0.20069593
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 297.58050942, sen-loss: 23.53264798, dom-loss: 77.90712065, src-aux-loss: 95.05236733, tar-aux-loss: 101.08837456
Epoch: [32 ] train-acc: 0.92500000, dom-acc: 0.52714286, val-acc: 0.92250000, val_loss: 0.19736226
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 296.35716891, sen-loss: 23.30587397, dom-loss: 77.94478065, src-aux-loss: 94.67843997, tar-aux-loss: 100.42807388
Epoch: [33 ] train-acc: 0.92928571, dom-acc: 0.51473214, val-acc: 0.93750000, val_loss: 0.19663598
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 295.99155974, sen-loss: 23.10148548, dom-loss: 77.91715330, src-aux-loss: 94.40461880, tar-aux-loss: 100.56830186
Epoch: [34 ] train-acc: 0.93196429, dom-acc: 0.48776786, val-acc: 0.93000000, val_loss: 0.19383691
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 294.17620444, sen-loss: 22.76182149, dom-loss: 78.17025721, src-aux-loss: 93.85869515, tar-aux-loss: 99.38543171
Epoch: [35 ] train-acc: 0.93000000, dom-acc: 0.47392857, val-acc: 0.94000000, val_loss: 0.19682607
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 294.04342937, sen-loss: 22.44478897, dom-loss: 78.21916986, src-aux-loss: 93.60400993, tar-aux-loss: 99.77546155
Epoch: [36 ] train-acc: 0.93142857, dom-acc: 0.46607143, val-acc: 0.93500000, val_loss: 0.19090429
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 294.26634789, sen-loss: 22.36742301, dom-loss: 78.12296522, src-aux-loss: 93.30388570, tar-aux-loss: 100.47207522
Epoch: [37 ] train-acc: 0.93339286, dom-acc: 0.46598214, val-acc: 0.93500000, val_loss: 0.19068868
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 292.68734241, sen-loss: 21.94973766, dom-loss: 78.34648216, src-aux-loss: 92.59240454, tar-aux-loss: 99.79871893
Epoch: [38 ] train-acc: 0.93464286, dom-acc: 0.48080357, val-acc: 0.94000000, val_loss: 0.19231644
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 291.02350402, sen-loss: 21.86471373, dom-loss: 78.25124532, src-aux-loss: 92.14573598, tar-aux-loss: 98.76180786
Epoch: [39 ] train-acc: 0.93678571, dom-acc: 0.45035714, val-acc: 0.94000000, val_loss: 0.18948378
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 291.29724240, sen-loss: 21.45640468, dom-loss: 78.20625091, src-aux-loss: 91.87326282, tar-aux-loss: 99.76132333
Epoch: [40 ] train-acc: 0.93696429, dom-acc: 0.46517857, val-acc: 0.94000000, val_loss: 0.18929696
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 290.88136101, sen-loss: 21.31667779, dom-loss: 78.18742257, src-aux-loss: 91.51512033, tar-aux-loss: 99.86214077
Epoch: [41 ] train-acc: 0.93857143, dom-acc: 0.45901786, val-acc: 0.94250000, val_loss: 0.18868232
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 289.57462239, sen-loss: 21.02470783, dom-loss: 78.10058624, src-aux-loss: 91.11187178, tar-aux-loss: 99.33745694
Epoch: [42 ] train-acc: 0.93982143, dom-acc: 0.46955357, val-acc: 0.94000000, val_loss: 0.18823953
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 288.26395798, sen-loss: 20.83686531, dom-loss: 77.95560765, src-aux-loss: 90.46482563, tar-aux-loss: 99.00665969
Epoch: [43 ] train-acc: 0.94125000, dom-acc: 0.45410714, val-acc: 0.93500000, val_loss: 0.18848833
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 287.85832262, sen-loss: 20.63286360, dom-loss: 77.92053086, src-aux-loss: 90.16751599, tar-aux-loss: 99.13741118
Epoch: [44 ] train-acc: 0.94267857, dom-acc: 0.49991071, val-acc: 0.93250000, val_loss: 0.18882014
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 286.65959072, sen-loss: 20.34141020, dom-loss: 77.75061721, src-aux-loss: 90.01290083, tar-aux-loss: 98.55466390
Epoch: [45 ] train-acc: 0.94375000, dom-acc: 0.50375000, val-acc: 0.93750000, val_loss: 0.18623433
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 286.55164051, sen-loss: 20.13583159, dom-loss: 77.60767126, src-aux-loss: 89.51503414, tar-aux-loss: 99.29310352
Epoch: [46 ] train-acc: 0.94446429, dom-acc: 0.47973214, val-acc: 0.93000000, val_loss: 0.18764427
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 285.16730261, sen-loss: 19.84517190, dom-loss: 77.50437498, src-aux-loss: 88.94216752, tar-aux-loss: 98.87558919
Epoch: [47 ] train-acc: 0.94232143, dom-acc: 0.49294643, val-acc: 0.93500000, val_loss: 0.19104338
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 285.27639914, sen-loss: 19.64896912, dom-loss: 77.42969203, src-aux-loss: 88.59077966, tar-aux-loss: 99.60695940
Epoch: [48 ] train-acc: 0.94678571, dom-acc: 0.53767857, val-acc: 0.93750000, val_loss: 0.18808870
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 283.45314693, sen-loss: 19.36303569, dom-loss: 77.35144985, src-aux-loss: 88.07732439, tar-aux-loss: 98.66133732
Epoch: [49 ] train-acc: 0.94696429, dom-acc: 0.49446429, val-acc: 0.93250000, val_loss: 0.18588096
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 282.20523810, sen-loss: 19.18860840, dom-loss: 77.30778658, src-aux-loss: 87.73127490, tar-aux-loss: 97.97756749
Epoch: [50 ] train-acc: 0.94839286, dom-acc: 0.52125000, val-acc: 0.93500000, val_loss: 0.18700428
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 281.50496674, sen-loss: 18.95846543, dom-loss: 77.30421370, src-aux-loss: 87.10895252, tar-aux-loss: 98.13333356
Epoch: [51 ] train-acc: 0.94839286, dom-acc: 0.55276786, val-acc: 0.93500000, val_loss: 0.18813872
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 281.43902946, sen-loss: 18.65908042, dom-loss: 77.27676570, src-aux-loss: 86.84981549, tar-aux-loss: 98.65336728
Epoch: [52 ] train-acc: 0.94428571, dom-acc: 0.47142857, val-acc: 0.93750000, val_loss: 0.19240318
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 281.97629094, sen-loss: 18.50507873, dom-loss: 77.26183486, src-aux-loss: 86.64418501, tar-aux-loss: 99.56519198
Epoch: [53 ] train-acc: 0.95017857, dom-acc: 0.50339286, val-acc: 0.93500000, val_loss: 0.18811502
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [54 ] loss: 280.67641330, sen-loss: 18.18850836, dom-loss: 77.29021013, src-aux-loss: 86.13938767, tar-aux-loss: 99.05830836
Epoch: [54 ] train-acc: 0.94839286, dom-acc: 0.48776786, val-acc: 0.93750000, val_loss: 0.19044833
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_books_HATN.ckpt
Best Epoch: [ 49] best val accuracy: 0.00000000 best val loss: 0.18588096
Testing accuracy: 0.83833333
./work/attentions/electronics_books_train_HATN.txt
./work/attentions/electronics_books_test_HATN.txt
loading data...
source domain:  electronics target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 11843
vocab-size:  85442
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'fantastic', 'happy', 'awesome', 'solid', 'amazing', 'highly', 'satisfied', 'durable', 'reliable', 'worth', 'quick', 'outstanding', 'perfectly', 'love', 'impressive', 'recommend', 'fast', 'pleased', 'simple', 'exactly', 'nice', 'recommended', 'impressed', 'cheaper', 'inexpensive', 'decent', 'old', 'awsome', 'far', 'useful', 'slick', 'effective', 'expected']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'useless', 'frustrating', 'bad', 'worst', 'awful', 'horrible', 'unreliable', 'cheap', 'failed', 'hard', 'died', 'impossible', 'broke', 'expensive', 'wrong', 'poorly', 'overpriced', 'unacceptable', 'defective', 'returning', 'misleading', 'quit', 'terrible', 'short', 'worthless', 'broken', 'flimsy', 'beware', 'save', 'ridiculous', 'lousy', 'lasted', 'much', 'missing', 'unhappy', 'trying', 'slow', 'low', 'uncomfortable', 'waste', 'difficult', 'flawed', 'sent', 'incompatible', 'tried', 'cannot', 'acceptable', 'average', 'sad', 'lose', 'waiting', 'mediocre', 'inferior']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
85442
5600 400 6000 23009 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 410.89641356, sen-loss: 77.48593593, dom-loss: 76.64822376, src-aux-loss: 133.93379372, tar-aux-loss: 122.82846260
Epoch: [1  ] train-acc: 0.71071429, dom-acc: 0.84741071, val-acc: 0.70750000, val_loss: 0.65234840
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 380.95163536, sen-loss: 68.96933490, dom-loss: 73.34856969, src-aux-loss: 124.49113649, tar-aux-loss: 114.14259654
Epoch: [2  ] train-acc: 0.75607143, dom-acc: 0.82116071, val-acc: 0.77750000, val_loss: 0.57003045
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.59401155, sen-loss: 58.40832987, dom-loss: 72.09480441, src-aux-loss: 119.71009409, tar-aux-loss: 110.38078302
Epoch: [3  ] train-acc: 0.80250000, dom-acc: 0.70312500, val-acc: 0.83750000, val_loss: 0.45127305
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 344.36130095, sen-loss: 48.58867344, dom-loss: 72.00912738, src-aux-loss: 115.85433549, tar-aux-loss: 107.90916431
Epoch: [4  ] train-acc: 0.82732143, dom-acc: 0.60339286, val-acc: 0.86500000, val_loss: 0.38123250
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 336.27194118, sen-loss: 44.26914874, dom-loss: 72.81191421, src-aux-loss: 113.71327102, tar-aux-loss: 105.47760618
Epoch: [5  ] train-acc: 0.84732143, dom-acc: 0.58705357, val-acc: 0.87500000, val_loss: 0.34630921
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 330.87550282, sen-loss: 40.88844556, dom-loss: 73.60740376, src-aux-loss: 111.61133695, tar-aux-loss: 104.76831758
Epoch: [6  ] train-acc: 0.85625000, dom-acc: 0.56714286, val-acc: 0.88000000, val_loss: 0.31728494
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 326.60053611, sen-loss: 38.21798913, dom-loss: 74.85910594, src-aux-loss: 109.64537638, tar-aux-loss: 103.87806457
Epoch: [7  ] train-acc: 0.86500000, dom-acc: 0.54062500, val-acc: 0.89250000, val_loss: 0.29695717
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 322.68480158, sen-loss: 36.40667005, dom-loss: 75.72986072, src-aux-loss: 108.36584014, tar-aux-loss: 102.18243301
Epoch: [8  ] train-acc: 0.87464286, dom-acc: 0.50437500, val-acc: 0.89750000, val_loss: 0.28438336
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 320.96709132, sen-loss: 35.08520645, dom-loss: 77.04748642, src-aux-loss: 107.06038851, tar-aux-loss: 101.77400923
Epoch: [9  ] train-acc: 0.88160714, dom-acc: 0.48107143, val-acc: 0.90250000, val_loss: 0.26747701
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 319.47372127, sen-loss: 33.71293777, dom-loss: 77.86447495, src-aux-loss: 106.05032218, tar-aux-loss: 101.84598726
Epoch: [10 ] train-acc: 0.88821429, dom-acc: 0.44901786, val-acc: 0.91000000, val_loss: 0.25741628
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 316.90947795, sen-loss: 32.54839845, dom-loss: 78.50791556, src-aux-loss: 105.49065959, tar-aux-loss: 100.36250466
Epoch: [11 ] train-acc: 0.89267857, dom-acc: 0.43232143, val-acc: 0.91500000, val_loss: 0.24795026
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 315.03514171, sen-loss: 31.86240520, dom-loss: 78.99150538, src-aux-loss: 104.37146306, tar-aux-loss: 99.80976921
Epoch: [12 ] train-acc: 0.89517857, dom-acc: 0.41205357, val-acc: 0.91250000, val_loss: 0.23957470
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.55315781, sen-loss: 31.02518001, dom-loss: 78.97359776, src-aux-loss: 103.71512592, tar-aux-loss: 100.83925349
Epoch: [13 ] train-acc: 0.89428571, dom-acc: 0.42767857, val-acc: 0.91750000, val_loss: 0.23535497
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 312.29077387, sen-loss: 30.40979335, dom-loss: 79.03238094, src-aux-loss: 102.94603550, tar-aux-loss: 99.90256447
Epoch: [14 ] train-acc: 0.90142857, dom-acc: 0.40500000, val-acc: 0.91250000, val_loss: 0.23185289
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 309.78585458, sen-loss: 29.81166156, dom-loss: 78.84572864, src-aux-loss: 102.29408270, tar-aux-loss: 98.83438033
Epoch: [15 ] train-acc: 0.90178571, dom-acc: 0.42982143, val-acc: 0.91500000, val_loss: 0.22520676
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 308.59128404, sen-loss: 29.15697512, dom-loss: 78.56513709, src-aux-loss: 101.96207756, tar-aux-loss: 98.90709233
Epoch: [16 ] train-acc: 0.90642857, dom-acc: 0.43669643, val-acc: 0.91750000, val_loss: 0.22196282
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 305.77592945, sen-loss: 28.60138128, dom-loss: 78.07165498, src-aux-loss: 101.23322070, tar-aux-loss: 97.86967313
Epoch: [17 ] train-acc: 0.90732143, dom-acc: 0.44133929, val-acc: 0.92000000, val_loss: 0.22089742
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 305.76182103, sen-loss: 28.34367933, dom-loss: 78.07156914, src-aux-loss: 100.67167878, tar-aux-loss: 98.67489451
Epoch: [18 ] train-acc: 0.90964286, dom-acc: 0.47214286, val-acc: 0.92000000, val_loss: 0.21816035
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 302.84208632, sen-loss: 27.79011609, dom-loss: 77.59745491, src-aux-loss: 100.18219095, tar-aux-loss: 97.27232480
Epoch: [19 ] train-acc: 0.91125000, dom-acc: 0.48035714, val-acc: 0.91750000, val_loss: 0.21185909
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 301.71042180, sen-loss: 27.37248667, dom-loss: 77.22128505, src-aux-loss: 99.52428412, tar-aux-loss: 97.59236622
Epoch: [20 ] train-acc: 0.91250000, dom-acc: 0.48928571, val-acc: 0.92500000, val_loss: 0.21411708
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 302.82520914, sen-loss: 27.01926622, dom-loss: 77.23446238, src-aux-loss: 99.24875039, tar-aux-loss: 99.32273126
Epoch: [21 ] train-acc: 0.91464286, dom-acc: 0.50750000, val-acc: 0.92000000, val_loss: 0.20741908
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 299.12170172, sen-loss: 26.63384239, dom-loss: 77.03383267, src-aux-loss: 98.69826931, tar-aux-loss: 96.75575888
Epoch: [22 ] train-acc: 0.91232143, dom-acc: 0.51125000, val-acc: 0.91500000, val_loss: 0.20674077
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 298.96378851, sen-loss: 26.19401636, dom-loss: 76.75677049, src-aux-loss: 98.28314316, tar-aux-loss: 97.72985739
Epoch: [23 ] train-acc: 0.91464286, dom-acc: 0.52910714, val-acc: 0.92000000, val_loss: 0.20509671
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 297.54175115, sen-loss: 25.84531906, dom-loss: 76.71579772, src-aux-loss: 98.26077795, tar-aux-loss: 96.71985602
Epoch: [24 ] train-acc: 0.91839286, dom-acc: 0.52241071, val-acc: 0.92500000, val_loss: 0.20499043
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 298.35848927, sen-loss: 25.53409313, dom-loss: 76.85906148, src-aux-loss: 97.39994442, tar-aux-loss: 98.56538939
Epoch: [25 ] train-acc: 0.91750000, dom-acc: 0.51562500, val-acc: 0.92750000, val_loss: 0.20253642
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.47755051, sen-loss: 25.34356583, dom-loss: 76.93314117, src-aux-loss: 96.89413071, tar-aux-loss: 96.30671233
Epoch: [26 ] train-acc: 0.92321429, dom-acc: 0.51062500, val-acc: 0.92500000, val_loss: 0.20287561
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 296.22846174, sen-loss: 24.99457340, dom-loss: 76.99996424, src-aux-loss: 96.45046383, tar-aux-loss: 97.78346026
Epoch: [27 ] train-acc: 0.92160714, dom-acc: 0.50964286, val-acc: 0.92500000, val_loss: 0.19905390
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 295.29804301, sen-loss: 24.79566361, dom-loss: 77.21493578, src-aux-loss: 96.22621876, tar-aux-loss: 97.06122428
Epoch: [28 ] train-acc: 0.92339286, dom-acc: 0.49598214, val-acc: 0.92500000, val_loss: 0.19904296
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 294.96568632, sen-loss: 24.59738264, dom-loss: 77.47625756, src-aux-loss: 95.83833277, tar-aux-loss: 97.05371422
Epoch: [29 ] train-acc: 0.92178571, dom-acc: 0.49651786, val-acc: 0.93000000, val_loss: 0.19723883
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 292.77310228, sen-loss: 24.15258488, dom-loss: 77.61462045, src-aux-loss: 95.12175113, tar-aux-loss: 95.88414532
Epoch: [30 ] train-acc: 0.92517857, dom-acc: 0.47500000, val-acc: 0.93000000, val_loss: 0.20242408
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 292.56357193, sen-loss: 23.96288860, dom-loss: 77.89571077, src-aux-loss: 94.76069134, tar-aux-loss: 95.94428211
Epoch: [31 ] train-acc: 0.92696429, dom-acc: 0.45803571, val-acc: 0.93000000, val_loss: 0.19951391
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 294.40920973, sen-loss: 23.56226815, dom-loss: 78.22652578, src-aux-loss: 94.52117932, tar-aux-loss: 98.09923601
Epoch: [32 ] train-acc: 0.92446429, dom-acc: 0.46705357, val-acc: 0.93000000, val_loss: 0.19591494
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 292.36976218, sen-loss: 23.40068050, dom-loss: 78.47479928, src-aux-loss: 94.08727938, tar-aux-loss: 96.40700352
Epoch: [33 ] train-acc: 0.92875000, dom-acc: 0.45169643, val-acc: 0.92750000, val_loss: 0.19570453
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 291.39470458, sen-loss: 23.13429198, dom-loss: 78.46028191, src-aux-loss: 93.87010819, tar-aux-loss: 95.93002284
Epoch: [34 ] train-acc: 0.92910714, dom-acc: 0.44250000, val-acc: 0.92750000, val_loss: 0.19278735
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 290.44682574, sen-loss: 22.80286662, dom-loss: 78.67674375, src-aux-loss: 93.18615621, tar-aux-loss: 95.78105837
Epoch: [35 ] train-acc: 0.92714286, dom-acc: 0.42758929, val-acc: 0.93500000, val_loss: 0.19809404
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 290.59827971, sen-loss: 22.56532913, dom-loss: 78.74883509, src-aux-loss: 93.00515193, tar-aux-loss: 96.27896345
Epoch: [36 ] train-acc: 0.93142857, dom-acc: 0.43232143, val-acc: 0.93500000, val_loss: 0.19118854
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 289.20619559, sen-loss: 22.42431119, dom-loss: 78.67343444, src-aux-loss: 92.53486991, tar-aux-loss: 95.57357967
Epoch: [37 ] train-acc: 0.93250000, dom-acc: 0.43571429, val-acc: 0.93500000, val_loss: 0.19007361
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 290.05837369, sen-loss: 22.05708836, dom-loss: 78.60743296, src-aux-loss: 92.01361692, tar-aux-loss: 97.38023645
Epoch: [38 ] train-acc: 0.93392857, dom-acc: 0.44035714, val-acc: 0.93500000, val_loss: 0.19347422
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 288.65530896, sen-loss: 21.98327302, dom-loss: 78.43735760, src-aux-loss: 91.55507940, tar-aux-loss: 96.67959952
Epoch: [39 ] train-acc: 0.93571429, dom-acc: 0.42901786, val-acc: 0.93250000, val_loss: 0.18962237
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 285.46137285, sen-loss: 21.54851812, dom-loss: 78.30145031, src-aux-loss: 91.21041936, tar-aux-loss: 94.40098423
Epoch: [40 ] train-acc: 0.93732143, dom-acc: 0.44589286, val-acc: 0.93750000, val_loss: 0.18892576
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 285.37487435, sen-loss: 21.48170431, dom-loss: 78.10272831, src-aux-loss: 90.78353596, tar-aux-loss: 95.00690615
Epoch: [41 ] train-acc: 0.93625000, dom-acc: 0.45383929, val-acc: 0.93750000, val_loss: 0.18858135
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 284.66447186, sen-loss: 21.08282772, dom-loss: 77.85590553, src-aux-loss: 90.43266386, tar-aux-loss: 95.29307270
Epoch: [42 ] train-acc: 0.93946429, dom-acc: 0.46214286, val-acc: 0.93500000, val_loss: 0.18764836
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 284.17858315, sen-loss: 20.95996772, dom-loss: 77.50570947, src-aux-loss: 89.93884814, tar-aux-loss: 95.77405858
Epoch: [43 ] train-acc: 0.94214286, dom-acc: 0.46044643, val-acc: 0.93750000, val_loss: 0.18827203
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 282.49845839, sen-loss: 20.74950705, dom-loss: 77.42109591, src-aux-loss: 89.47820234, tar-aux-loss: 94.84965330
Epoch: [44 ] train-acc: 0.94196429, dom-acc: 0.49035714, val-acc: 0.93750000, val_loss: 0.18724674
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 282.95754695, sen-loss: 20.51457079, dom-loss: 77.20128226, src-aux-loss: 89.37693793, tar-aux-loss: 95.86475593
Epoch: [45 ] train-acc: 0.94142857, dom-acc: 0.49732143, val-acc: 0.93250000, val_loss: 0.18576144
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 281.33118701, sen-loss: 20.24298319, dom-loss: 77.08490622, src-aux-loss: 88.92171472, tar-aux-loss: 95.08158302
Epoch: [46 ] train-acc: 0.94214286, dom-acc: 0.49526786, val-acc: 0.93000000, val_loss: 0.18853064
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 279.81931973, sen-loss: 19.94687642, dom-loss: 76.76113600, src-aux-loss: 88.37092453, tar-aux-loss: 94.74038333
Epoch: [47 ] train-acc: 0.93946429, dom-acc: 0.49892857, val-acc: 0.92750000, val_loss: 0.19376071
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 280.72145677, sen-loss: 19.75254379, dom-loss: 76.94664794, src-aux-loss: 87.92361885, tar-aux-loss: 96.09864694
Epoch: [48 ] train-acc: 0.94392857, dom-acc: 0.52901786, val-acc: 0.93000000, val_loss: 0.18663859
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 279.68301988, sen-loss: 19.50561579, dom-loss: 76.80006689, src-aux-loss: 87.70052510, tar-aux-loss: 95.67681319
Epoch: [49 ] train-acc: 0.94446429, dom-acc: 0.48535714, val-acc: 0.93250000, val_loss: 0.18435214
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 277.75464988, sen-loss: 19.34058971, dom-loss: 76.98505974, src-aux-loss: 87.19479549, tar-aux-loss: 94.23420382
Epoch: [50 ] train-acc: 0.94535714, dom-acc: 0.51758929, val-acc: 0.93000000, val_loss: 0.18600142
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 277.42556834, sen-loss: 19.06362991, dom-loss: 77.01799929, src-aux-loss: 86.68891001, tar-aux-loss: 94.65502918
Epoch: [51 ] train-acc: 0.94678571, dom-acc: 0.50214286, val-acc: 0.93000000, val_loss: 0.18591204
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 275.52927136, sen-loss: 18.76350781, dom-loss: 77.01830965, src-aux-loss: 86.23553544, tar-aux-loss: 93.51191902
Epoch: [52 ] train-acc: 0.94517857, dom-acc: 0.48321429, val-acc: 0.93500000, val_loss: 0.18897818
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 278.36461687, sen-loss: 18.62658590, dom-loss: 77.22010583, src-aux-loss: 85.89463115, tar-aux-loss: 96.62329280
Epoch: [53 ] train-acc: 0.94821429, dom-acc: 0.49705357, val-acc: 0.93500000, val_loss: 0.18658346
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [54 ] loss: 276.89009929, sen-loss: 18.29793023, dom-loss: 77.22346890, src-aux-loss: 85.57759351, tar-aux-loss: 95.79110748
Epoch: [54 ] train-acc: 0.94678571, dom-acc: 0.47151786, val-acc: 0.93250000, val_loss: 0.19014782
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_dvd_HATN.ckpt
Best Epoch: [ 49] best val accuracy: 0.00000000 best val loss: 0.18435214
Testing accuracy: 0.83533333
./work/attentions/electronics_dvd_train_HATN.txt
./work/attentions/electronics_dvd_test_HATN.txt
loading data...
source domain:  electronics target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 13856
vocab-size:  49470
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'happy', 'works', 'awesome', 'fantastic', 'solid', 'love', 'highly', 'amazing', 'satisfied', 'reliable', 'worth', 'durable', 'far', 'quick', 'perfectly', 'recommend', 'outstanding', 'pleased', 'impressive', 'old', 'simple', 'exactly', 'fast', 'expected', 'recommended', 'decent', 'impressed', 'nice', 'useful', 'cheaper', 'arrived', 'inexpensive', 'awsome', 'true', 'slick', 'effective']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'frustrating', 'useless', 'worst', 'bad', 'unreliable', 'awful', 'horrible', 'cheap', 'failed', 'broke', 'hard', 'wrong', 'died', 'impossible', 'defective', 'poorly', 'expensive', 'returning', 'unacceptable', 'overpriced', 'misleading', 'quit', 'broken', 'short', 'terrible', 'worthless', 'flimsy', 'ridiculous', 'beware', 'uncomfortable', 'much', 'trying', 'slow', 'save', 'lousy', 'lasted', 'missing', 'unhappy', 'average', 'low', 'waste', 'difficult', 'flawed', 'lose', 'incompatible', 'tried', 'sad', 'waiting', 'cannot', 'unable', 'mediocre', 'inferior']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
49470
5600 400 6000 23009 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 408.72053432, sen-loss: 77.46784014, dom-loss: 78.12013710, src-aux-loss: 134.49243236, tar-aux-loss: 118.64012545
Epoch: [1  ] train-acc: 0.71357143, dom-acc: 0.71223214, val-acc: 0.71500000, val_loss: 0.65199614
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 379.84485507, sen-loss: 68.86144370, dom-loss: 75.80254835, src-aux-loss: 125.04737210, tar-aux-loss: 110.13349193
Epoch: [2  ] train-acc: 0.75696429, dom-acc: 0.77285714, val-acc: 0.76250000, val_loss: 0.56798261
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 358.79344654, sen-loss: 58.25085658, dom-loss: 74.76516539, src-aux-loss: 119.66628104, tar-aux-loss: 106.11114174
Epoch: [3  ] train-acc: 0.80160714, dom-acc: 0.77267857, val-acc: 0.83750000, val_loss: 0.45082355
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 341.64059901, sen-loss: 48.50087151, dom-loss: 74.32173538, src-aux-loss: 115.95381755, tar-aux-loss: 102.86417443
Epoch: [4  ] train-acc: 0.82732143, dom-acc: 0.73580357, val-acc: 0.85750000, val_loss: 0.38135704
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 334.46908998, sen-loss: 44.04912122, dom-loss: 74.42856538, src-aux-loss: 113.71863788, tar-aux-loss: 102.27276671
Epoch: [5  ] train-acc: 0.84678571, dom-acc: 0.74276786, val-acc: 0.87000000, val_loss: 0.34479538
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 326.12139893, sen-loss: 40.70178045, dom-loss: 74.17598134, src-aux-loss: 111.53642017, tar-aux-loss: 99.70721763
Epoch: [6  ] train-acc: 0.85964286, dom-acc: 0.72455357, val-acc: 0.88250000, val_loss: 0.31654045
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 321.89989114, sen-loss: 38.02074653, dom-loss: 74.47507066, src-aux-loss: 109.65413237, tar-aux-loss: 99.74994087
Epoch: [7  ] train-acc: 0.86571429, dom-acc: 0.71526786, val-acc: 0.88250000, val_loss: 0.29705158
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 319.12763238, sen-loss: 36.22282590, dom-loss: 74.74594378, src-aux-loss: 108.37651402, tar-aux-loss: 99.78234893
Epoch: [8  ] train-acc: 0.87625000, dom-acc: 0.72830357, val-acc: 0.89750000, val_loss: 0.28576961
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 314.06762767, sen-loss: 34.89161846, dom-loss: 74.90776217, src-aux-loss: 107.05802768, tar-aux-loss: 97.21021867
Epoch: [9  ] train-acc: 0.88250000, dom-acc: 0.71339286, val-acc: 0.90250000, val_loss: 0.27056322
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 312.42190385, sen-loss: 33.52932550, dom-loss: 75.31135875, src-aux-loss: 105.96890825, tar-aux-loss: 97.61231261
Epoch: [10 ] train-acc: 0.89017857, dom-acc: 0.70580357, val-acc: 0.91250000, val_loss: 0.25633827
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 310.47056270, sen-loss: 32.47908212, dom-loss: 75.60824233, src-aux-loss: 105.61907768, tar-aux-loss: 96.76416147
Epoch: [11 ] train-acc: 0.88946429, dom-acc: 0.69223214, val-acc: 0.92000000, val_loss: 0.24652910
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 307.66946363, sen-loss: 31.76331952, dom-loss: 75.84241188, src-aux-loss: 104.20630765, tar-aux-loss: 95.85742390
Epoch: [12 ] train-acc: 0.89553571, dom-acc: 0.69017857, val-acc: 0.92000000, val_loss: 0.23804897
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 307.10359859, sen-loss: 30.94661688, dom-loss: 76.06127840, src-aux-loss: 103.54381198, tar-aux-loss: 96.55189133
Epoch: [13 ] train-acc: 0.89160714, dom-acc: 0.67133929, val-acc: 0.91750000, val_loss: 0.23698081
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 305.60201907, sen-loss: 30.33216476, dom-loss: 76.20866978, src-aux-loss: 102.89696252, tar-aux-loss: 96.16422158
Epoch: [14 ] train-acc: 0.90267857, dom-acc: 0.68437500, val-acc: 0.91750000, val_loss: 0.23006076
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 303.12413812, sen-loss: 29.71760699, dom-loss: 76.67599338, src-aux-loss: 102.23133594, tar-aux-loss: 94.49920201
Epoch: [15 ] train-acc: 0.90553571, dom-acc: 0.67196429, val-acc: 0.91750000, val_loss: 0.22375397
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 303.67031837, sen-loss: 29.17386533, dom-loss: 76.98439354, src-aux-loss: 102.05193633, tar-aux-loss: 95.46012539
Epoch: [16 ] train-acc: 0.90767857, dom-acc: 0.66857143, val-acc: 0.92500000, val_loss: 0.22220261
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 300.35028601, sen-loss: 28.57717866, dom-loss: 77.00430143, src-aux-loss: 101.32896525, tar-aux-loss: 93.43984008
Epoch: [17 ] train-acc: 0.90928571, dom-acc: 0.66946429, val-acc: 0.93000000, val_loss: 0.22117734
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 299.28009939, sen-loss: 28.23584734, dom-loss: 77.01221317, src-aux-loss: 100.62120640, tar-aux-loss: 93.41083246
Epoch: [18 ] train-acc: 0.91214286, dom-acc: 0.66633929, val-acc: 0.93000000, val_loss: 0.21559350
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 298.22347069, sen-loss: 27.66040819, dom-loss: 77.26918197, src-aux-loss: 100.17403108, tar-aux-loss: 93.11984825
Epoch: [19 ] train-acc: 0.91071429, dom-acc: 0.65901786, val-acc: 0.91750000, val_loss: 0.20955950
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 299.05798507, sen-loss: 27.33814859, dom-loss: 77.44912767, src-aux-loss: 99.55505180, tar-aux-loss: 94.71565700
Epoch: [20 ] train-acc: 0.91232143, dom-acc: 0.66410714, val-acc: 0.93250000, val_loss: 0.21409497
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 298.14832473, sen-loss: 26.98470693, dom-loss: 77.63200676, src-aux-loss: 99.34398019, tar-aux-loss: 94.18763155
Epoch: [21 ] train-acc: 0.91357143, dom-acc: 0.65303571, val-acc: 0.92000000, val_loss: 0.20488739
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 295.92530155, sen-loss: 26.63542443, dom-loss: 77.68507969, src-aux-loss: 98.79051256, tar-aux-loss: 92.81428456
Epoch: [22 ] train-acc: 0.91214286, dom-acc: 0.64687500, val-acc: 0.91750000, val_loss: 0.20382865
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 294.43144464, sen-loss: 26.12691171, dom-loss: 77.65885031, src-aux-loss: 98.28754354, tar-aux-loss: 92.35813922
Epoch: [23 ] train-acc: 0.91696429, dom-acc: 0.64517857, val-acc: 0.92750000, val_loss: 0.20294550
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 296.38740253, sen-loss: 25.79770604, dom-loss: 77.97536755, src-aux-loss: 98.32469767, tar-aux-loss: 94.28963149
Epoch: [24 ] train-acc: 0.91946429, dom-acc: 0.64687500, val-acc: 0.92750000, val_loss: 0.20351283
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 294.29022074, sen-loss: 25.50136918, dom-loss: 77.74951279, src-aux-loss: 97.47131073, tar-aux-loss: 93.56802702
Epoch: [25 ] train-acc: 0.92035714, dom-acc: 0.65544643, val-acc: 0.93000000, val_loss: 0.20044866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 292.76998830, sen-loss: 25.32411193, dom-loss: 77.91942328, src-aux-loss: 97.13614804, tar-aux-loss: 92.39030492
Epoch: [26 ] train-acc: 0.92142857, dom-acc: 0.65705357, val-acc: 0.93000000, val_loss: 0.20198753
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 292.53878856, sen-loss: 24.89416126, dom-loss: 77.83497036, src-aux-loss: 96.60328376, tar-aux-loss: 93.20637321
Epoch: [27 ] train-acc: 0.92232143, dom-acc: 0.64687500, val-acc: 0.93000000, val_loss: 0.19680460
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.90081620, sen-loss: 24.74047381, dom-loss: 78.01050192, src-aux-loss: 96.45041031, tar-aux-loss: 92.69943213
Epoch: [28 ] train-acc: 0.92392857, dom-acc: 0.65892857, val-acc: 0.92750000, val_loss: 0.19807607
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 291.50520945, sen-loss: 24.51052312, dom-loss: 77.70520699, src-aux-loss: 95.89435887, tar-aux-loss: 93.39512056
Epoch: [29 ] train-acc: 0.92285714, dom-acc: 0.65035714, val-acc: 0.93000000, val_loss: 0.19555159
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.13051796, sen-loss: 24.07611619, dom-loss: 77.85488820, src-aux-loss: 95.61560673, tar-aux-loss: 91.58390719
Epoch: [30 ] train-acc: 0.92714286, dom-acc: 0.66428571, val-acc: 0.92500000, val_loss: 0.19830164
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 289.53697610, sen-loss: 23.91169986, dom-loss: 77.92102635, src-aux-loss: 95.09477973, tar-aux-loss: 92.60946876
Epoch: [31 ] train-acc: 0.92964286, dom-acc: 0.65491071, val-acc: 0.93000000, val_loss: 0.19699600
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 287.85631537, sen-loss: 23.53945079, dom-loss: 77.40746009, src-aux-loss: 94.92109829, tar-aux-loss: 91.98830682
Epoch: [32 ] train-acc: 0.92464286, dom-acc: 0.65526786, val-acc: 0.93500000, val_loss: 0.19475619
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 286.96381664, sen-loss: 23.35783567, dom-loss: 77.64298481, src-aux-loss: 94.55157787, tar-aux-loss: 91.41141784
Epoch: [33 ] train-acc: 0.93142857, dom-acc: 0.66767857, val-acc: 0.93250000, val_loss: 0.19407788
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 287.32120943, sen-loss: 23.13477584, dom-loss: 77.64673889, src-aux-loss: 94.27737057, tar-aux-loss: 92.26232600
Epoch: [34 ] train-acc: 0.93053571, dom-acc: 0.66616071, val-acc: 0.92750000, val_loss: 0.19278048
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 286.37675881, sen-loss: 22.80634962, dom-loss: 77.51051158, src-aux-loss: 93.67090404, tar-aux-loss: 92.38899410
Epoch: [35 ] train-acc: 0.93125000, dom-acc: 0.66803571, val-acc: 0.93750000, val_loss: 0.19387867
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 285.92491841, sen-loss: 22.51704784, dom-loss: 77.40796137, src-aux-loss: 93.62186998, tar-aux-loss: 92.37803698
Epoch: [36 ] train-acc: 0.93321429, dom-acc: 0.66910714, val-acc: 0.93500000, val_loss: 0.18963201
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 284.03249741, sen-loss: 22.40893857, dom-loss: 77.32835633, src-aux-loss: 93.01798880, tar-aux-loss: 91.27721435
Epoch: [37 ] train-acc: 0.93464286, dom-acc: 0.65919643, val-acc: 0.93750000, val_loss: 0.18961392
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 282.88664436, sen-loss: 22.02799802, dom-loss: 77.04008818, src-aux-loss: 92.56716883, tar-aux-loss: 91.25138760
Epoch: [38 ] train-acc: 0.93178571, dom-acc: 0.67160714, val-acc: 0.93250000, val_loss: 0.19557239
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 283.10555720, sen-loss: 21.91938825, dom-loss: 77.33006257, src-aux-loss: 92.11417985, tar-aux-loss: 91.74192721
Epoch: [39 ] train-acc: 0.93553571, dom-acc: 0.67000000, val-acc: 0.93750000, val_loss: 0.18807304
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 281.16472793, sen-loss: 21.56126256, dom-loss: 77.15755808, src-aux-loss: 91.86805648, tar-aux-loss: 90.57785076
Epoch: [40 ] train-acc: 0.93660714, dom-acc: 0.66723214, val-acc: 0.93750000, val_loss: 0.18823512
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 281.18683076, sen-loss: 21.44909555, dom-loss: 77.11625504, src-aux-loss: 91.50165927, tar-aux-loss: 91.11982089
Epoch: [41 ] train-acc: 0.93696429, dom-acc: 0.67660714, val-acc: 0.93750000, val_loss: 0.18828730
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 280.64900017, sen-loss: 21.03518765, dom-loss: 76.88302118, src-aux-loss: 91.11679250, tar-aux-loss: 91.61399817
Epoch: [42 ] train-acc: 0.93875000, dom-acc: 0.67571429, val-acc: 0.94000000, val_loss: 0.18820037
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 278.97736692, sen-loss: 20.90989225, dom-loss: 77.02988273, src-aux-loss: 90.51708430, tar-aux-loss: 90.52050680
Epoch: [43 ] train-acc: 0.94000000, dom-acc: 0.67169643, val-acc: 0.93750000, val_loss: 0.18848975
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 279.48628569, sen-loss: 20.75101281, dom-loss: 76.92643440, src-aux-loss: 90.34379733, tar-aux-loss: 91.46504116
Epoch: [44 ] train-acc: 0.94232143, dom-acc: 0.67437500, val-acc: 0.94000000, val_loss: 0.18856865
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_kitchen_HATN.ckpt
Best Epoch: [ 39] best val accuracy: 0.00000000 best val loss: 0.18807304
Testing accuracy: 0.90283333
./work/attentions/electronics_kitchen_train_HATN.txt
./work/attentions/electronics_kitchen_test_HATN.txt
loading data...
source domain:  electronics target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 30180
vocab-size:  83059
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'fantastic', 'happy', 'awesome', 'solid', 'amazing', 'highly', 'satisfied', 'durable', 'reliable', 'worth', 'quick', 'perfectly', 'outstanding', 'love', 'impressive', 'recommend', 'fast', 'pleased', 'simple', 'nice', 'recommended', 'exactly', 'impressed', 'cheaper', 'inexpensive', 'decent', 'old', 'awsome', 'far', 'useful', 'slick', 'effective', 'expected', 'advertised']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'useless', 'frustrating', 'bad', 'worst', 'awful', 'horrible', 'unreliable', 'failed', 'cheap', 'hard', 'died', 'impossible', 'broke', 'expensive', 'wrong', 'poorly', 'defective', 'overpriced', 'unacceptable', 'returning', 'misleading', 'terrible', 'quit', 'broken', 'short', 'worthless', 'much', 'beware', 'flimsy', 'save', 'ridiculous', 'lousy', 'lasted', 'missing', 'unhappy', 'trying', 'slow', 'low', 'uncomfortable', 'cannot', 'waste', 'difficult', 'flawed', 'sent', 'incompatible', 'tried', 'acceptable', 'average', 'lose', 'nowhere', 'waiting', 'unable', 'mediocre', 'sad', 'inferior']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
83059
5600 400 6000 23009 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 411.64043260, sen-loss: 77.47696757, dom-loss: 76.48912460, src-aux-loss: 133.23706031, tar-aux-loss: 124.43728191
Epoch: [1  ] train-acc: 0.71196429, dom-acc: 0.86276786, val-acc: 0.71250000, val_loss: 0.65227538
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 382.10787535, sen-loss: 68.97751325, dom-loss: 73.06214917, src-aux-loss: 123.88189530, tar-aux-loss: 116.18631697
Epoch: [2  ] train-acc: 0.75500000, dom-acc: 0.84901786, val-acc: 0.77750000, val_loss: 0.57056952
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 362.35075355, sen-loss: 58.48431322, dom-loss: 71.73237079, src-aux-loss: 119.15081859, tar-aux-loss: 112.98325074
Epoch: [3  ] train-acc: 0.80464286, dom-acc: 0.73410714, val-acc: 0.84000000, val_loss: 0.45300987
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 345.49219990, sen-loss: 48.61376306, dom-loss: 71.82376480, src-aux-loss: 115.60572875, tar-aux-loss: 109.44894326
Epoch: [4  ] train-acc: 0.82428571, dom-acc: 0.62223214, val-acc: 0.85250000, val_loss: 0.38322368
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 338.53070283, sen-loss: 44.35163391, dom-loss: 72.70313454, src-aux-loss: 113.15047753, tar-aux-loss: 108.32545817
Epoch: [5  ] train-acc: 0.84642857, dom-acc: 0.59857143, val-acc: 0.87250000, val_loss: 0.34717494
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 333.62044621, sen-loss: 41.04045117, dom-loss: 74.14292955, src-aux-loss: 111.17044216, tar-aux-loss: 107.26662397
Epoch: [6  ] train-acc: 0.85803571, dom-acc: 0.58964286, val-acc: 0.88750000, val_loss: 0.31753591
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 328.16549277, sen-loss: 38.22851200, dom-loss: 75.45684963, src-aux-loss: 109.20971948, tar-aux-loss: 105.27041149
Epoch: [7  ] train-acc: 0.86517857, dom-acc: 0.54071429, val-acc: 0.89250000, val_loss: 0.29719138
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 325.66943669, sen-loss: 36.35189547, dom-loss: 76.42838335, src-aux-loss: 108.12785083, tar-aux-loss: 104.76130784
Epoch: [8  ] train-acc: 0.87660714, dom-acc: 0.49258929, val-acc: 0.89750000, val_loss: 0.28252596
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 322.96907377, sen-loss: 35.03907152, dom-loss: 77.63386416, src-aux-loss: 106.68672311, tar-aux-loss: 103.60941553
Epoch: [9  ] train-acc: 0.88232143, dom-acc: 0.46642857, val-acc: 0.90250000, val_loss: 0.26669452
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 321.98484397, sen-loss: 33.69451006, dom-loss: 78.69076228, src-aux-loss: 105.63629365, tar-aux-loss: 103.96327895
Epoch: [10 ] train-acc: 0.88750000, dom-acc: 0.43446429, val-acc: 0.91250000, val_loss: 0.25614136
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 319.06602550, sen-loss: 32.62450844, dom-loss: 79.36779070, src-aux-loss: 105.10083330, tar-aux-loss: 101.97289187
Epoch: [11 ] train-acc: 0.89035714, dom-acc: 0.40875000, val-acc: 0.92000000, val_loss: 0.24668300
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 317.57148147, sen-loss: 31.86285056, dom-loss: 79.58465987, src-aux-loss: 103.96195376, tar-aux-loss: 102.16201609
Epoch: [12 ] train-acc: 0.89535714, dom-acc: 0.39142857, val-acc: 0.91750000, val_loss: 0.23828730
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.64399004, sen-loss: 31.00533257, dom-loss: 79.58752787, src-aux-loss: 103.17710602, tar-aux-loss: 100.87402236
Epoch: [13 ] train-acc: 0.89428571, dom-acc: 0.40044643, val-acc: 0.92250000, val_loss: 0.23490088
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 314.03188014, sen-loss: 30.48981573, dom-loss: 79.50231701, src-aux-loss: 102.64107639, tar-aux-loss: 101.39867175
Epoch: [14 ] train-acc: 0.90107143, dom-acc: 0.39312500, val-acc: 0.91500000, val_loss: 0.23081708
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 311.70599008, sen-loss: 29.73908781, dom-loss: 78.99663299, src-aux-loss: 101.94214267, tar-aux-loss: 101.02812624
Epoch: [15 ] train-acc: 0.90392857, dom-acc: 0.42035714, val-acc: 0.91500000, val_loss: 0.22488798
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 309.43213534, sen-loss: 29.15574998, dom-loss: 78.54733193, src-aux-loss: 101.68223494, tar-aux-loss: 100.04681796
Epoch: [16 ] train-acc: 0.90446429, dom-acc: 0.43616071, val-acc: 0.91500000, val_loss: 0.22000952
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 306.39595318, sen-loss: 28.62493843, dom-loss: 78.15286690, src-aux-loss: 100.93877798, tar-aux-loss: 98.67936850
Epoch: [17 ] train-acc: 0.90875000, dom-acc: 0.46133929, val-acc: 0.92000000, val_loss: 0.22096825
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 306.04685998, sen-loss: 28.26810826, dom-loss: 77.84347498, src-aux-loss: 100.19967407, tar-aux-loss: 99.73560268
Epoch: [18 ] train-acc: 0.90964286, dom-acc: 0.49223214, val-acc: 0.92250000, val_loss: 0.21752967
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 303.84999967, sen-loss: 27.68697993, dom-loss: 77.62929535, src-aux-loss: 99.74554485, tar-aux-loss: 98.78817886
Epoch: [19 ] train-acc: 0.91178571, dom-acc: 0.50982143, val-acc: 0.92000000, val_loss: 0.21084554
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 303.37674499, sen-loss: 27.34052072, dom-loss: 77.13652778, src-aux-loss: 99.05370164, tar-aux-loss: 99.84599227
Epoch: [20 ] train-acc: 0.91250000, dom-acc: 0.51312500, val-acc: 0.92500000, val_loss: 0.21342862
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 301.73645020, sen-loss: 26.96141654, dom-loss: 76.92061925, src-aux-loss: 98.99722248, tar-aux-loss: 98.85719329
Epoch: [21 ] train-acc: 0.91196429, dom-acc: 0.51116071, val-acc: 0.92500000, val_loss: 0.20672023
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 300.13449359, sen-loss: 26.57591757, dom-loss: 76.90479076, src-aux-loss: 98.14319140, tar-aux-loss: 98.51059180
Epoch: [22 ] train-acc: 0.91303571, dom-acc: 0.51901786, val-acc: 0.92250000, val_loss: 0.20530373
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 297.41075659, sen-loss: 26.16155223, dom-loss: 76.76960140, src-aux-loss: 97.74651635, tar-aux-loss: 96.73308599
Epoch: [23 ] train-acc: 0.91321429, dom-acc: 0.52133929, val-acc: 0.92500000, val_loss: 0.20433511
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 300.65973353, sen-loss: 25.76736105, dom-loss: 76.90360397, src-aux-loss: 97.72797811, tar-aux-loss: 100.26078892
Epoch: [24 ] train-acc: 0.92000000, dom-acc: 0.51928571, val-acc: 0.92750000, val_loss: 0.20449163
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 298.89915276, sen-loss: 25.48939928, dom-loss: 77.23080552, src-aux-loss: 96.89431584, tar-aux-loss: 99.28463364
Epoch: [25 ] train-acc: 0.91660714, dom-acc: 0.51428571, val-acc: 0.93250000, val_loss: 0.20252049
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.58812046, sen-loss: 25.27416392, dom-loss: 77.36336058, src-aux-loss: 96.42346328, tar-aux-loss: 96.52713346
Epoch: [26 ] train-acc: 0.92285714, dom-acc: 0.49205357, val-acc: 0.92750000, val_loss: 0.20304859
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 295.07987547, sen-loss: 24.93836354, dom-loss: 77.55485994, src-aux-loss: 95.84983653, tar-aux-loss: 96.73681492
Epoch: [27 ] train-acc: 0.92107143, dom-acc: 0.49464286, val-acc: 0.92500000, val_loss: 0.19950952
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 296.07766032, sen-loss: 24.76298060, dom-loss: 77.73436677, src-aux-loss: 95.68216628, tar-aux-loss: 97.89814657
Epoch: [28 ] train-acc: 0.92446429, dom-acc: 0.46830357, val-acc: 0.92500000, val_loss: 0.20083131
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 296.34482765, sen-loss: 24.53430617, dom-loss: 78.11397266, src-aux-loss: 95.19100815, tar-aux-loss: 98.50554067
Epoch: [29 ] train-acc: 0.92071429, dom-acc: 0.46455357, val-acc: 0.93250000, val_loss: 0.19830877
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 292.61893892, sen-loss: 24.05001584, dom-loss: 78.37296748, src-aux-loss: 94.64893001, tar-aux-loss: 95.54702473
Epoch: [30 ] train-acc: 0.92767857, dom-acc: 0.45625000, val-acc: 0.93000000, val_loss: 0.20130891
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 293.17280531, sen-loss: 23.87536158, dom-loss: 78.55413377, src-aux-loss: 94.23339051, tar-aux-loss: 96.50991809
Epoch: [31 ] train-acc: 0.92750000, dom-acc: 0.43125000, val-acc: 0.92500000, val_loss: 0.19806559
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 293.89390707, sen-loss: 23.48796054, dom-loss: 78.80947125, src-aux-loss: 93.89083791, tar-aux-loss: 97.70563614
Epoch: [32 ] train-acc: 0.92375000, dom-acc: 0.44062500, val-acc: 0.93750000, val_loss: 0.19731757
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 293.27395940, sen-loss: 23.30111364, dom-loss: 78.87419277, src-aux-loss: 93.49149466, tar-aux-loss: 97.60715902
Epoch: [33 ] train-acc: 0.92857143, dom-acc: 0.43616071, val-acc: 0.93000000, val_loss: 0.19874127
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 291.45044923, sen-loss: 23.05401210, dom-loss: 78.85423005, src-aux-loss: 93.21787214, tar-aux-loss: 96.32433558
Epoch: [34 ] train-acc: 0.93071429, dom-acc: 0.43544643, val-acc: 0.93000000, val_loss: 0.19528431
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 291.12098956, sen-loss: 22.74968938, dom-loss: 78.75867254, src-aux-loss: 92.64316046, tar-aux-loss: 96.96946651
Epoch: [35 ] train-acc: 0.92892857, dom-acc: 0.42437500, val-acc: 0.93000000, val_loss: 0.19796832
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 289.11001682, sen-loss: 22.43454257, dom-loss: 78.64943987, src-aux-loss: 92.28763342, tar-aux-loss: 95.73840070
Epoch: [36 ] train-acc: 0.93107143, dom-acc: 0.43866071, val-acc: 0.93500000, val_loss: 0.19205546
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 288.62350082, sen-loss: 22.32544234, dom-loss: 78.50061446, src-aux-loss: 91.93162471, tar-aux-loss: 95.86581993
Epoch: [37 ] train-acc: 0.93428571, dom-acc: 0.44633929, val-acc: 0.94000000, val_loss: 0.19168946
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 287.79300976, sen-loss: 21.91884488, dom-loss: 78.22643816, src-aux-loss: 91.28018469, tar-aux-loss: 96.36754072
Epoch: [38 ] train-acc: 0.93410714, dom-acc: 0.44017857, val-acc: 0.93250000, val_loss: 0.19368002
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 286.98466372, sen-loss: 21.79988877, dom-loss: 77.99667513, src-aux-loss: 90.73576063, tar-aux-loss: 96.45233941
Epoch: [39 ] train-acc: 0.93589286, dom-acc: 0.44901786, val-acc: 0.93750000, val_loss: 0.19105449
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 285.19936514, sen-loss: 21.49525373, dom-loss: 77.79125661, src-aux-loss: 90.56809485, tar-aux-loss: 95.34476042
Epoch: [40 ] train-acc: 0.93750000, dom-acc: 0.47232143, val-acc: 0.93750000, val_loss: 0.19089688
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 284.22496247, sen-loss: 21.31076250, dom-loss: 77.55494791, src-aux-loss: 90.08560401, tar-aux-loss: 95.27364892
Epoch: [41 ] train-acc: 0.93625000, dom-acc: 0.48375000, val-acc: 0.93750000, val_loss: 0.19206657
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 283.51784754, sen-loss: 20.97127897, dom-loss: 77.18347883, src-aux-loss: 89.68137115, tar-aux-loss: 95.68171787
Epoch: [42 ] train-acc: 0.93910714, dom-acc: 0.48892857, val-acc: 0.94000000, val_loss: 0.18986674
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 282.26820517, sen-loss: 20.81015545, dom-loss: 77.11172736, src-aux-loss: 88.91768044, tar-aux-loss: 95.42864144
Epoch: [43 ] train-acc: 0.94035714, dom-acc: 0.50223214, val-acc: 0.93250000, val_loss: 0.19008042
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 281.21074986, sen-loss: 20.65655939, dom-loss: 76.95825994, src-aux-loss: 88.66644347, tar-aux-loss: 94.92948538
Epoch: [44 ] train-acc: 0.94107143, dom-acc: 0.52919643, val-acc: 0.93500000, val_loss: 0.18994676
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 280.13776565, sen-loss: 20.42998819, dom-loss: 76.92822218, src-aux-loss: 88.39788836, tar-aux-loss: 94.38166660
Epoch: [45 ] train-acc: 0.94125000, dom-acc: 0.52276786, val-acc: 0.93500000, val_loss: 0.18743521
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 279.96819115, sen-loss: 20.15092770, dom-loss: 76.98356342, src-aux-loss: 87.94970924, tar-aux-loss: 94.88399029
Epoch: [46 ] train-acc: 0.94375000, dom-acc: 0.51642857, val-acc: 0.93250000, val_loss: 0.18899411
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 278.57296705, sen-loss: 19.87559115, dom-loss: 76.81328815, src-aux-loss: 87.36160314, tar-aux-loss: 94.52248538
Epoch: [47 ] train-acc: 0.93857143, dom-acc: 0.51250000, val-acc: 0.92750000, val_loss: 0.19561967
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 279.48176575, sen-loss: 19.67765434, dom-loss: 77.16154021, src-aux-loss: 87.07827133, tar-aux-loss: 95.56429946
Epoch: [48 ] train-acc: 0.94517857, dom-acc: 0.54214286, val-acc: 0.93500000, val_loss: 0.18958864
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 277.94869947, sen-loss: 19.40247492, dom-loss: 77.16968471, src-aux-loss: 86.59336895, tar-aux-loss: 94.78317112
Epoch: [49 ] train-acc: 0.94535714, dom-acc: 0.50651786, val-acc: 0.93250000, val_loss: 0.18767130
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 275.79083538, sen-loss: 19.23006512, dom-loss: 77.33347899, src-aux-loss: 86.02113301, tar-aux-loss: 93.20615780
Epoch: [50 ] train-acc: 0.94625000, dom-acc: 0.51053571, val-acc: 0.93500000, val_loss: 0.18852325
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_video_HATN.ckpt
Best Epoch: [ 45] best val accuracy: 0.00000000 best val loss: 0.18743521
Testing accuracy: 0.83816667
./work/attentions/electronics_video_train_HATN.txt
./work/attentions/electronics_video_test_HATN.txt
loading data...
source domain:  kitchen target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 9750
vocab-size:  78006
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'wonderful', 'pleased', 'loves', 'fantastic', 'amazing', 'highly', 'awesome', 'satisfied', 'fabulous', 'attractive', 'nice', 'well', 'favorite', 'terrific', 'incredible', 'fast', 'quick', 'simple', 'loved', 'outstanding', 'useful', 'solid', 'elegant', 'better', 'tough', 'perfectly', 'decent', 'surprised', 'far']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'useless', 'broke', 'horrible', 'bad', 'poorly', 'broken', 'defective', 'save', 'impossible', 'returned', 'flimsy', 'awful', 'expensive', 'wrong', 'dissapointed', 'dangerous', 'ruined', 'overpriced', 'difficult', 'failed', 'worthless', 'worked', 'frustrating', 'stopped', 'misleading', 'unhappy', 'flawed', 'hard', 'quit', 'disapointed', 'worse', 'sad', 'messy', 'ugly', 'unacceptable', 'return', 'weak', 'lousy', 'ridiculous', 'shattered', 'hated', 'overrated', 'shocked', 'waiting', 'stuck', 'leaky']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
78006
5600 400 6000 19856 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 415.80530214, sen-loss: 76.80036819, dom-loss: 77.93006951, src-aux-loss: 140.44714451, tar-aux-loss: 120.62771863
Epoch: [1  ] train-acc: 0.76071429, dom-acc: 0.80125000, val-acc: 0.76250000, val_loss: 0.63802958
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 385.17102313, sen-loss: 66.55983931, dom-loss: 74.65663320, src-aux-loss: 131.35952270, tar-aux-loss: 112.59502763
Epoch: [2  ] train-acc: 0.77732143, dom-acc: 0.83035714, val-acc: 0.78500000, val_loss: 0.53564209
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.94388676, sen-loss: 54.69112197, dom-loss: 73.49142277, src-aux-loss: 124.97993898, tar-aux-loss: 107.78140235
Epoch: [3  ] train-acc: 0.83625000, dom-acc: 0.71580357, val-acc: 0.83750000, val_loss: 0.42163303
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 342.67659235, sen-loss: 43.72613040, dom-loss: 73.25585556, src-aux-loss: 120.63991147, tar-aux-loss: 105.05469739
Epoch: [4  ] train-acc: 0.86232143, dom-acc: 0.59187500, val-acc: 0.86500000, val_loss: 0.35336509
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 335.55764222, sen-loss: 38.63589741, dom-loss: 73.96614033, src-aux-loss: 117.79389757, tar-aux-loss: 105.16170591
Epoch: [5  ] train-acc: 0.88053571, dom-acc: 0.57455357, val-acc: 0.88500000, val_loss: 0.33060768
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 327.24640799, sen-loss: 35.79589324, dom-loss: 74.31072557, src-aux-loss: 115.23205173, tar-aux-loss: 101.90773743
Epoch: [6  ] train-acc: 0.88821429, dom-acc: 0.54616071, val-acc: 0.89000000, val_loss: 0.31556720
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 324.29908729, sen-loss: 33.55667119, dom-loss: 75.59620947, src-aux-loss: 113.46161395, tar-aux-loss: 101.68459082
Epoch: [7  ] train-acc: 0.88964286, dom-acc: 0.52526786, val-acc: 0.89750000, val_loss: 0.30593553
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 321.35474849, sen-loss: 31.98059940, dom-loss: 76.21014422, src-aux-loss: 112.37874013, tar-aux-loss: 100.78526402
Epoch: [8  ] train-acc: 0.89678571, dom-acc: 0.50553571, val-acc: 0.91000000, val_loss: 0.29873386
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 319.51033664, sen-loss: 30.34790386, dom-loss: 77.13004237, src-aux-loss: 110.86972123, tar-aux-loss: 101.16266936
Epoch: [9  ] train-acc: 0.90285714, dom-acc: 0.47616071, val-acc: 0.89500000, val_loss: 0.30565944
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 316.09876633, sen-loss: 29.54022923, dom-loss: 77.47206032, src-aux-loss: 109.97738564, tar-aux-loss: 99.10909307
Epoch: [10 ] train-acc: 0.90464286, dom-acc: 0.46232143, val-acc: 0.91000000, val_loss: 0.29316401
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 314.90817094, sen-loss: 28.55827707, dom-loss: 77.98183626, src-aux-loss: 109.18845141, tar-aux-loss: 99.17960799
Epoch: [11 ] train-acc: 0.90750000, dom-acc: 0.45116071, val-acc: 0.91500000, val_loss: 0.28735834
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 313.75834942, sen-loss: 27.81427404, dom-loss: 78.12095332, src-aux-loss: 108.11285055, tar-aux-loss: 99.71027172
Epoch: [12 ] train-acc: 0.90589286, dom-acc: 0.44366071, val-acc: 0.92500000, val_loss: 0.28508586
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 310.58185768, sen-loss: 27.21254139, dom-loss: 78.29576373, src-aux-loss: 107.17998821, tar-aux-loss: 97.89356476
Epoch: [13 ] train-acc: 0.90964286, dom-acc: 0.45267857, val-acc: 0.89500000, val_loss: 0.30550838
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 311.37144661, sen-loss: 26.65567620, dom-loss: 78.68870932, src-aux-loss: 106.55555087, tar-aux-loss: 99.47150820
Epoch: [14 ] train-acc: 0.91285714, dom-acc: 0.45946429, val-acc: 0.90750000, val_loss: 0.29388964
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 308.34006834, sen-loss: 26.39997491, dom-loss: 78.68888241, src-aux-loss: 105.71292555, tar-aux-loss: 97.53828639
Epoch: [15 ] train-acc: 0.91553571, dom-acc: 0.46508929, val-acc: 0.91500000, val_loss: 0.28168154
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 307.35927033, sen-loss: 25.59534608, dom-loss: 78.77242714, src-aux-loss: 105.03211683, tar-aux-loss: 97.95938021
Epoch: [16 ] train-acc: 0.91553571, dom-acc: 0.47714286, val-acc: 0.91250000, val_loss: 0.28369865
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 305.04427481, sen-loss: 25.16113632, dom-loss: 78.69104415, src-aux-loss: 104.28421324, tar-aux-loss: 96.90788114
Epoch: [17 ] train-acc: 0.91750000, dom-acc: 0.47616071, val-acc: 0.91000000, val_loss: 0.28710622
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 305.68658686, sen-loss: 24.81785657, dom-loss: 78.69119537, src-aux-loss: 103.77247250, tar-aux-loss: 98.40506399
Epoch: [18 ] train-acc: 0.91910714, dom-acc: 0.49303571, val-acc: 0.91000000, val_loss: 0.28462058
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 302.84385943, sen-loss: 24.60439893, dom-loss: 78.60647637, src-aux-loss: 103.22386199, tar-aux-loss: 96.40912282
Epoch: [19 ] train-acc: 0.92160714, dom-acc: 0.49071429, val-acc: 0.91500000, val_loss: 0.28277647
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 301.60938907, sen-loss: 24.15445236, dom-loss: 78.63371444, src-aux-loss: 102.60753518, tar-aux-loss: 96.21368873
Epoch: [20 ] train-acc: 0.92250000, dom-acc: 0.51598214, val-acc: 0.92250000, val_loss: 0.27521023
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 302.02570128, sen-loss: 23.92690091, dom-loss: 78.20091575, src-aux-loss: 102.06650174, tar-aux-loss: 97.83138293
Epoch: [21 ] train-acc: 0.92375000, dom-acc: 0.49973214, val-acc: 0.92250000, val_loss: 0.27285928
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 300.08896279, sen-loss: 23.70971975, dom-loss: 78.10148191, src-aux-loss: 101.35042924, tar-aux-loss: 96.92733204
Epoch: [22 ] train-acc: 0.92517857, dom-acc: 0.49330357, val-acc: 0.91500000, val_loss: 0.27562550
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 297.89827013, sen-loss: 23.27120143, dom-loss: 78.01242065, src-aux-loss: 100.91241831, tar-aux-loss: 95.70223129
Epoch: [23 ] train-acc: 0.92625000, dom-acc: 0.50794643, val-acc: 0.91500000, val_loss: 0.27377912
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 297.88588476, sen-loss: 23.11140177, dom-loss: 77.79003358, src-aux-loss: 100.45274454, tar-aux-loss: 96.53170621
Epoch: [24 ] train-acc: 0.92750000, dom-acc: 0.50982143, val-acc: 0.91750000, val_loss: 0.27978137
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 296.92584562, sen-loss: 22.70252284, dom-loss: 77.67962766, src-aux-loss: 99.61157393, tar-aux-loss: 96.93212032
Epoch: [25 ] train-acc: 0.92803571, dom-acc: 0.53848214, val-acc: 0.91750000, val_loss: 0.27179441
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 294.98823738, sen-loss: 22.49908496, dom-loss: 77.47815597, src-aux-loss: 99.47086221, tar-aux-loss: 95.54013437
Epoch: [26 ] train-acc: 0.92964286, dom-acc: 0.51035714, val-acc: 0.91500000, val_loss: 0.27527955
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 293.62498283, sen-loss: 22.33452817, dom-loss: 77.32340431, src-aux-loss: 98.87714732, tar-aux-loss: 95.08990204
Epoch: [27 ] train-acc: 0.93071429, dom-acc: 0.51616071, val-acc: 0.91250000, val_loss: 0.27461904
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 293.82477808, sen-loss: 22.24396408, dom-loss: 77.15356320, src-aux-loss: 98.34038121, tar-aux-loss: 96.08687043
Epoch: [28 ] train-acc: 0.93000000, dom-acc: 0.51982143, val-acc: 0.92000000, val_loss: 0.26761767
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 292.35763860, sen-loss: 21.95506500, dom-loss: 77.27705634, src-aux-loss: 97.73199880, tar-aux-loss: 95.39351851
Epoch: [29 ] train-acc: 0.93107143, dom-acc: 0.53616071, val-acc: 0.91500000, val_loss: 0.26911175
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 291.57646322, sen-loss: 21.70352093, dom-loss: 76.97443855, src-aux-loss: 97.28409177, tar-aux-loss: 95.61441201
Epoch: [30 ] train-acc: 0.93321429, dom-acc: 0.52330357, val-acc: 0.91750000, val_loss: 0.27532083
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 290.49881840, sen-loss: 21.54980066, dom-loss: 76.95241421, src-aux-loss: 96.81670266, tar-aux-loss: 95.17990065
Epoch: [31 ] train-acc: 0.93339286, dom-acc: 0.52705357, val-acc: 0.91500000, val_loss: 0.27420866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 289.22717547, sen-loss: 21.30973080, dom-loss: 77.06174850, src-aux-loss: 96.27690649, tar-aux-loss: 94.57878953
Epoch: [32 ] train-acc: 0.93446429, dom-acc: 0.52875000, val-acc: 0.92000000, val_loss: 0.27552956
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 288.53262424, sen-loss: 21.00960066, dom-loss: 76.77660549, src-aux-loss: 95.67348242, tar-aux-loss: 95.07293385
Epoch: [33 ] train-acc: 0.93517857, dom-acc: 0.49812500, val-acc: 0.91750000, val_loss: 0.27355725
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_books_HATN.ckpt
Best Epoch: [ 28] best val accuracy: 0.00000000 best val loss: 0.26761767
Testing accuracy: 0.85233333
./work/attentions/kitchen_books_train_HATN.txt
./work/attentions/kitchen_books_test_HATN.txt
loading data...
source domain:  kitchen target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 11843
vocab-size:  80685
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'pleased', 'wonderful', 'loves', 'fantastic', 'highly', 'amazing', 'awesome', 'satisfied', 'fabulous', 'nice', 'attractive', 'well', 'terrific', 'fast', 'favorite', 'incredible', 'quick', 'useful', 'solid', 'simple', 'outstanding', 'elegant', 'tough', 'easier', 'perfectly', 'decent', 'surprised', 'durable', 'far', 'impressed']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'useless', 'broke', 'horrible', 'poorly', 'bad', 'broken', 'defective', 'save', 'returned', 'flimsy', 'expensive', 'awful', 'impossible', 'wrong', 'dangerous', 'dissapointed', 'failed', 'overpriced', 'stopped', 'ruined', 'difficult', 'worthless', 'hard', 'frustrating', 'misleading', 'unhappy', 'better', 'quit', 'disapointed', 'flawed', 'worse', 'worked', 'sad', 'return', 'messy', 'weak', 'lousy', 'stuck', 'shattered', 'embarrassed', 'fell', 'overrated', 'shocked', 'waiting', 'unacceptable', 'leaky']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
80685
5600 400 6000 19856 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 419.65668964, sen-loss: 76.84336847, dom-loss: 77.09116000, src-aux-loss: 141.95029366, tar-aux-loss: 123.77186871
Epoch: [1  ] train-acc: 0.76589286, dom-acc: 0.81446429, val-acc: 0.76750000, val_loss: 0.63838810
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 388.97062302, sen-loss: 66.76787871, dom-loss: 73.89264667, src-aux-loss: 133.04075503, tar-aux-loss: 115.26934451
Epoch: [2  ] train-acc: 0.77660714, dom-acc: 0.79035714, val-acc: 0.79250000, val_loss: 0.53863019
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 365.17713642, sen-loss: 54.93314475, dom-loss: 72.72805232, src-aux-loss: 126.95091099, tar-aux-loss: 110.56502849
Epoch: [3  ] train-acc: 0.83910714, dom-acc: 0.68294643, val-acc: 0.83750000, val_loss: 0.42376044
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.79347420, sen-loss: 43.87467739, dom-loss: 72.66149676, src-aux-loss: 122.79630524, tar-aux-loss: 108.46099555
Epoch: [4  ] train-acc: 0.85714286, dom-acc: 0.58598214, val-acc: 0.86750000, val_loss: 0.35515708
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 339.55179024, sen-loss: 39.04689771, dom-loss: 73.90980768, src-aux-loss: 119.94032031, tar-aux-loss: 106.65476453
Epoch: [5  ] train-acc: 0.87803571, dom-acc: 0.58125000, val-acc: 0.88250000, val_loss: 0.33090186
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 332.64493704, sen-loss: 35.97467636, dom-loss: 74.03908128, src-aux-loss: 117.76574266, tar-aux-loss: 104.86543757
Epoch: [6  ] train-acc: 0.88482143, dom-acc: 0.54589286, val-acc: 0.88250000, val_loss: 0.31491804
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 329.80592966, sen-loss: 33.74734266, dom-loss: 75.61567521, src-aux-loss: 115.72902697, tar-aux-loss: 104.71388555
Epoch: [7  ] train-acc: 0.89017857, dom-acc: 0.52437500, val-acc: 0.89750000, val_loss: 0.30672786
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 326.08385420, sen-loss: 32.13681652, dom-loss: 76.37497687, src-aux-loss: 114.76419687, tar-aux-loss: 102.80786508
Epoch: [8  ] train-acc: 0.89642857, dom-acc: 0.49464286, val-acc: 0.90000000, val_loss: 0.30071968
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 324.99938583, sen-loss: 30.43910326, dom-loss: 77.70089000, src-aux-loss: 112.79747117, tar-aux-loss: 104.06192231
Epoch: [9  ] train-acc: 0.90035714, dom-acc: 0.45651786, val-acc: 0.89250000, val_loss: 0.30103230
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 320.57781601, sen-loss: 29.55676122, dom-loss: 77.70307046, src-aux-loss: 111.98587054, tar-aux-loss: 101.33211404
Epoch: [10 ] train-acc: 0.90428571, dom-acc: 0.45312500, val-acc: 0.90250000, val_loss: 0.29247501
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 319.03156590, sen-loss: 28.59069180, dom-loss: 78.29753828, src-aux-loss: 111.11471087, tar-aux-loss: 101.02862525
Epoch: [11 ] train-acc: 0.90517857, dom-acc: 0.43116071, val-acc: 0.90250000, val_loss: 0.28788069
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 317.30296946, sen-loss: 27.84081513, dom-loss: 78.46413195, src-aux-loss: 109.89588279, tar-aux-loss: 101.10213876
Epoch: [12 ] train-acc: 0.90607143, dom-acc: 0.43633929, val-acc: 0.90250000, val_loss: 0.28601819
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.95205784, sen-loss: 27.39992086, dom-loss: 78.42523223, src-aux-loss: 108.96270275, tar-aux-loss: 100.16420263
Epoch: [13 ] train-acc: 0.90928571, dom-acc: 0.42928571, val-acc: 0.89500000, val_loss: 0.30664876
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 313.39334178, sen-loss: 26.57989425, dom-loss: 78.39243329, src-aux-loss: 108.15634793, tar-aux-loss: 100.26466662
Epoch: [14 ] train-acc: 0.91071429, dom-acc: 0.42383929, val-acc: 0.90250000, val_loss: 0.29587746
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 311.96771526, sen-loss: 26.40801328, dom-loss: 78.30093169, src-aux-loss: 107.56433898, tar-aux-loss: 99.69443303
Epoch: [15 ] train-acc: 0.91375000, dom-acc: 0.44312500, val-acc: 0.91750000, val_loss: 0.28178430
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 310.92429614, sen-loss: 25.62662079, dom-loss: 78.31857693, src-aux-loss: 106.69880170, tar-aux-loss: 100.28029692
Epoch: [16 ] train-acc: 0.91428571, dom-acc: 0.45133929, val-acc: 0.91000000, val_loss: 0.28206149
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 308.02035737, sen-loss: 25.22095288, dom-loss: 77.95096153, src-aux-loss: 106.16735691, tar-aux-loss: 98.68108606
Epoch: [17 ] train-acc: 0.91589286, dom-acc: 0.45178571, val-acc: 0.90500000, val_loss: 0.28439316
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 308.41057205, sen-loss: 24.77798943, dom-loss: 77.89670795, src-aux-loss: 105.73185164, tar-aux-loss: 100.00402254
Epoch: [18 ] train-acc: 0.91785714, dom-acc: 0.47258929, val-acc: 0.90750000, val_loss: 0.28416705
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 305.01965261, sen-loss: 24.60685510, dom-loss: 77.47148830, src-aux-loss: 104.94439000, tar-aux-loss: 97.99691963
Epoch: [19 ] train-acc: 0.91839286, dom-acc: 0.46714286, val-acc: 0.91000000, val_loss: 0.28179163
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 304.14288902, sen-loss: 24.17258081, dom-loss: 77.39515966, src-aux-loss: 104.36793166, tar-aux-loss: 98.20721668
Epoch: [20 ] train-acc: 0.92071429, dom-acc: 0.48491071, val-acc: 0.92000000, val_loss: 0.27667558
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 304.37194014, sen-loss: 23.98442546, dom-loss: 77.48332357, src-aux-loss: 103.86564994, tar-aux-loss: 99.03854144
Epoch: [21 ] train-acc: 0.92178571, dom-acc: 0.49785714, val-acc: 0.91750000, val_loss: 0.27478120
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 302.29009199, sen-loss: 23.70887662, dom-loss: 77.17680770, src-aux-loss: 103.16111797, tar-aux-loss: 98.24329072
Epoch: [22 ] train-acc: 0.92285714, dom-acc: 0.49098214, val-acc: 0.92250000, val_loss: 0.27590293
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 301.27867460, sen-loss: 23.18544574, dom-loss: 77.11879396, src-aux-loss: 102.60068381, tar-aux-loss: 98.37375057
Epoch: [23 ] train-acc: 0.92428571, dom-acc: 0.50383929, val-acc: 0.92000000, val_loss: 0.27473584
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 300.29417801, sen-loss: 23.09260757, dom-loss: 77.00970250, src-aux-loss: 102.39075762, tar-aux-loss: 97.80111063
Epoch: [24 ] train-acc: 0.92464286, dom-acc: 0.50178571, val-acc: 0.91750000, val_loss: 0.27722254
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 299.36161017, sen-loss: 22.65024351, dom-loss: 77.13024634, src-aux-loss: 101.52402836, tar-aux-loss: 98.05709141
Epoch: [25 ] train-acc: 0.92607143, dom-acc: 0.50750000, val-acc: 0.92250000, val_loss: 0.27517956
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 298.09499145, sen-loss: 22.46399944, dom-loss: 76.84326845, src-aux-loss: 101.15128475, tar-aux-loss: 97.63643724
Epoch: [26 ] train-acc: 0.92785714, dom-acc: 0.49732143, val-acc: 0.92250000, val_loss: 0.27627549
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 297.37705302, sen-loss: 22.33999427, dom-loss: 77.06982738, src-aux-loss: 100.66573858, tar-aux-loss: 97.30149466
Epoch: [27 ] train-acc: 0.92785714, dom-acc: 0.50080357, val-acc: 0.92000000, val_loss: 0.27324015
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 297.37704492, sen-loss: 22.20492930, dom-loss: 76.93645281, src-aux-loss: 100.37638688, tar-aux-loss: 97.85927635
Epoch: [28 ] train-acc: 0.92839286, dom-acc: 0.49866071, val-acc: 0.92250000, val_loss: 0.27139071
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 295.14662027, sen-loss: 21.88911832, dom-loss: 77.27086413, src-aux-loss: 99.73723346, tar-aux-loss: 96.24940550
Epoch: [29 ] train-acc: 0.93035714, dom-acc: 0.49446429, val-acc: 0.92500000, val_loss: 0.27092350
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 294.87926221, sen-loss: 21.66105258, dom-loss: 77.12277484, src-aux-loss: 99.14354998, tar-aux-loss: 96.95188665
Epoch: [30 ] train-acc: 0.93125000, dom-acc: 0.48919643, val-acc: 0.91750000, val_loss: 0.27672300
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 294.35161066, sen-loss: 21.50632980, dom-loss: 77.19843721, src-aux-loss: 98.68281150, tar-aux-loss: 96.96403432
Epoch: [31 ] train-acc: 0.93250000, dom-acc: 0.48848214, val-acc: 0.91750000, val_loss: 0.27482587
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 294.26773787, sen-loss: 21.19565674, dom-loss: 77.26916009, src-aux-loss: 98.11320108, tar-aux-loss: 97.68971962
Epoch: [32 ] train-acc: 0.93357143, dom-acc: 0.48160714, val-acc: 0.92000000, val_loss: 0.27689111
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 292.57866025, sen-loss: 20.94593852, dom-loss: 77.20609206, src-aux-loss: 97.66656607, tar-aux-loss: 96.76006454
Epoch: [33 ] train-acc: 0.93285714, dom-acc: 0.46544643, val-acc: 0.92000000, val_loss: 0.27730531
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 291.65716076, sen-loss: 20.87334752, dom-loss: 77.38743216, src-aux-loss: 97.45176756, tar-aux-loss: 95.94461483
Epoch: [34 ] train-acc: 0.93517857, dom-acc: 0.47410714, val-acc: 0.92000000, val_loss: 0.27495247
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_dvd_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.27092350
Testing accuracy: 0.84916667
./work/attentions/kitchen_dvd_train_HATN.txt
./work/attentions/kitchen_dvd_test_HATN.txt
loading data...
source domain:  kitchen target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 17009
vocab-size:  49470
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'wonderful', 'pleased', 'loves', 'fantastic', 'amazing', 'highly', 'awesome', 'satisfied', 'attractive', 'fabulous', 'nice', 'well', 'terrific', 'loved', 'favorite', 'incredible', 'fast', 'quick', 'simple', 'outstanding', 'useful', 'solid', 'elegant', 'tough', 'perfectly', 'decent', 'surprised', 'pretty']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'broke', 'useless', 'horrible', 'poorly', 'returned', 'bad', 'broken', 'flimsy', 'defective', 'save', 'impossible', 'awful', 'wrong', 'expensive', 'dissapointed', 'dangerous', 'stopped', 'failed', 'ruined', 'overpriced', 'difficult', 'worthless', 'hard', 'quit', 'frustrating', 'misleading', 'unhappy', 'flawed', 'sad', 'disapointed', 'worse', 'worked', 'better', 'messy', 'return', 'ugly', 'unacceptable', 'weak', 'lousy', 'ridiculous', 'inconsistent', 'shattered', 'hated', 'frustrated', 'overrated', 'shocked', 'waiting', 'stuck', 'sadly', 'leaky']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
49470
5600 400 6000 19856 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 421.80430365, sen-loss: 76.81231582, dom-loss: 78.85567093, src-aux-loss: 142.70579505, tar-aux-loss: 123.43052143
Epoch: [1  ] train-acc: 0.76500000, dom-acc: 0.64482143, val-acc: 0.76500000, val_loss: 0.63777435
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 391.56938171, sen-loss: 66.53404322, dom-loss: 76.28193253, src-aux-loss: 133.78776717, tar-aux-loss: 114.96563923
Epoch: [2  ] train-acc: 0.78392857, dom-acc: 0.66767857, val-acc: 0.80750000, val_loss: 0.53441083
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 367.81789017, sen-loss: 54.59596977, dom-loss: 75.04479051, src-aux-loss: 127.36051589, tar-aux-loss: 110.81661350
Epoch: [3  ] train-acc: 0.84285714, dom-acc: 0.60982143, val-acc: 0.84750000, val_loss: 0.42002416
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 348.49078274, sen-loss: 43.57408124, dom-loss: 74.95466250, src-aux-loss: 123.15670556, tar-aux-loss: 106.80533242
Epoch: [4  ] train-acc: 0.85910714, dom-acc: 0.62910714, val-acc: 0.86750000, val_loss: 0.35032335
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 339.25538898, sen-loss: 38.68611833, dom-loss: 74.69918549, src-aux-loss: 120.04023266, tar-aux-loss: 105.82985288
Epoch: [5  ] train-acc: 0.87910714, dom-acc: 0.71821429, val-acc: 0.88000000, val_loss: 0.32922217
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 332.45803380, sen-loss: 35.69971374, dom-loss: 74.67333621, src-aux-loss: 117.73172188, tar-aux-loss: 104.35325903
Epoch: [6  ] train-acc: 0.88500000, dom-acc: 0.66544643, val-acc: 0.89250000, val_loss: 0.31281778
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 329.34145617, sen-loss: 33.71775419, dom-loss: 74.85390371, src-aux-loss: 116.35020614, tar-aux-loss: 104.41959286
Epoch: [7  ] train-acc: 0.88964286, dom-acc: 0.64107143, val-acc: 0.89500000, val_loss: 0.30464882
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 323.56999063, sen-loss: 32.01750110, dom-loss: 75.03416085, src-aux-loss: 114.94006056, tar-aux-loss: 101.57826900
Epoch: [8  ] train-acc: 0.89714286, dom-acc: 0.66366071, val-acc: 0.90500000, val_loss: 0.29902485
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 321.42169309, sen-loss: 30.42197248, dom-loss: 75.37751108, src-aux-loss: 113.33876467, tar-aux-loss: 102.28344434
Epoch: [9  ] train-acc: 0.90178571, dom-acc: 0.66750000, val-acc: 0.89500000, val_loss: 0.30218279
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 319.16914964, sen-loss: 29.45256609, dom-loss: 75.60861701, src-aux-loss: 112.19106150, tar-aux-loss: 101.91690570
Epoch: [10 ] train-acc: 0.90357143, dom-acc: 0.64080357, val-acc: 0.90500000, val_loss: 0.29224938
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 315.39313293, sen-loss: 28.60499125, dom-loss: 75.87811732, src-aux-loss: 111.54941511, tar-aux-loss: 99.36060911
Epoch: [11 ] train-acc: 0.90678571, dom-acc: 0.66526786, val-acc: 0.90750000, val_loss: 0.28686893
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 313.95306969, sen-loss: 27.76620596, dom-loss: 76.06257451, src-aux-loss: 110.36174363, tar-aux-loss: 99.76254493
Epoch: [12 ] train-acc: 0.90767857, dom-acc: 0.65232143, val-acc: 0.90750000, val_loss: 0.28479093
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 311.66774845, sen-loss: 27.29957769, dom-loss: 76.42816657, src-aux-loss: 109.43793094, tar-aux-loss: 98.50207317
Epoch: [13 ] train-acc: 0.90642857, dom-acc: 0.65803571, val-acc: 0.89750000, val_loss: 0.30774811
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 311.28360844, sen-loss: 26.57515571, dom-loss: 76.54236221, src-aux-loss: 108.49691057, tar-aux-loss: 99.66917789
Epoch: [14 ] train-acc: 0.91160714, dom-acc: 0.64839286, val-acc: 0.90500000, val_loss: 0.29206574
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 309.47452426, sen-loss: 26.31823170, dom-loss: 76.81531090, src-aux-loss: 107.86422956, tar-aux-loss: 98.47675121
Epoch: [15 ] train-acc: 0.91607143, dom-acc: 0.63508929, val-acc: 0.91000000, val_loss: 0.28094190
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 306.93046188, sen-loss: 25.54855014, dom-loss: 76.89029986, src-aux-loss: 106.85230118, tar-aux-loss: 97.63931024
Epoch: [16 ] train-acc: 0.91482143, dom-acc: 0.64544643, val-acc: 0.91250000, val_loss: 0.28189671
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 306.36974692, sen-loss: 25.13493787, dom-loss: 77.11936545, src-aux-loss: 106.31536740, tar-aux-loss: 97.80007529
Epoch: [17 ] train-acc: 0.91589286, dom-acc: 0.63312500, val-acc: 0.91000000, val_loss: 0.29073671
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 306.76594806, sen-loss: 24.67970165, dom-loss: 77.30394828, src-aux-loss: 105.81587118, tar-aux-loss: 98.96642685
Epoch: [18 ] train-acc: 0.91892857, dom-acc: 0.65714286, val-acc: 0.91500000, val_loss: 0.28171781
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 303.63815689, sen-loss: 24.59614570, dom-loss: 77.46915931, src-aux-loss: 105.06802267, tar-aux-loss: 96.50482792
Epoch: [19 ] train-acc: 0.91910714, dom-acc: 0.63000000, val-acc: 0.92000000, val_loss: 0.27987003
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 303.59127879, sen-loss: 24.08536963, dom-loss: 77.52743775, src-aux-loss: 104.52156979, tar-aux-loss: 97.45690244
Epoch: [20 ] train-acc: 0.92214286, dom-acc: 0.65446429, val-acc: 0.92000000, val_loss: 0.27653837
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 302.67022395, sen-loss: 23.83075454, dom-loss: 77.69079894, src-aux-loss: 103.97879344, tar-aux-loss: 97.16987765
Epoch: [21 ] train-acc: 0.92107143, dom-acc: 0.62526786, val-acc: 0.91500000, val_loss: 0.27286446
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 301.91242909, sen-loss: 23.63762651, dom-loss: 77.86570352, src-aux-loss: 103.24697918, tar-aux-loss: 97.16211808
Epoch: [22 ] train-acc: 0.92464286, dom-acc: 0.60169643, val-acc: 0.92000000, val_loss: 0.27822202
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 300.24915981, sen-loss: 23.18355725, dom-loss: 77.86107355, src-aux-loss: 102.85244405, tar-aux-loss: 96.35208368
Epoch: [23 ] train-acc: 0.92571429, dom-acc: 0.63401786, val-acc: 0.92250000, val_loss: 0.27463374
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 299.56277418, sen-loss: 22.99212492, dom-loss: 78.02677345, src-aux-loss: 102.29237062, tar-aux-loss: 96.25150609
Epoch: [24 ] train-acc: 0.92589286, dom-acc: 0.62098214, val-acc: 0.91750000, val_loss: 0.28191912
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 299.06172562, sen-loss: 22.63486534, dom-loss: 78.08368069, src-aux-loss: 101.66620755, tar-aux-loss: 96.67697167
Epoch: [25 ] train-acc: 0.92732143, dom-acc: 0.64035714, val-acc: 0.92000000, val_loss: 0.27219746
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 299.01871657, sen-loss: 22.45940481, dom-loss: 78.25566876, src-aux-loss: 101.45715374, tar-aux-loss: 96.84648877
Epoch: [26 ] train-acc: 0.92785714, dom-acc: 0.58375000, val-acc: 0.92500000, val_loss: 0.27699664
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 296.36510682, sen-loss: 22.33852805, dom-loss: 78.18503124, src-aux-loss: 100.83662993, tar-aux-loss: 95.00491869
Epoch: [27 ] train-acc: 0.92767857, dom-acc: 0.59303571, val-acc: 0.92000000, val_loss: 0.27391106
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 297.84777546, sen-loss: 22.25773787, dom-loss: 78.39336485, src-aux-loss: 100.42900836, tar-aux-loss: 96.76766294
Epoch: [28 ] train-acc: 0.92875000, dom-acc: 0.57776786, val-acc: 0.92250000, val_loss: 0.27064976
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 296.05992508, sen-loss: 21.91789566, dom-loss: 78.31654829, src-aux-loss: 99.86641467, tar-aux-loss: 95.95906526
Epoch: [29 ] train-acc: 0.93017857, dom-acc: 0.59973214, val-acc: 0.92250000, val_loss: 0.27037379
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 294.87771559, sen-loss: 21.59950188, dom-loss: 78.45619524, src-aux-loss: 99.46303934, tar-aux-loss: 95.35897988
Epoch: [30 ] train-acc: 0.93178571, dom-acc: 0.59732143, val-acc: 0.92750000, val_loss: 0.27121729
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 293.53787875, sen-loss: 21.48644917, dom-loss: 78.39487970, src-aux-loss: 98.88997459, tar-aux-loss: 94.76657498
Epoch: [31 ] train-acc: 0.93142857, dom-acc: 0.60017857, val-acc: 0.92750000, val_loss: 0.27181017
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 293.93394709, sen-loss: 21.19270696, dom-loss: 78.48604071, src-aux-loss: 98.33371174, tar-aux-loss: 95.92148894
Epoch: [32 ] train-acc: 0.93196429, dom-acc: 0.60339286, val-acc: 0.93000000, val_loss: 0.26931772
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 291.73484945, sen-loss: 20.95829058, dom-loss: 78.51085681, src-aux-loss: 98.02565277, tar-aux-loss: 94.24005067
Epoch: [33 ] train-acc: 0.93517857, dom-acc: 0.57089286, val-acc: 0.92750000, val_loss: 0.27332258
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 291.36453652, sen-loss: 20.80810960, dom-loss: 78.53363317, src-aux-loss: 97.64674729, tar-aux-loss: 94.37604511
Epoch: [34 ] train-acc: 0.93607143, dom-acc: 0.58508929, val-acc: 0.92250000, val_loss: 0.27334583
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 291.64458728, sen-loss: 20.59823773, dom-loss: 78.53732002, src-aux-loss: 96.96349299, tar-aux-loss: 95.54553711
Epoch: [35 ] train-acc: 0.93571429, dom-acc: 0.61508929, val-acc: 0.92000000, val_loss: 0.27280661
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 291.16045976, sen-loss: 20.33837049, dom-loss: 78.36069638, src-aux-loss: 96.56846112, tar-aux-loss: 95.89293218
Epoch: [36 ] train-acc: 0.93589286, dom-acc: 0.60607143, val-acc: 0.92500000, val_loss: 0.26148456
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 289.18242550, sen-loss: 20.28631231, dom-loss: 78.50333238, src-aux-loss: 96.27094525, tar-aux-loss: 94.12183356
Epoch: [37 ] train-acc: 0.93714286, dom-acc: 0.57678571, val-acc: 0.92250000, val_loss: 0.27067348
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 289.65148926, sen-loss: 20.09306828, dom-loss: 78.48655051, src-aux-loss: 95.94703704, tar-aux-loss: 95.12483400
Epoch: [38 ] train-acc: 0.93714286, dom-acc: 0.61151786, val-acc: 0.92750000, val_loss: 0.26741877
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 288.44415784, sen-loss: 19.71711684, dom-loss: 78.39391148, src-aux-loss: 95.27479613, tar-aux-loss: 95.05833519
Epoch: [39 ] train-acc: 0.94017857, dom-acc: 0.60437500, val-acc: 0.92750000, val_loss: 0.26019135
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 287.24391222, sen-loss: 19.56168912, dom-loss: 78.36382782, src-aux-loss: 94.71312535, tar-aux-loss: 94.60527003
Epoch: [40 ] train-acc: 0.94089286, dom-acc: 0.60794643, val-acc: 0.92750000, val_loss: 0.26487994
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 286.03483343, sen-loss: 19.40024063, dom-loss: 78.36289746, src-aux-loss: 94.22011048, tar-aux-loss: 94.05158532
Epoch: [41 ] train-acc: 0.94196429, dom-acc: 0.62142857, val-acc: 0.92500000, val_loss: 0.26627213
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 286.28464746, sen-loss: 19.07616691, dom-loss: 78.33413583, src-aux-loss: 93.96148556, tar-aux-loss: 94.91286105
Epoch: [42 ] train-acc: 0.94017857, dom-acc: 0.61776786, val-acc: 0.92500000, val_loss: 0.25821105
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 283.73211598, sen-loss: 19.04313682, dom-loss: 78.18322045, src-aux-loss: 93.55440170, tar-aux-loss: 92.95135772
Epoch: [43 ] train-acc: 0.94214286, dom-acc: 0.58946429, val-acc: 0.92750000, val_loss: 0.25802428
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 283.37596703, sen-loss: 18.77763095, dom-loss: 78.19917434, src-aux-loss: 92.67467338, tar-aux-loss: 93.72448999
Epoch: [44 ] train-acc: 0.94339286, dom-acc: 0.61946429, val-acc: 0.92750000, val_loss: 0.26184380
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 284.43900514, sen-loss: 18.69455399, dom-loss: 78.12200940, src-aux-loss: 92.16793835, tar-aux-loss: 95.45450330
Epoch: [45 ] train-acc: 0.94446429, dom-acc: 0.62776786, val-acc: 0.91750000, val_loss: 0.27084723
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 282.67548227, sen-loss: 18.38612033, dom-loss: 78.05371541, src-aux-loss: 91.63513654, tar-aux-loss: 94.60050923
Epoch: [46 ] train-acc: 0.94482143, dom-acc: 0.61276786, val-acc: 0.91750000, val_loss: 0.27034083
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 280.33130956, sen-loss: 18.27982682, dom-loss: 77.98082167, src-aux-loss: 91.29264289, tar-aux-loss: 92.77801710
Epoch: [47 ] train-acc: 0.94553571, dom-acc: 0.61517857, val-acc: 0.92750000, val_loss: 0.26148549
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 281.77246952, sen-loss: 17.95551132, dom-loss: 77.97250724, src-aux-loss: 90.89687395, tar-aux-loss: 94.94757593
Epoch: [48 ] train-acc: 0.94482143, dom-acc: 0.63687500, val-acc: 0.93000000, val_loss: 0.25932339
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_electronics_HATN.ckpt
Best Epoch: [ 43] best val accuracy: 0.00000000 best val loss: 0.25802428
Testing accuracy: 0.89450000
./work/attentions/kitchen_electronics_train_HATN.txt
./work/attentions/kitchen_electronics_test_HATN.txt
loading data...
source domain:  kitchen target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 30180
vocab-size:  78115
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'pleased', 'wonderful', 'loves', 'fantastic', 'highly', 'amazing', 'awesome', 'satisfied', 'fabulous', 'nice', 'attractive', 'well', 'terrific', 'favorite', 'incredible', 'fast', 'quick', 'solid', 'simple', 'outstanding', 'useful', 'elegant', 'tough', 'easier', 'perfectly', 'decent', 'surprised', 'durable', 'far', 'impressed']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'useless', 'broke', 'horrible', 'poorly', 'bad', 'save', 'broken', 'defective', 'returned', 'flimsy', 'expensive', 'awful', 'impossible', 'wrong', 'dissapointed', 'dangerous', 'failed', 'overpriced', 'ruined', 'stopped', 'difficult', 'worthless', 'hard', 'frustrating', 'misleading', 'unhappy', 'better', 'quit', 'disapointed', 'flawed', 'worse', 'sad', 'return', 'worked', 'messy', 'weak', 'lousy', 'stuck', 'shattered', 'embarrassed', 'fell', 'overrated', 'shocked', 'ridiculous', 'waiting', 'unacceptable', 'leaky']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
78115
5600 400 6000 19856 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 419.94967866, sen-loss: 76.84569967, dom-loss: 76.90178233, src-aux-loss: 142.25935221, tar-aux-loss: 123.94284683
Epoch: [1  ] train-acc: 0.76571429, dom-acc: 0.83669643, val-acc: 0.76500000, val_loss: 0.63839531
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 389.83006620, sen-loss: 66.79017586, dom-loss: 73.56829351, src-aux-loss: 133.28646493, tar-aux-loss: 116.18513197
Epoch: [2  ] train-acc: 0.76750000, dom-acc: 0.81750000, val-acc: 0.78750000, val_loss: 0.54007667
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 367.54562521, sen-loss: 55.00320578, dom-loss: 72.36341697, src-aux-loss: 127.07595974, tar-aux-loss: 113.10304236
Epoch: [3  ] train-acc: 0.83875000, dom-acc: 0.71267857, val-acc: 0.83500000, val_loss: 0.42394704
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.84161520, sen-loss: 43.89117506, dom-loss: 72.58700001, src-aux-loss: 122.91012740, tar-aux-loss: 108.45331067
Epoch: [4  ] train-acc: 0.85892857, dom-acc: 0.61133929, val-acc: 0.86500000, val_loss: 0.35616302
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 341.07371879, sen-loss: 38.97223116, dom-loss: 73.88088185, src-aux-loss: 119.78050494, tar-aux-loss: 108.44010252
Epoch: [5  ] train-acc: 0.87767857, dom-acc: 0.59401786, val-acc: 0.88250000, val_loss: 0.33161831
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 334.35054564, sen-loss: 35.99441835, dom-loss: 74.61651134, src-aux-loss: 117.53479433, tar-aux-loss: 106.20482129
Epoch: [6  ] train-acc: 0.88535714, dom-acc: 0.56392857, val-acc: 0.88500000, val_loss: 0.31492585
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 331.17495966, sen-loss: 33.79521897, dom-loss: 76.18189472, src-aux-loss: 115.94636738, tar-aux-loss: 105.25147629
Epoch: [7  ] train-acc: 0.89035714, dom-acc: 0.52151786, val-acc: 0.90500000, val_loss: 0.30628556
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 328.59842134, sen-loss: 32.21276867, dom-loss: 77.04715008, src-aux-loss: 114.74108928, tar-aux-loss: 104.59741360
Epoch: [8  ] train-acc: 0.89607143, dom-acc: 0.47357143, val-acc: 0.89750000, val_loss: 0.30161023
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 326.17199039, sen-loss: 30.41105829, dom-loss: 78.22395426, src-aux-loss: 112.74024135, tar-aux-loss: 104.79673529
Epoch: [9  ] train-acc: 0.90125000, dom-acc: 0.43892857, val-acc: 0.90000000, val_loss: 0.29854783
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 322.41320419, sen-loss: 29.54187389, dom-loss: 78.41339749, src-aux-loss: 111.94819421, tar-aux-loss: 102.50974029
Epoch: [10 ] train-acc: 0.90196429, dom-acc: 0.42785714, val-acc: 0.91000000, val_loss: 0.29274508
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 319.78422952, sen-loss: 28.50334145, dom-loss: 79.08667827, src-aux-loss: 111.03897959, tar-aux-loss: 101.15523136
Epoch: [11 ] train-acc: 0.90482143, dom-acc: 0.40116071, val-acc: 0.91000000, val_loss: 0.28884375
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 319.08336473, sen-loss: 27.85735925, dom-loss: 79.05570406, src-aux-loss: 109.73850554, tar-aux-loss: 102.43179780
Epoch: [12 ] train-acc: 0.90482143, dom-acc: 0.41767857, val-acc: 0.91250000, val_loss: 0.28576621
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.83675742, sen-loss: 27.40648112, dom-loss: 79.00723141, src-aux-loss: 108.95609158, tar-aux-loss: 99.46695334
Epoch: [13 ] train-acc: 0.90982143, dom-acc: 0.41017857, val-acc: 0.89500000, val_loss: 0.30643904
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 314.40406179, sen-loss: 26.62469723, dom-loss: 78.86607790, src-aux-loss: 108.01562381, tar-aux-loss: 100.89766252
Epoch: [14 ] train-acc: 0.91071429, dom-acc: 0.42160714, val-acc: 0.90000000, val_loss: 0.29525882
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 313.31336880, sen-loss: 26.42150694, dom-loss: 78.64393967, src-aux-loss: 107.43749297, tar-aux-loss: 100.81042874
Epoch: [15 ] train-acc: 0.91196429, dom-acc: 0.43660714, val-acc: 0.91750000, val_loss: 0.28419805
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 310.81929421, sen-loss: 25.67384182, dom-loss: 78.45423126, src-aux-loss: 106.47133476, tar-aux-loss: 100.21988779
Epoch: [16 ] train-acc: 0.91642857, dom-acc: 0.45580357, val-acc: 0.91750000, val_loss: 0.27997679
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 308.48544741, sen-loss: 25.18182498, dom-loss: 78.12256765, src-aux-loss: 105.91954094, tar-aux-loss: 99.26151270
Epoch: [17 ] train-acc: 0.91571429, dom-acc: 0.47169643, val-acc: 0.90500000, val_loss: 0.28490201
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 307.98982549, sen-loss: 24.82347319, dom-loss: 77.76104915, src-aux-loss: 105.58729619, tar-aux-loss: 99.81800967
Epoch: [18 ] train-acc: 0.91785714, dom-acc: 0.49580357, val-acc: 0.91000000, val_loss: 0.28002483
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 305.05449843, sen-loss: 24.65740751, dom-loss: 77.63058364, src-aux-loss: 104.78771144, tar-aux-loss: 97.97879553
Epoch: [19 ] train-acc: 0.91910714, dom-acc: 0.49142857, val-acc: 0.91250000, val_loss: 0.27848440
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 304.75712013, sen-loss: 24.18451054, dom-loss: 77.35892111, src-aux-loss: 104.21196520, tar-aux-loss: 99.00172490
Epoch: [20 ] train-acc: 0.92125000, dom-acc: 0.50901786, val-acc: 0.91250000, val_loss: 0.27591339
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 303.36051464, sen-loss: 23.97295471, dom-loss: 77.18244642, src-aux-loss: 103.56990492, tar-aux-loss: 98.63521063
Epoch: [21 ] train-acc: 0.92214286, dom-acc: 0.50839286, val-acc: 0.91750000, val_loss: 0.27290988
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 301.94701195, sen-loss: 23.73646851, dom-loss: 77.01946944, src-aux-loss: 102.71990633, tar-aux-loss: 98.47116828
Epoch: [22 ] train-acc: 0.92357143, dom-acc: 0.50589286, val-acc: 0.91750000, val_loss: 0.27496442
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 299.52285123, sen-loss: 23.21183762, dom-loss: 76.90654087, src-aux-loss: 102.36366957, tar-aux-loss: 97.04080337
Epoch: [23 ] train-acc: 0.92553571, dom-acc: 0.51410714, val-acc: 0.91750000, val_loss: 0.27287894
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 301.98943567, sen-loss: 23.12880497, dom-loss: 77.03673768, src-aux-loss: 102.07386571, tar-aux-loss: 99.75002718
Epoch: [24 ] train-acc: 0.92589286, dom-acc: 0.51517857, val-acc: 0.92000000, val_loss: 0.27473205
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 298.67507839, sen-loss: 22.74723595, dom-loss: 77.11779690, src-aux-loss: 101.11742193, tar-aux-loss: 97.69262373
Epoch: [25 ] train-acc: 0.92517857, dom-acc: 0.52071429, val-acc: 0.92000000, val_loss: 0.27265197
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 297.75907159, sen-loss: 22.51948868, dom-loss: 76.93225527, src-aux-loss: 100.65495199, tar-aux-loss: 97.65237689
Epoch: [26 ] train-acc: 0.92839286, dom-acc: 0.49482143, val-acc: 0.92250000, val_loss: 0.27692869
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 295.21503997, sen-loss: 22.36870321, dom-loss: 77.21316940, src-aux-loss: 100.25019836, tar-aux-loss: 95.38296860
Epoch: [27 ] train-acc: 0.92821429, dom-acc: 0.50633929, val-acc: 0.91500000, val_loss: 0.27071387
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 296.24875903, sen-loss: 22.29490153, dom-loss: 76.93911427, src-aux-loss: 99.75151843, tar-aux-loss: 97.26322460
Epoch: [28 ] train-acc: 0.92910714, dom-acc: 0.48955357, val-acc: 0.92500000, val_loss: 0.27324635
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 294.58466315, sen-loss: 21.89618167, dom-loss: 77.38999373, src-aux-loss: 99.21896803, tar-aux-loss: 96.07951933
Epoch: [29 ] train-acc: 0.92964286, dom-acc: 0.48276786, val-acc: 0.92500000, val_loss: 0.26944140
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 293.58013105, sen-loss: 21.63940646, dom-loss: 77.58789039, src-aux-loss: 98.60721028, tar-aux-loss: 95.74562377
Epoch: [30 ] train-acc: 0.93125000, dom-acc: 0.48383929, val-acc: 0.92250000, val_loss: 0.27343187
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 293.25035214, sen-loss: 21.48862693, dom-loss: 77.57182175, src-aux-loss: 98.06998765, tar-aux-loss: 96.11991596
Epoch: [31 ] train-acc: 0.93160714, dom-acc: 0.47982143, val-acc: 0.92250000, val_loss: 0.27324948
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 293.88012981, sen-loss: 21.20742012, dom-loss: 77.71389568, src-aux-loss: 97.37763786, tar-aux-loss: 97.58117652
Epoch: [32 ] train-acc: 0.93357143, dom-acc: 0.47312500, val-acc: 0.92250000, val_loss: 0.27539414
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 291.38272572, sen-loss: 20.97728251, dom-loss: 77.67131275, src-aux-loss: 97.03894913, tar-aux-loss: 95.69518018
Epoch: [33 ] train-acc: 0.93285714, dom-acc: 0.45276786, val-acc: 0.92250000, val_loss: 0.27646276
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 290.66596794, sen-loss: 20.92489772, dom-loss: 77.87596363, src-aux-loss: 96.76721936, tar-aux-loss: 95.09788626
Epoch: [34 ] train-acc: 0.93357143, dom-acc: 0.47008929, val-acc: 0.92000000, val_loss: 0.27361909
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_video_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26944140
Testing accuracy: 0.84750000
./work/attentions/kitchen_video_train_HATN.txt
./work/attentions/kitchen_video_test_HATN.txt
loading data...
source domain:  video target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 9750
vocab-size:  98084
['best', 'great', 'funny', 'good', 'enjoyable', 'excellent', 'classic', 'amazing', 'better', 'favorite', 'love', 'greatest', 'brilliant', 'superb', 'perfect', 'hard', 'awesome', 'fantastic', 'beautifully', 'underrated', 'informative', 'loved', 'nice', 'hilarious', 'easy', 'poignant', 'recommended', 'funniest', 'solid', 'clever', 'terrific', 'rather', 'epic', 'immortal', 'beautiful', 'recommend', 'slow', 'believable', 'revisited', 'entertaining', 'sad', 'brilliantly', 'wonderful', 'incredible', 'simple', 'jokes', 'liked', 'riveting', 'fabulous', 'old', 'leavens', 'watchable', 'masterful', 'fine', 'heartbreaking', 'realistic', 'pleasant', 'scary', 'emotional', 'sweet', 'romantic', 'uplifting', 'funnier']
['worst', 'horrible', 'terrible', 'bad', 'poor', 'boring', 'awful', 'worse', 'disappointing', 'garbled', 'dissapointing', 'bother', 'harmful', 'wrong', 'wasted', 'incomplete', 'frustrating', 'dull', 'disappointed', 'ruined', 'unfunny', 'cuts', 'annoying', 'forgettable', 'unconvincing', 'low', 'dreadful', 'poorly', 'hated', 'uninspired', 'weak', 'unwatchable', 'needless', 'atrocious', 'ok', 'save', 'skip', 'contrived', 'uncooked', 'lousy', 'predictable', 'cheesy', 'pointless', 'mediocre', 'lackluster', 'waste', 'laughable', 'amusing', 'cut', 'sappy', 'unmemorable', 'pretentious', 'disgusting', 'vapid', 'befuddled', 'unbearable']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
98084
5600 400 6000 36180 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.73683977, sen-loss: 77.49388653, dom-loss: 79.45769709, src-aux-loss: 135.68252206, tar-aux-loss: 112.10273361
Epoch: [1  ] train-acc: 0.67982143, dom-acc: 0.59866071, val-acc: 0.69000000, val_loss: 0.65501088
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 377.55227852, sen-loss: 70.22277915, dom-loss: 77.15645558, src-aux-loss: 125.27788627, tar-aux-loss: 104.89515728
Epoch: [2  ] train-acc: 0.73535714, dom-acc: 0.75160714, val-acc: 0.70500000, val_loss: 0.59161997
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 351.77520251, sen-loss: 62.03230631, dom-loss: 76.06318170, src-aux-loss: 115.77679819, tar-aux-loss: 97.90291649
Epoch: [3  ] train-acc: 0.78553571, dom-acc: 0.78455357, val-acc: 0.80750000, val_loss: 0.51513249
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 330.80823708, sen-loss: 53.49436986, dom-loss: 75.50819105, src-aux-loss: 107.36973178, tar-aux-loss: 94.43594575
Epoch: [4  ] train-acc: 0.81660714, dom-acc: 0.79946429, val-acc: 0.80000000, val_loss: 0.44064182
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 316.76878428, sen-loss: 47.13374549, dom-loss: 75.17513907, src-aux-loss: 100.98728412, tar-aux-loss: 93.47261727
Epoch: [5  ] train-acc: 0.84375000, dom-acc: 0.79616071, val-acc: 0.83250000, val_loss: 0.39894250
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 306.88041115, sen-loss: 42.52639526, dom-loss: 74.92895412, src-aux-loss: 96.62084019, tar-aux-loss: 92.80422080
Epoch: [6  ] train-acc: 0.86339286, dom-acc: 0.78919643, val-acc: 0.86000000, val_loss: 0.36441931
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 295.99686670, sen-loss: 39.15705366, dom-loss: 74.94633955, src-aux-loss: 93.08376962, tar-aux-loss: 88.80970436
Epoch: [7  ] train-acc: 0.86857143, dom-acc: 0.78455357, val-acc: 0.86750000, val_loss: 0.35321313
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 292.41238308, sen-loss: 37.04586291, dom-loss: 74.87354124, src-aux-loss: 90.56060028, tar-aux-loss: 89.93237740
Epoch: [8  ] train-acc: 0.88589286, dom-acc: 0.77866071, val-acc: 0.88000000, val_loss: 0.32313871
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 285.96740294, sen-loss: 34.69154239, dom-loss: 75.18496960, src-aux-loss: 87.64123666, tar-aux-loss: 88.44965446
Epoch: [9  ] train-acc: 0.88714286, dom-acc: 0.77455357, val-acc: 0.88000000, val_loss: 0.31243038
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 281.01094627, sen-loss: 33.07730699, dom-loss: 75.23365945, src-aux-loss: 85.75122571, tar-aux-loss: 86.94875205
Epoch: [10 ] train-acc: 0.89392857, dom-acc: 0.76321429, val-acc: 0.89250000, val_loss: 0.29440835
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 279.24771690, sen-loss: 31.73975195, dom-loss: 75.39640719, src-aux-loss: 84.06942517, tar-aux-loss: 88.04213297
Epoch: [11 ] train-acc: 0.89982143, dom-acc: 0.74839286, val-acc: 0.90750000, val_loss: 0.28374460
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 276.76714659, sen-loss: 30.59893169, dom-loss: 75.63612455, src-aux-loss: 82.52507615, tar-aux-loss: 88.00701547
Epoch: [12 ] train-acc: 0.89625000, dom-acc: 0.72892857, val-acc: 0.87500000, val_loss: 0.30514005
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 274.94614196, sen-loss: 29.74806914, dom-loss: 75.81430268, src-aux-loss: 81.34423146, tar-aux-loss: 88.03953969
Epoch: [13 ] train-acc: 0.90375000, dom-acc: 0.71732143, val-acc: 0.90250000, val_loss: 0.29130724
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 273.35412073, sen-loss: 29.10677833, dom-loss: 76.17246145, src-aux-loss: 79.91045648, tar-aux-loss: 88.16442460
Epoch: [14 ] train-acc: 0.90714286, dom-acc: 0.69151786, val-acc: 0.90500000, val_loss: 0.27829614
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 269.32796979, sen-loss: 28.38697535, dom-loss: 76.15666217, src-aux-loss: 78.88806951, tar-aux-loss: 85.89626312
Epoch: [15 ] train-acc: 0.90875000, dom-acc: 0.69187500, val-acc: 0.89750000, val_loss: 0.29103982
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 268.10100341, sen-loss: 27.90555447, dom-loss: 76.34487420, src-aux-loss: 77.65964913, tar-aux-loss: 86.19092476
Epoch: [16 ] train-acc: 0.91267857, dom-acc: 0.68035714, val-acc: 0.90500000, val_loss: 0.27667439
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 264.72676516, sen-loss: 27.26799455, dom-loss: 76.57291347, src-aux-loss: 76.44251928, tar-aux-loss: 84.44333765
Epoch: [17 ] train-acc: 0.91500000, dom-acc: 0.67553571, val-acc: 0.91750000, val_loss: 0.27303797
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 264.15871024, sen-loss: 26.75972878, dom-loss: 76.86337930, src-aux-loss: 75.39280573, tar-aux-loss: 85.14279473
Epoch: [18 ] train-acc: 0.92000000, dom-acc: 0.66473214, val-acc: 0.91750000, val_loss: 0.27243701
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 263.17647696, sen-loss: 26.38306862, dom-loss: 77.00934160, src-aux-loss: 74.35487971, tar-aux-loss: 85.42918593
Epoch: [19 ] train-acc: 0.92214286, dom-acc: 0.65973214, val-acc: 0.91750000, val_loss: 0.27179065
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 260.54872096, sen-loss: 25.99846703, dom-loss: 77.17923325, src-aux-loss: 73.69235110, tar-aux-loss: 83.67866915
Epoch: [20 ] train-acc: 0.91625000, dom-acc: 0.65598214, val-acc: 0.90000000, val_loss: 0.27262524
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 260.71244276, sen-loss: 25.55717361, dom-loss: 77.13368386, src-aux-loss: 72.82219219, tar-aux-loss: 85.19939208
Epoch: [21 ] train-acc: 0.92375000, dom-acc: 0.64187500, val-acc: 0.91500000, val_loss: 0.27084917
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 260.11105895, sen-loss: 24.99452452, dom-loss: 77.38811916, src-aux-loss: 71.95471755, tar-aux-loss: 85.77369654
Epoch: [22 ] train-acc: 0.92660714, dom-acc: 0.62910714, val-acc: 0.90250000, val_loss: 0.27069789
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 256.62599075, sen-loss: 24.73339885, dom-loss: 77.64750129, src-aux-loss: 70.77662331, tar-aux-loss: 83.46846730
Epoch: [23 ] train-acc: 0.92607143, dom-acc: 0.62812500, val-acc: 0.91250000, val_loss: 0.27188608
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 254.81366777, sen-loss: 24.28631806, dom-loss: 77.72943842, src-aux-loss: 69.86398876, tar-aux-loss: 82.93392116
Epoch: [24 ] train-acc: 0.92750000, dom-acc: 0.61982143, val-acc: 0.91250000, val_loss: 0.27421966
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 255.22377086, sen-loss: 23.89795204, dom-loss: 77.66490179, src-aux-loss: 69.31156573, tar-aux-loss: 84.34935218
Epoch: [25 ] train-acc: 0.92910714, dom-acc: 0.61714286, val-acc: 0.91500000, val_loss: 0.26943207
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 255.06671047, sen-loss: 23.71160300, dom-loss: 77.95102400, src-aux-loss: 68.48058280, tar-aux-loss: 84.92350096
Epoch: [26 ] train-acc: 0.93000000, dom-acc: 0.61803571, val-acc: 0.91000000, val_loss: 0.26865381
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 252.15734065, sen-loss: 23.29651212, dom-loss: 77.92514920, src-aux-loss: 67.61840907, tar-aux-loss: 83.31727219
Epoch: [27 ] train-acc: 0.93089286, dom-acc: 0.60892857, val-acc: 0.90750000, val_loss: 0.26988301
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 252.68033719, sen-loss: 23.06130730, dom-loss: 77.95230120, src-aux-loss: 67.04308334, tar-aux-loss: 84.62364596
Epoch: [28 ] train-acc: 0.93250000, dom-acc: 0.61437500, val-acc: 0.90750000, val_loss: 0.27181855
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 249.74888039, sen-loss: 22.70444584, dom-loss: 77.98690408, src-aux-loss: 66.36665505, tar-aux-loss: 82.69087625
Epoch: [29 ] train-acc: 0.93446429, dom-acc: 0.60526786, val-acc: 0.90500000, val_loss: 0.26952106
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 247.68237317, sen-loss: 22.42545384, dom-loss: 77.81746614, src-aux-loss: 65.60027352, tar-aux-loss: 81.83917922
Epoch: [30 ] train-acc: 0.93660714, dom-acc: 0.58125000, val-acc: 0.91000000, val_loss: 0.27067959
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 247.99458230, sen-loss: 22.07212861, dom-loss: 78.12593299, src-aux-loss: 64.98950493, tar-aux-loss: 82.80701679
Epoch: [31 ] train-acc: 0.93625000, dom-acc: 0.60142857, val-acc: 0.90500000, val_loss: 0.26999810
---------------------------------------------------

Successfully load model from save path: ./work/models/video_books_HATN.ckpt
Best Epoch: [ 26] best val accuracy: 0.00000000 best val loss: 0.26865381
Testing accuracy: 0.86883333
./work/attentions/video_books_train_HATN.txt
./work/attentions/video_books_test_HATN.txt
loading data...
source domain:  video target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 11843
vocab-size:  91852
['best', 'funny', 'great', 'good', 'classic', 'enjoyable', 'love', 'amazing', 'favorite', 'excellent', 'awesome', 'better', 'loved', 'underrated', 'superb', 'brilliant', 'greatest', 'hard', 'perfect', 'beautifully', 'funniest', 'fantastic', 'hilarious', 'poignant', 'entertaining', 'easy', 'nice', 'informative', 'watching', 'terrific', 'solid', 'epic', 'clever', 'believable', 'brilliantly', 'beautiful', 'performed', 'riveting', 'wonderful', 'jokes', 'watchable', 'romantic', 'incredible', 'scary', 'fabulous', 'liked', 'fine', 'masterful', 'emotional', 'magnificent', 'uplifting', 'realistic', 'funnier', 'old']
['worst', 'horrible', 'terrible', 'bad', 'poor', 'boring', 'disappointing', 'worse', 'awful', 'garbled', 'bother', 'dissapointing', 'harmful', 'average', 'wrong', 'wasted', 'dull', 'ruined', 'unfunny', 'hated', 'cuts', 'annoying', 'disappointed', 'forgettable', 'unwatchable', 'low', 'poorly', 'laughable', 'dreadful', 'needless', 'skip', 'uninspired', 'unconvincing', 'atrocious', 'ok', 'contrived', 'uncooked', 'lousy', 'dissapointed', 'pointless', 'predictable', 'weak', 'cheesy', 'mediocre', 'lackluster', 'amusing', 'cut', 'sappy', 'unmemorable', 'pretentious', 'disgusting', 'vapid', 'lame', 'unbearable', 'save']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
91852
5600 400 6000 36180 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 405.24652696, sen-loss: 77.53578264, dom-loss: 79.33402556, src-aux-loss: 135.77370751, tar-aux-loss: 112.60301107
Epoch: [1  ] train-acc: 0.67089286, dom-acc: 0.44267857, val-acc: 0.68500000, val_loss: 0.65563440
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 378.55518770, sen-loss: 70.33261317, dom-loss: 78.76445544, src-aux-loss: 124.47539508, tar-aux-loss: 104.98272276
Epoch: [2  ] train-acc: 0.74267857, dom-acc: 0.42616071, val-acc: 0.71750000, val_loss: 0.59142447
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 354.99926615, sen-loss: 62.13237885, dom-loss: 78.62170702, src-aux-loss: 115.59566915, tar-aux-loss: 98.64950961
Epoch: [3  ] train-acc: 0.77517857, dom-acc: 0.41517857, val-acc: 0.79500000, val_loss: 0.51799607
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 335.40511036, sen-loss: 53.31803986, dom-loss: 78.63475084, src-aux-loss: 107.16249746, tar-aux-loss: 96.28982019
Epoch: [4  ] train-acc: 0.82089286, dom-acc: 0.40883929, val-acc: 0.79500000, val_loss: 0.43453863
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 319.44414902, sen-loss: 46.72077054, dom-loss: 78.59738916, src-aux-loss: 101.18116957, tar-aux-loss: 92.94482005
Epoch: [5  ] train-acc: 0.84321429, dom-acc: 0.43562500, val-acc: 0.84000000, val_loss: 0.39475900
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 310.18608785, sen-loss: 42.00453702, dom-loss: 78.48897558, src-aux-loss: 97.24085963, tar-aux-loss: 92.45171261
Epoch: [6  ] train-acc: 0.86053571, dom-acc: 0.44250000, val-acc: 0.86250000, val_loss: 0.35492170
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 302.23141623, sen-loss: 38.91645963, dom-loss: 78.49062508, src-aux-loss: 94.32203013, tar-aux-loss: 90.50229925
Epoch: [7  ] train-acc: 0.86642857, dom-acc: 0.46026786, val-acc: 0.86250000, val_loss: 0.35264316
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 296.63738680, sen-loss: 36.80359936, dom-loss: 78.42630720, src-aux-loss: 91.93204820, tar-aux-loss: 89.47543365
Epoch: [8  ] train-acc: 0.88285714, dom-acc: 0.49276786, val-acc: 0.88500000, val_loss: 0.31675872
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 291.51502109, sen-loss: 34.62016456, dom-loss: 78.33797312, src-aux-loss: 89.07018858, tar-aux-loss: 89.48669481
Epoch: [9  ] train-acc: 0.88928571, dom-acc: 0.54848214, val-acc: 0.88500000, val_loss: 0.30321008
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 286.57023668, sen-loss: 33.01685126, dom-loss: 78.39681381, src-aux-loss: 87.31098890, tar-aux-loss: 87.84558392
Epoch: [10 ] train-acc: 0.89232143, dom-acc: 0.52857143, val-acc: 0.89500000, val_loss: 0.29053509
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 283.61312270, sen-loss: 31.85830657, dom-loss: 78.33148688, src-aux-loss: 85.65601224, tar-aux-loss: 87.76731730
Epoch: [11 ] train-acc: 0.89982143, dom-acc: 0.53553571, val-acc: 0.89250000, val_loss: 0.28514728
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 280.53923249, sen-loss: 30.64699529, dom-loss: 78.20817262, src-aux-loss: 84.19827402, tar-aux-loss: 87.48578990
Epoch: [12 ] train-acc: 0.89232143, dom-acc: 0.54910714, val-acc: 0.87750000, val_loss: 0.30741975
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 278.72531509, sen-loss: 29.83880652, dom-loss: 78.17783606, src-aux-loss: 82.92416352, tar-aux-loss: 87.78450918
Epoch: [13 ] train-acc: 0.90053571, dom-acc: 0.55973214, val-acc: 0.88500000, val_loss: 0.29525593
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 276.83289576, sen-loss: 29.20015027, dom-loss: 78.19372046, src-aux-loss: 81.60632372, tar-aux-loss: 87.83270144
Epoch: [14 ] train-acc: 0.90910714, dom-acc: 0.48901786, val-acc: 0.90000000, val_loss: 0.28081396
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 274.02076530, sen-loss: 28.55178292, dom-loss: 78.14733338, src-aux-loss: 80.39307857, tar-aux-loss: 86.92857087
Epoch: [15 ] train-acc: 0.90839286, dom-acc: 0.58508929, val-acc: 0.89750000, val_loss: 0.28896356
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 271.45556307, sen-loss: 28.10055754, dom-loss: 78.07789910, src-aux-loss: 79.09104693, tar-aux-loss: 86.18605888
Epoch: [16 ] train-acc: 0.91142857, dom-acc: 0.61125000, val-acc: 0.90000000, val_loss: 0.27677041
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 268.15743101, sen-loss: 27.41716406, dom-loss: 78.06181741, src-aux-loss: 77.81719512, tar-aux-loss: 84.86125463
Epoch: [17 ] train-acc: 0.91482143, dom-acc: 0.61017857, val-acc: 0.90750000, val_loss: 0.27558100
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 265.77797771, sen-loss: 26.95869085, dom-loss: 78.06503540, src-aux-loss: 76.76974413, tar-aux-loss: 83.98450691
Epoch: [18 ] train-acc: 0.91839286, dom-acc: 0.61267857, val-acc: 0.91000000, val_loss: 0.27375180
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 265.51729035, sen-loss: 26.48962379, dom-loss: 78.00208306, src-aux-loss: 75.85211471, tar-aux-loss: 85.17346978
Epoch: [19 ] train-acc: 0.91946429, dom-acc: 0.57035714, val-acc: 0.90500000, val_loss: 0.27146810
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 263.18429470, sen-loss: 26.22009887, dom-loss: 77.97490466, src-aux-loss: 74.98056981, tar-aux-loss: 84.00872105
Epoch: [20 ] train-acc: 0.91500000, dom-acc: 0.54089286, val-acc: 0.89500000, val_loss: 0.27535701
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 262.69859493, sen-loss: 25.70572666, dom-loss: 77.89274919, src-aux-loss: 74.05681142, tar-aux-loss: 85.04330730
Epoch: [21 ] train-acc: 0.92053571, dom-acc: 0.64410714, val-acc: 0.90750000, val_loss: 0.27517191
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 261.38578212, sen-loss: 25.16120141, dom-loss: 77.90677333, src-aux-loss: 73.49100986, tar-aux-loss: 84.82679862
Epoch: [22 ] train-acc: 0.92160714, dom-acc: 0.59714286, val-acc: 0.90000000, val_loss: 0.27112108
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 258.42190576, sen-loss: 24.90753949, dom-loss: 77.93533707, src-aux-loss: 72.27782705, tar-aux-loss: 83.30120182
Epoch: [23 ] train-acc: 0.92517857, dom-acc: 0.63723214, val-acc: 0.91500000, val_loss: 0.27200875
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 256.16712117, sen-loss: 24.49396204, dom-loss: 77.89172596, src-aux-loss: 71.26026416, tar-aux-loss: 82.52117050
Epoch: [24 ] train-acc: 0.92464286, dom-acc: 0.62437500, val-acc: 0.91750000, val_loss: 0.27281788
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 256.81783259, sen-loss: 24.07669985, dom-loss: 77.88741899, src-aux-loss: 70.68835822, tar-aux-loss: 84.16535547
Epoch: [25 ] train-acc: 0.92642857, dom-acc: 0.58776786, val-acc: 0.91500000, val_loss: 0.27042678
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 256.08583605, sen-loss: 23.93380474, dom-loss: 77.85317731, src-aux-loss: 69.71051371, tar-aux-loss: 84.58833981
Epoch: [26 ] train-acc: 0.92821429, dom-acc: 0.59535714, val-acc: 0.91250000, val_loss: 0.27378285
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 253.56004846, sen-loss: 23.58701146, dom-loss: 77.83303142, src-aux-loss: 68.87435168, tar-aux-loss: 83.26565450
Epoch: [27 ] train-acc: 0.92875000, dom-acc: 0.61553571, val-acc: 0.91000000, val_loss: 0.26998106
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 253.03072548, sen-loss: 23.28811159, dom-loss: 77.79068762, src-aux-loss: 68.28203920, tar-aux-loss: 83.66988730
Epoch: [28 ] train-acc: 0.93142857, dom-acc: 0.62919643, val-acc: 0.91000000, val_loss: 0.27339464
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 249.69170690, sen-loss: 22.88918138, dom-loss: 77.84516662, src-aux-loss: 67.48675260, tar-aux-loss: 81.47060639
Epoch: [29 ] train-acc: 0.93303571, dom-acc: 0.59241071, val-acc: 0.90750000, val_loss: 0.27111846
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 248.43094623, sen-loss: 22.56544235, dom-loss: 77.77164656, src-aux-loss: 66.57620665, tar-aux-loss: 81.51765001
Epoch: [30 ] train-acc: 0.93517857, dom-acc: 0.54339286, val-acc: 0.91000000, val_loss: 0.27269804
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 248.89709067, sen-loss: 22.20808638, dom-loss: 77.79225290, src-aux-loss: 65.97029471, tar-aux-loss: 82.92645812
Epoch: [31 ] train-acc: 0.93446429, dom-acc: 0.59955357, val-acc: 0.91000000, val_loss: 0.27390394
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 247.21613753, sen-loss: 21.83277994, dom-loss: 77.73720729, src-aux-loss: 65.23168394, tar-aux-loss: 82.41446710
Epoch: [32 ] train-acc: 0.93571429, dom-acc: 0.64133929, val-acc: 0.90500000, val_loss: 0.27243170
---------------------------------------------------

Successfully load model from save path: ./work/models/video_dvd_HATN.ckpt
Best Epoch: [ 27] best val accuracy: 0.00000000 best val loss: 0.26998106
Testing accuracy: 0.87633333
./work/attentions/video_dvd_train_HATN.txt
./work/attentions/video_dvd_test_HATN.txt
loading data...
source domain:  video target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 17009
vocab-size:  83059
['best', 'great', 'funny', 'good', 'enjoyable', 'classic', 'excellent', 'better', 'amazing', 'awesome', 'love', 'favorite', 'nice', 'perfect', 'superb', 'fantastic', 'greatest', 'underrated', 'loved', 'brilliant', 'beautifully', 'hilarious', 'funniest', 'solid', 'terrific', 'easy', 'clever', 'watching', 'epic', 'fine', 'un', 'beautiful', 'decent', 'performed', 'poignant', 'entertaining', 'brilliantly', 'pretty', 'wonderful', 'incredible', 'sad', 'jokes', 'liked', 'believable', 'perfectly', 'scary', 'fabulous', 'riveting', 'recommend', 'watchable', 'masterful', 'old', 'sweet', 'romantic', 'realistic', 'simple', 'funnier']
['worst', 'horrible', 'terrible', 'bad', 'boring', 'poor', 'disappointing', 'awful', 'worse', 'garbled', 'dissapointing', 'bother', 'hard', 'average', 'wrong', 'wasted', 'ruined', 'dull', 'unfunny', 'annoying', 'cuts', 'disappointed', 'hated', 'forgettable', 'unwatchable', 'poorly', 'low', 'dreadful', 'weak', 'lousy', 'needless', 'uninspired', 'atrocious', 'ok', 'save', 'laughable', 'skip', 'contrived', 'uncooked', 'predictable', 'cheesy', 'mediocre', 'lackluster', 'amusing', 'dissapointed', 'cut', 'sappy', 'unmemorable', 'unconvincing', 'dreary', 'disgusting', 'vapid', 'lame']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
83059
5600 400 6000 36180 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.75847840, sen-loss: 77.51501787, dom-loss: 78.81957608, src-aux-loss: 134.83251941, tar-aux-loss: 113.59136569
Epoch: [1  ] train-acc: 0.67625000, dom-acc: 0.75232143, val-acc: 0.68500000, val_loss: 0.65539992
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 374.92695975, sen-loss: 70.50507796, dom-loss: 74.59957540, src-aux-loss: 123.65584213, tar-aux-loss: 106.16646522
Epoch: [2  ] train-acc: 0.73857143, dom-acc: 0.80669643, val-acc: 0.70750000, val_loss: 0.59570754
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 351.23303890, sen-loss: 62.64616010, dom-loss: 72.34320438, src-aux-loss: 114.97318077, tar-aux-loss: 101.27049369
Epoch: [3  ] train-acc: 0.78000000, dom-acc: 0.75687500, val-acc: 0.78750000, val_loss: 0.52004403
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 331.50700426, sen-loss: 53.98790398, dom-loss: 72.22256112, src-aux-loss: 107.50392944, tar-aux-loss: 97.79260957
Epoch: [4  ] train-acc: 0.82089286, dom-acc: 0.65705357, val-acc: 0.80250000, val_loss: 0.44215029
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 319.07543540, sen-loss: 47.42397210, dom-loss: 72.83577871, src-aux-loss: 102.47977638, tar-aux-loss: 96.33590782
Epoch: [5  ] train-acc: 0.84160714, dom-acc: 0.59330357, val-acc: 0.83500000, val_loss: 0.40485641
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 312.13928008, sen-loss: 42.72869125, dom-loss: 74.17327011, src-aux-loss: 99.02937269, tar-aux-loss: 96.20794779
Epoch: [6  ] train-acc: 0.86750000, dom-acc: 0.60901786, val-acc: 0.85250000, val_loss: 0.36720598
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 304.63441753, sen-loss: 38.87544753, dom-loss: 75.10904932, src-aux-loss: 95.86168522, tar-aux-loss: 94.78823483
Epoch: [7  ] train-acc: 0.87089286, dom-acc: 0.57276786, val-acc: 0.86750000, val_loss: 0.35214585
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 299.95169210, sen-loss: 36.37193350, dom-loss: 76.54586518, src-aux-loss: 93.54395378, tar-aux-loss: 93.48994070
Epoch: [8  ] train-acc: 0.88625000, dom-acc: 0.54803571, val-acc: 0.88000000, val_loss: 0.32151711
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 296.31283331, sen-loss: 34.06917737, dom-loss: 77.93567526, src-aux-loss: 91.55638629, tar-aux-loss: 92.75159347
Epoch: [9  ] train-acc: 0.89160714, dom-acc: 0.48616071, val-acc: 0.88500000, val_loss: 0.31166506
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 293.53013015, sen-loss: 32.46958345, dom-loss: 78.91796154, src-aux-loss: 89.94853061, tar-aux-loss: 92.19405437
Epoch: [10 ] train-acc: 0.89482143, dom-acc: 0.42312500, val-acc: 0.88750000, val_loss: 0.29744434
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 290.59801054, sen-loss: 31.37487544, dom-loss: 79.61556947, src-aux-loss: 88.15170097, tar-aux-loss: 91.45586419
Epoch: [11 ] train-acc: 0.89607143, dom-acc: 0.42964286, val-acc: 0.88250000, val_loss: 0.29054072
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 288.32815742, sen-loss: 30.38579695, dom-loss: 79.80440551, src-aux-loss: 86.34877485, tar-aux-loss: 91.78917885
Epoch: [12 ] train-acc: 0.90107143, dom-acc: 0.42383929, val-acc: 0.88000000, val_loss: 0.30805203
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 286.41278434, sen-loss: 29.50689770, dom-loss: 79.70832747, src-aux-loss: 85.38053411, tar-aux-loss: 91.81702322
Epoch: [13 ] train-acc: 0.90428571, dom-acc: 0.43205357, val-acc: 0.88250000, val_loss: 0.30052969
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 283.99419236, sen-loss: 28.92576893, dom-loss: 79.26689214, src-aux-loss: 83.51484716, tar-aux-loss: 92.28668481
Epoch: [14 ] train-acc: 0.91035714, dom-acc: 0.43758929, val-acc: 0.89750000, val_loss: 0.28126845
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 280.33944941, sen-loss: 28.24674567, dom-loss: 78.70020515, src-aux-loss: 82.85330826, tar-aux-loss: 90.53919047
Epoch: [15 ] train-acc: 0.91000000, dom-acc: 0.47687500, val-acc: 0.88750000, val_loss: 0.29394034
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 276.73036861, sen-loss: 27.80529124, dom-loss: 78.33758789, src-aux-loss: 81.23986411, tar-aux-loss: 89.34762371
Epoch: [16 ] train-acc: 0.91321429, dom-acc: 0.49687500, val-acc: 0.90500000, val_loss: 0.27932948
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 275.84383082, sen-loss: 27.28073682, dom-loss: 78.01451385, src-aux-loss: 80.09003055, tar-aux-loss: 90.45855027
Epoch: [17 ] train-acc: 0.91482143, dom-acc: 0.51187500, val-acc: 0.90250000, val_loss: 0.28317612
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 272.63241386, sen-loss: 26.74241167, dom-loss: 77.78320527, src-aux-loss: 78.97029617, tar-aux-loss: 89.13650078
Epoch: [18 ] train-acc: 0.91982143, dom-acc: 0.52276786, val-acc: 0.91250000, val_loss: 0.27378714
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 272.17793918, sen-loss: 26.23860029, dom-loss: 77.48406559, src-aux-loss: 78.24193290, tar-aux-loss: 90.21333915
Epoch: [19 ] train-acc: 0.92125000, dom-acc: 0.51607143, val-acc: 0.91250000, val_loss: 0.27403072
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 269.75181818, sen-loss: 25.95331146, dom-loss: 77.38583875, src-aux-loss: 77.30475011, tar-aux-loss: 89.10791743
Epoch: [20 ] train-acc: 0.92142857, dom-acc: 0.52714286, val-acc: 0.90000000, val_loss: 0.27240220
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 267.51901793, sen-loss: 25.47711230, dom-loss: 77.46685016, src-aux-loss: 76.20759878, tar-aux-loss: 88.36745656
Epoch: [21 ] train-acc: 0.92089286, dom-acc: 0.51107143, val-acc: 0.91250000, val_loss: 0.27736673
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 267.76186943, sen-loss: 24.93386307, dom-loss: 77.42889118, src-aux-loss: 75.56259197, tar-aux-loss: 89.83652186
Epoch: [22 ] train-acc: 0.92660714, dom-acc: 0.48464286, val-acc: 0.89750000, val_loss: 0.27191699
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 265.13726890, sen-loss: 24.69032050, dom-loss: 77.78630757, src-aux-loss: 74.52996609, tar-aux-loss: 88.13067484
Epoch: [23 ] train-acc: 0.92392857, dom-acc: 0.49633929, val-acc: 0.91250000, val_loss: 0.27366191
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 263.14450431, sen-loss: 24.28291959, dom-loss: 77.77533996, src-aux-loss: 73.51299885, tar-aux-loss: 87.57324523
Epoch: [24 ] train-acc: 0.92535714, dom-acc: 0.46866071, val-acc: 0.91500000, val_loss: 0.27379733
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 262.36387575, sen-loss: 23.93966785, dom-loss: 77.84861743, src-aux-loss: 72.99455997, tar-aux-loss: 87.58102995
Epoch: [25 ] train-acc: 0.93017857, dom-acc: 0.45392857, val-acc: 0.91000000, val_loss: 0.26835945
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 263.21614957, sen-loss: 23.69109831, dom-loss: 77.92239344, src-aux-loss: 72.13134378, tar-aux-loss: 89.47131628
Epoch: [26 ] train-acc: 0.93089286, dom-acc: 0.44044643, val-acc: 0.91500000, val_loss: 0.26908818
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 261.67047381, sen-loss: 23.35171213, dom-loss: 77.98073810, src-aux-loss: 71.38585168, tar-aux-loss: 88.95216984
Epoch: [27 ] train-acc: 0.93000000, dom-acc: 0.43178571, val-acc: 0.91000000, val_loss: 0.26990482
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 260.30828929, sen-loss: 23.08721063, dom-loss: 78.18310696, src-aux-loss: 70.73964459, tar-aux-loss: 88.29832786
Epoch: [28 ] train-acc: 0.92928571, dom-acc: 0.43410714, val-acc: 0.90750000, val_loss: 0.27781397
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 257.88344538, sen-loss: 22.71063846, dom-loss: 78.02218139, src-aux-loss: 70.00245634, tar-aux-loss: 87.14816815
Epoch: [29 ] train-acc: 0.93267857, dom-acc: 0.43000000, val-acc: 0.90500000, val_loss: 0.26943052
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 254.66498280, sen-loss: 22.43922015, dom-loss: 77.80791783, src-aux-loss: 69.12834826, tar-aux-loss: 85.28949618
Epoch: [30 ] train-acc: 0.93446429, dom-acc: 0.42455357, val-acc: 0.91250000, val_loss: 0.27215549
---------------------------------------------------

Successfully load model from save path: ./work/models/video_electronics_HATN.ckpt
Best Epoch: [ 25] best val accuracy: 0.00000000 best val loss: 0.26835945
Testing accuracy: 0.84933333
./work/attentions/video_electronics_train_HATN.txt
./work/attentions/video_electronics_test_HATN.txt
loading data...
source domain:  video target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 13856
vocab-size:  78115
['best', 'funny', 'great', 'good', 'classic', 'enjoyable', 'love', 'excellent', 'amazing', 'favorite', 'better', 'nice', 'awesome', 'superb', 'perfect', 'underrated', 'greatest', 'loved', 'brilliant', 'hard', 'beautifully', 'fantastic', 'hilarious', 'funniest', 'terrific', 'easy', 'watching', 'solid', 'entertaining', 'epic', 'beautiful', 'poignant', 'fine', 'un', 'brilliantly', 'clever', 'performed', 'believable', 'wonderful', 'sad', 'jokes', 'liked', 'riveting', 'incredible', 'scary', 'fabulous', 'watchable', 'masterful', 'sweet', 'recommend', 'romantic', 'perfectly', 'heartbreaking', 'simple', 'emotional', 'funnier', 'old']
['worst', 'horrible', 'terrible', 'bad', 'poor', 'boring', 'disappointing', 'worse', 'awful', 'garbled', 'dissapointing', 'bother', 'harmful', 'average', 'wrong', 'wasted', 'dull', 'ruined', 'unfunny', 'annoying', 'cuts', 'hated', 'disappointed', 'forgettable', 'unwatchable', 'dreadful', 'poorly', 'low', 'laughable', 'uninspired', 'weak', 'lousy', 'needless', 'skip', 'unconvincing', 'atrocious', 'ok', 'lame', 'contrived', 'uncooked', 'predictable', 'cheesy', 'mediocre', 'lackluster', 'amusing', 'dissapointed', 'cut', 'sappy', 'unmemorable', 'disgusting', 'vapid', 'save']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
78115
5600 400 6000 36180 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 399.71223021, sen-loss: 77.50138086, dom-loss: 78.66552478, src-aux-loss: 136.31486750, tar-aux-loss: 107.23045665
Epoch: [1  ] train-acc: 0.67392857, dom-acc: 0.73964286, val-acc: 0.68250000, val_loss: 0.65550709
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 370.73491192, sen-loss: 70.46665305, dom-loss: 75.01947713, src-aux-loss: 125.21818990, tar-aux-loss: 100.03059220
Epoch: [2  ] train-acc: 0.73553571, dom-acc: 0.82687500, val-acc: 0.71250000, val_loss: 0.59492195
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 345.11001849, sen-loss: 62.41253835, dom-loss: 73.44221604, src-aux-loss: 115.92911470, tar-aux-loss: 93.32614893
Epoch: [3  ] train-acc: 0.78357143, dom-acc: 0.80419643, val-acc: 0.80250000, val_loss: 0.51832557
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 325.74380112, sen-loss: 53.69216049, dom-loss: 73.51163059, src-aux-loss: 107.68667662, tar-aux-loss: 90.85333240
Epoch: [4  ] train-acc: 0.82214286, dom-acc: 0.77312500, val-acc: 0.80000000, val_loss: 0.44115791
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 311.92611599, sen-loss: 46.95722449, dom-loss: 74.17956614, src-aux-loss: 102.05607951, tar-aux-loss: 88.73324579
Epoch: [5  ] train-acc: 0.84625000, dom-acc: 0.76723214, val-acc: 0.84250000, val_loss: 0.39344206
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 304.71973348, sen-loss: 42.26434880, dom-loss: 74.81324667, src-aux-loss: 98.15959924, tar-aux-loss: 89.48253781
Epoch: [6  ] train-acc: 0.86053571, dom-acc: 0.75348214, val-acc: 0.85750000, val_loss: 0.35811579
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 295.60557556, sen-loss: 38.78030339, dom-loss: 75.96719420, src-aux-loss: 94.97644007, tar-aux-loss: 85.88163853
Epoch: [7  ] train-acc: 0.86803571, dom-acc: 0.70571429, val-acc: 0.86000000, val_loss: 0.34707278
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 293.29577327, sen-loss: 36.47838458, dom-loss: 77.01593047, src-aux-loss: 92.78308761, tar-aux-loss: 87.01837093
Epoch: [8  ] train-acc: 0.88428571, dom-acc: 0.65741071, val-acc: 0.87750000, val_loss: 0.32064807
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 286.39096546, sen-loss: 34.33851080, dom-loss: 77.92580122, src-aux-loss: 89.94639844, tar-aux-loss: 84.18025357
Epoch: [9  ] train-acc: 0.88892857, dom-acc: 0.62562500, val-acc: 0.88000000, val_loss: 0.30772048
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 284.52355790, sen-loss: 32.72056997, dom-loss: 78.92203516, src-aux-loss: 88.41098005, tar-aux-loss: 84.46997339
Epoch: [10 ] train-acc: 0.89517857, dom-acc: 0.59767857, val-acc: 0.89000000, val_loss: 0.29457876
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 281.93721771, sen-loss: 31.60012926, dom-loss: 79.17174637, src-aux-loss: 86.59203291, tar-aux-loss: 84.57330847
Epoch: [11 ] train-acc: 0.90053571, dom-acc: 0.57991071, val-acc: 0.88500000, val_loss: 0.28537378
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 278.95077896, sen-loss: 30.51133456, dom-loss: 79.13699698, src-aux-loss: 84.87192571, tar-aux-loss: 84.43052262
Epoch: [12 ] train-acc: 0.89464286, dom-acc: 0.59250000, val-acc: 0.87750000, val_loss: 0.30851650
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 276.62069011, sen-loss: 29.72320196, dom-loss: 79.04809481, src-aux-loss: 83.61484164, tar-aux-loss: 84.23455131
Epoch: [13 ] train-acc: 0.90714286, dom-acc: 0.58589286, val-acc: 0.89000000, val_loss: 0.28850445
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 273.70217133, sen-loss: 29.03393093, dom-loss: 78.79909307, src-aux-loss: 82.06915861, tar-aux-loss: 83.79998747
Epoch: [14 ] train-acc: 0.91017857, dom-acc: 0.59401786, val-acc: 0.90000000, val_loss: 0.27943954
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 272.15062273, sen-loss: 28.44058587, dom-loss: 78.14032298, src-aux-loss: 81.41684657, tar-aux-loss: 84.15286446
Epoch: [15 ] train-acc: 0.91160714, dom-acc: 0.62366071, val-acc: 0.89500000, val_loss: 0.28480738
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 268.16512918, sen-loss: 28.01275407, dom-loss: 77.99305004, src-aux-loss: 79.84547102, tar-aux-loss: 82.31385422
Epoch: [16 ] train-acc: 0.91500000, dom-acc: 0.62017857, val-acc: 0.90250000, val_loss: 0.27983159
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 265.41523004, sen-loss: 27.25993980, dom-loss: 77.74345332, src-aux-loss: 78.71508443, tar-aux-loss: 81.69675124
Epoch: [17 ] train-acc: 0.91607143, dom-acc: 0.63839286, val-acc: 0.90500000, val_loss: 0.27524102
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 262.33398890, sen-loss: 26.84889515, dom-loss: 77.68469298, src-aux-loss: 77.44732541, tar-aux-loss: 80.35307634
Epoch: [18 ] train-acc: 0.91910714, dom-acc: 0.63160714, val-acc: 0.91000000, val_loss: 0.27189744
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 262.27746797, sen-loss: 26.42866942, dom-loss: 77.62180191, src-aux-loss: 76.62521008, tar-aux-loss: 81.60178620
Epoch: [19 ] train-acc: 0.92089286, dom-acc: 0.62660714, val-acc: 0.91500000, val_loss: 0.27195582
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 262.43505144, sen-loss: 26.10048650, dom-loss: 77.56556386, src-aux-loss: 75.77304214, tar-aux-loss: 82.99595821
Epoch: [20 ] train-acc: 0.91839286, dom-acc: 0.61982143, val-acc: 0.89750000, val_loss: 0.27182877
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 260.00099850, sen-loss: 25.71150087, dom-loss: 77.82382613, src-aux-loss: 74.94352585, tar-aux-loss: 81.52214587
Epoch: [21 ] train-acc: 0.92232143, dom-acc: 0.61580357, val-acc: 0.92000000, val_loss: 0.27397051
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 258.45124018, sen-loss: 25.08778369, dom-loss: 78.02726960, src-aux-loss: 74.47179857, tar-aux-loss: 80.86438853
Epoch: [22 ] train-acc: 0.92321429, dom-acc: 0.58955357, val-acc: 0.90250000, val_loss: 0.26942304
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 255.18239844, sen-loss: 24.89714679, dom-loss: 78.28181618, src-aux-loss: 73.01099828, tar-aux-loss: 78.99243748
Epoch: [23 ] train-acc: 0.92464286, dom-acc: 0.58901786, val-acc: 0.91500000, val_loss: 0.27457067
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 256.80563831, sen-loss: 24.47887546, dom-loss: 78.48253208, src-aux-loss: 72.10424632, tar-aux-loss: 81.73998365
Epoch: [24 ] train-acc: 0.92625000, dom-acc: 0.55776786, val-acc: 0.92000000, val_loss: 0.27063012
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 254.08540988, sen-loss: 24.10067480, dom-loss: 78.56707442, src-aux-loss: 71.58788791, tar-aux-loss: 79.82977271
Epoch: [25 ] train-acc: 0.92857143, dom-acc: 0.55598214, val-acc: 0.91250000, val_loss: 0.26768866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 255.92933071, sen-loss: 23.86445389, dom-loss: 78.61293632, src-aux-loss: 70.79145890, tar-aux-loss: 82.66048235
Epoch: [26 ] train-acc: 0.92964286, dom-acc: 0.54312500, val-acc: 0.91500000, val_loss: 0.26686263
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 251.86062527, sen-loss: 23.49448306, dom-loss: 78.71876591, src-aux-loss: 69.75722203, tar-aux-loss: 79.89015442
Epoch: [27 ] train-acc: 0.93053571, dom-acc: 0.52741071, val-acc: 0.90250000, val_loss: 0.26725882
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 251.74884403, sen-loss: 23.27641438, dom-loss: 78.55906671, src-aux-loss: 69.07577023, tar-aux-loss: 80.83759195
Epoch: [28 ] train-acc: 0.92875000, dom-acc: 0.53357143, val-acc: 0.91250000, val_loss: 0.27570099
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 248.39915657, sen-loss: 22.86360218, dom-loss: 78.42705625, src-aux-loss: 68.38120586, tar-aux-loss: 78.72729117
Epoch: [29 ] train-acc: 0.93285714, dom-acc: 0.50866071, val-acc: 0.91250000, val_loss: 0.26776379
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 247.06318831, sen-loss: 22.59852415, dom-loss: 78.09701264, src-aux-loss: 67.47286314, tar-aux-loss: 78.89478803
Epoch: [30 ] train-acc: 0.93464286, dom-acc: 0.48750000, val-acc: 0.91500000, val_loss: 0.27055839
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 247.10980427, sen-loss: 22.27125595, dom-loss: 77.75646365, src-aux-loss: 66.86631295, tar-aux-loss: 80.21576977
Epoch: [31 ] train-acc: 0.93500000, dom-acc: 0.56160714, val-acc: 0.91250000, val_loss: 0.27048984
---------------------------------------------------

Successfully load model from save path: ./work/models/video_kitchen_HATN.ckpt
Best Epoch: [ 26] best val accuracy: 0.00000000 best val loss: 0.26686263
Testing accuracy: 0.86066667
./work/attentions/video_kitchen_train_HATN.txt
./work/attentions/video_kitchen_test_HATN.txt
loading data...
source domain:  books target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 11843
vocab-size:  100530
['great', 'good', 'excellent', 'best', 'wonderful', 'easy', 'fantastic', 'enjoyable', 'highly', 'entertaining', 'love', 'funny', 'loved', 'brilliant', 'awesome', 'interesting', 'amazing', 'useful', 'incredible', 'classic', 'honest', 'well', 'fascinating', 'favorite', 'important', 'sad', 'nice', 'solid', 'beautiful', 'inspiring', 'fun', 'superb', 'fabulous', 'emotional', 'compelling', 'inspirational', 'valuable', 'loves', 'amusing', 'perfect', 'essential', 'enjoyed', 'impressive', 'familiar', 'liked', 'riveting', 'wonderfully', 'marvelous', 'believable', 'real', 'greatest', 'humorous', 'enlightening', 'true', 'invaluable', 'finest', 'terrific', 'exciting']
['disappointing', 'boring', 'disappointed', 'better', 'poorly', 'horrible', 'useless', 'misleading', 'awful', 'confusing', 'repetitive', 'terrible', 'bad', 'annoying', 'flawed', 'simplistic', 'predictable', 'wasted', 'mediocre', 'difficult', 'tedious', 'worst', 'ridiculous', 'pathetic', 'laughable', 'silly', 'unnecessary', 'lacking', 'frustrating', 'trite', 'waste', 'poor', 'hard', 'outdated', 'lacks', 'biased', 'uninteresting', 'sloppy', 'tired', 'uninspired', 'disjointed', 'slow', 'dull', 'pretentious', 'weak', 'expensive', 'contrived', 'inaccurate', 'tiresome', 'superficial', 'absurd', 'stupid', 'amateurish', 'insulting', 'wrong', 'overrated', 'unlikeable', 'dissapointed', 'unbelievable', 'hackneyed', 'shallow', 'clumsy', 'dangerous', 'lame', 'impossible', 'utterly', 'wastes', 'terribly', 'unreadable']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
100530
5600 400 6000 15750 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 402.63857675, sen-loss: 77.64801139, dom-loss: 77.77053362, src-aux-loss: 131.00984269, tar-aux-loss: 116.21018696
Epoch: [1  ] train-acc: 0.67232143, dom-acc: 0.69053571, val-acc: 0.68500000, val_loss: 0.65555382
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 377.85021043, sen-loss: 70.77374393, dom-loss: 75.69011706, src-aux-loss: 122.77883101, tar-aux-loss: 108.60751641
Epoch: [2  ] train-acc: 0.74553571, dom-acc: 0.71535714, val-acc: 0.76250000, val_loss: 0.59159207
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.35580635, sen-loss: 63.59830466, dom-loss: 74.88275021, src-aux-loss: 118.13540256, tar-aux-loss: 103.73935020
Epoch: [3  ] train-acc: 0.79392857, dom-acc: 0.67660714, val-acc: 0.79750000, val_loss: 0.51895082
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.36631751, sen-loss: 55.75049651, dom-loss: 74.69802576, src-aux-loss: 114.59869397, tar-aux-loss: 102.31909919
Epoch: [4  ] train-acc: 0.82446429, dom-acc: 0.62812500, val-acc: 0.83500000, val_loss: 0.44513515
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 334.28875566, sen-loss: 48.48933274, dom-loss: 74.83299571, src-aux-loss: 111.88666803, tar-aux-loss: 99.07976002
Epoch: [5  ] train-acc: 0.84339286, dom-acc: 0.62285714, val-acc: 0.85500000, val_loss: 0.38687193
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 325.60546207, sen-loss: 43.00317907, dom-loss: 75.00629830, src-aux-loss: 109.66155905, tar-aux-loss: 97.93442571
Epoch: [6  ] train-acc: 0.86410714, dom-acc: 0.62955357, val-acc: 0.86500000, val_loss: 0.34569362
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 317.91447377, sen-loss: 38.67031963, dom-loss: 75.02388215, src-aux-loss: 107.70855850, tar-aux-loss: 96.51171488
Epoch: [7  ] train-acc: 0.87482143, dom-acc: 0.64982143, val-acc: 0.86500000, val_loss: 0.32803488
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 313.56518316, sen-loss: 36.13525806, dom-loss: 75.12620866, src-aux-loss: 106.51283485, tar-aux-loss: 95.79088336
Epoch: [8  ] train-acc: 0.87089286, dom-acc: 0.68339286, val-acc: 0.86750000, val_loss: 0.34598705
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 309.53674865, sen-loss: 34.11736222, dom-loss: 75.29226053, src-aux-loss: 105.35211420, tar-aux-loss: 94.77501130
Epoch: [9  ] train-acc: 0.88678571, dom-acc: 0.69741071, val-acc: 0.87500000, val_loss: 0.32324490
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 307.12653089, sen-loss: 32.87290896, dom-loss: 75.71017432, src-aux-loss: 104.20483083, tar-aux-loss: 94.33861834
Epoch: [10 ] train-acc: 0.89160714, dom-acc: 0.69133929, val-acc: 0.86750000, val_loss: 0.32532755
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 305.62326336, sen-loss: 32.03999902, dom-loss: 75.78039569, src-aux-loss: 103.20125896, tar-aux-loss: 94.60160947
Epoch: [11 ] train-acc: 0.89571429, dom-acc: 0.69410714, val-acc: 0.87750000, val_loss: 0.32499155
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 304.46788502, sen-loss: 31.49000530, dom-loss: 76.46285957, src-aux-loss: 102.71922868, tar-aux-loss: 93.79579169
Epoch: [12 ] train-acc: 0.89482143, dom-acc: 0.67982143, val-acc: 0.87500000, val_loss: 0.32819259
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 302.76476741, sen-loss: 30.86101557, dom-loss: 76.40495718, src-aux-loss: 102.06937301, tar-aux-loss: 93.42942184
Epoch: [13 ] train-acc: 0.89982143, dom-acc: 0.67892857, val-acc: 0.87750000, val_loss: 0.31885278
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 301.46071291, sen-loss: 30.22561489, dom-loss: 76.83855653, src-aux-loss: 101.45577967, tar-aux-loss: 92.94076258
Epoch: [14 ] train-acc: 0.89928571, dom-acc: 0.67964286, val-acc: 0.87000000, val_loss: 0.32510465
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 300.10679150, sen-loss: 29.71214357, dom-loss: 76.95868289, src-aux-loss: 100.56298381, tar-aux-loss: 92.87298173
Epoch: [15 ] train-acc: 0.89928571, dom-acc: 0.68000000, val-acc: 0.87250000, val_loss: 0.32512665
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 299.74735785, sen-loss: 29.33720655, dom-loss: 77.35771561, src-aux-loss: 100.26883382, tar-aux-loss: 92.78360212
Epoch: [16 ] train-acc: 0.90285714, dom-acc: 0.65455357, val-acc: 0.87000000, val_loss: 0.31788951
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 298.55851722, sen-loss: 28.89449178, dom-loss: 77.67804199, src-aux-loss: 99.43933713, tar-aux-loss: 92.54664606
Epoch: [17 ] train-acc: 0.90535714, dom-acc: 0.64133929, val-acc: 0.88000000, val_loss: 0.31371257
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 298.36336017, sen-loss: 28.46746352, dom-loss: 77.69937485, src-aux-loss: 99.01594573, tar-aux-loss: 93.18057555
Epoch: [18 ] train-acc: 0.90553571, dom-acc: 0.64330357, val-acc: 0.88000000, val_loss: 0.31164214
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 296.52076292, sen-loss: 28.05672851, dom-loss: 78.07588100, src-aux-loss: 98.43808943, tar-aux-loss: 91.95006472
Epoch: [19 ] train-acc: 0.90750000, dom-acc: 0.64133929, val-acc: 0.87500000, val_loss: 0.31562325
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 294.20023298, sen-loss: 27.92675966, dom-loss: 78.23689026, src-aux-loss: 98.15306932, tar-aux-loss: 89.88351351
Epoch: [20 ] train-acc: 0.91000000, dom-acc: 0.61196429, val-acc: 0.87750000, val_loss: 0.31026819
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 294.68466091, sen-loss: 27.41305165, dom-loss: 78.21892154, src-aux-loss: 97.49703640, tar-aux-loss: 91.55565363
Epoch: [21 ] train-acc: 0.91017857, dom-acc: 0.63312500, val-acc: 0.88000000, val_loss: 0.31482989
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 293.90282989, sen-loss: 27.12809391, dom-loss: 78.40989095, src-aux-loss: 96.94288468, tar-aux-loss: 91.42195922
Epoch: [22 ] train-acc: 0.91196429, dom-acc: 0.61107143, val-acc: 0.88000000, val_loss: 0.30914727
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 294.05865598, sen-loss: 26.81527007, dom-loss: 78.38106430, src-aux-loss: 96.86906743, tar-aux-loss: 91.99325329
Epoch: [23 ] train-acc: 0.91321429, dom-acc: 0.62464286, val-acc: 0.88000000, val_loss: 0.31824774
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 292.48637509, sen-loss: 26.41176990, dom-loss: 78.49348354, src-aux-loss: 96.14552492, tar-aux-loss: 91.43559504
Epoch: [24 ] train-acc: 0.91321429, dom-acc: 0.61232143, val-acc: 0.88250000, val_loss: 0.31299329
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 292.20505977, sen-loss: 26.11199976, dom-loss: 78.40099281, src-aux-loss: 95.55810070, tar-aux-loss: 92.13396859
Epoch: [25 ] train-acc: 0.91553571, dom-acc: 0.60696429, val-acc: 0.88500000, val_loss: 0.31348798
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 289.14221597, sen-loss: 25.78486276, dom-loss: 78.44214809, src-aux-loss: 95.21236628, tar-aux-loss: 89.70283991
Epoch: [26 ] train-acc: 0.91678571, dom-acc: 0.60580357, val-acc: 0.88250000, val_loss: 0.31262803
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 289.32463026, sen-loss: 25.59023770, dom-loss: 78.45034319, src-aux-loss: 94.90056252, tar-aux-loss: 90.38348836
Epoch: [27 ] train-acc: 0.91750000, dom-acc: 0.60553571, val-acc: 0.88250000, val_loss: 0.31152368
---------------------------------------------------

Successfully load model from save path: ./work/models/books_dvd_HATN.ckpt
Best Epoch: [ 22] best val accuracy: 0.00000000 best val loss: 0.30914727
Testing accuracy: 0.87100000
./work/attentions/books_dvd_train_HATN.txt
./work/attentions/books_dvd_test_HATN.txt
loading data...
source domain:  books target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 17009
vocab-size:  83050
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'fantastic', 'love', 'funny', 'well', 'awesome', 'entertaining', 'classic', 'enjoyed', 'brilliant', 'favorite', 'amazing', 'fascinating', 'interesting', 'loved', 'wonderful', 'nice', 'incredible', 'solid', 'sad', 'essential', 'believable', 'important', 'amusing', 'fun', 'superb', 'riveting', 'useful', 'inspiring', 'inspirational', 'real', 'greatest', 'compelling', 'wonderfully', 'hilarious', 'true', 'liked', 'emotional', 'finest', 'honest', 'loves', 'perfect', 'valuable', 'authentic', 'humorous', 'enlightening', 'invaluable', 'refreshing', 'makes', 'impressive', 'gives']
['disappointing', 'disappointed', 'boring', 'poorly', 'better', 'horrible', 'repetitive', 'confusing', 'useless', 'lacks', 'annoying', 'predictable', 'misleading', 'hard', 'awful', 'worst', 'difficult', 'wasted', 'flawed', 'tedious', 'pathetic', 'simplistic', 'laughable', 'terrible', 'unnecessary', 'slow', 'frustrating', 'ridiculous', 'trite', 'uninteresting', 'mediocre', 'much', 'sloppy', 'waste', 'weak', 'uninspired', 'lacking', 'utterly', 'tiresome', 'superficial', 'biased', 'silly', 'contrived', 'unbelievable', 'disjointed', 'outdated', 'wrong', 'terribly', 'expensive', 'dangerous', 'lame', 'unlikeable', 'inaccurate', 'bad', 'amateurish', 'dissapointed', 'unreadable', 'hackneyed', 'shallow', 'poor', 'deceived', 'ruined', 'fails', 'overrated', 'dull', 'repetitious', 'impossible', 'instead', 'wastes', 'proven', 'stilted', 'irritating', 'clumsy']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
83050
5600 400 6000 15750 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 406.10414338, sen-loss: 77.68093652, dom-loss: 78.43494052, src-aux-loss: 126.89139593, tar-aux-loss: 123.09687191
Epoch: [1  ] train-acc: 0.66982143, dom-acc: 0.75312500, val-acc: 0.66500000, val_loss: 0.65611833
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 376.99358940, sen-loss: 70.95194155, dom-loss: 74.73614687, src-aux-loss: 117.24497163, tar-aux-loss: 114.06052810
Epoch: [2  ] train-acc: 0.73803571, dom-acc: 0.82062500, val-acc: 0.75750000, val_loss: 0.59543836
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.48172951, sen-loss: 63.98345438, dom-loss: 73.13479948, src-aux-loss: 112.45607829, tar-aux-loss: 110.90739769
Epoch: [3  ] train-acc: 0.79392857, dom-acc: 0.80580357, val-acc: 0.79750000, val_loss: 0.52468306
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.49728656, sen-loss: 56.37329060, dom-loss: 73.09109592, src-aux-loss: 109.20368654, tar-aux-loss: 108.82921344
Epoch: [4  ] train-acc: 0.82107143, dom-acc: 0.75142857, val-acc: 0.82750000, val_loss: 0.45154208
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 335.74368262, sen-loss: 48.99030393, dom-loss: 73.60929298, src-aux-loss: 107.02552474, tar-aux-loss: 106.11856091
Epoch: [5  ] train-acc: 0.84107143, dom-acc: 0.73116071, val-acc: 0.84750000, val_loss: 0.38988945
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 327.85630226, sen-loss: 43.17859706, dom-loss: 74.39981818, src-aux-loss: 105.34956360, tar-aux-loss: 104.92832267
Epoch: [6  ] train-acc: 0.86196429, dom-acc: 0.75910714, val-acc: 0.86750000, val_loss: 0.34549758
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 321.15756249, sen-loss: 38.56309415, dom-loss: 74.98034406, src-aux-loss: 103.84577996, tar-aux-loss: 103.76834482
Epoch: [7  ] train-acc: 0.87660714, dom-acc: 0.75901786, val-acc: 0.87250000, val_loss: 0.32553625
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 315.81305623, sen-loss: 35.74422948, dom-loss: 75.62111783, src-aux-loss: 102.31421351, tar-aux-loss: 102.13349682
Epoch: [8  ] train-acc: 0.88035714, dom-acc: 0.70794643, val-acc: 0.88000000, val_loss: 0.32928956
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 313.32053041, sen-loss: 34.00152130, dom-loss: 76.55039310, src-aux-loss: 101.36096007, tar-aux-loss: 101.40765643
Epoch: [9  ] train-acc: 0.88714286, dom-acc: 0.66562500, val-acc: 0.87250000, val_loss: 0.32404754
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 313.33228564, sen-loss: 32.94719806, dom-loss: 77.75157100, src-aux-loss: 100.40418619, tar-aux-loss: 102.22933066
Epoch: [10 ] train-acc: 0.89125000, dom-acc: 0.63205357, val-acc: 0.87250000, val_loss: 0.32379538
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 311.42641592, sen-loss: 32.22355533, dom-loss: 78.26091081, src-aux-loss: 99.85671204, tar-aux-loss: 101.08524007
Epoch: [11 ] train-acc: 0.89446429, dom-acc: 0.61678571, val-acc: 0.87250000, val_loss: 0.32298577
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 310.48362017, sen-loss: 31.51590312, dom-loss: 79.18780237, src-aux-loss: 99.00003165, tar-aux-loss: 100.77988529
Epoch: [12 ] train-acc: 0.89500000, dom-acc: 0.57875000, val-acc: 0.87750000, val_loss: 0.32433864
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 308.99544024, sen-loss: 30.98374376, dom-loss: 79.51844269, src-aux-loss: 98.57121128, tar-aux-loss: 99.92204297
Epoch: [13 ] train-acc: 0.89821429, dom-acc: 0.57026786, val-acc: 0.88000000, val_loss: 0.32121259
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 308.90870786, sen-loss: 30.48218000, dom-loss: 79.84137261, src-aux-loss: 98.15807736, tar-aux-loss: 100.42707688
Epoch: [14 ] train-acc: 0.89946429, dom-acc: 0.55508929, val-acc: 0.88000000, val_loss: 0.32269990
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 305.31336379, sen-loss: 29.84386748, dom-loss: 79.80419409, src-aux-loss: 97.29045630, tar-aux-loss: 98.37484664
Epoch: [15 ] train-acc: 0.89946429, dom-acc: 0.55982143, val-acc: 0.87500000, val_loss: 0.32454655
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 305.07550478, sen-loss: 29.49966337, dom-loss: 79.62279344, src-aux-loss: 97.05970997, tar-aux-loss: 98.89333767
Epoch: [16 ] train-acc: 0.90285714, dom-acc: 0.54169643, val-acc: 0.88000000, val_loss: 0.32046497
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 304.20260358, sen-loss: 29.00562321, dom-loss: 79.42610216, src-aux-loss: 96.34194803, tar-aux-loss: 99.42892957
Epoch: [17 ] train-acc: 0.90500000, dom-acc: 0.53758929, val-acc: 0.87750000, val_loss: 0.31272876
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 303.23254633, sen-loss: 28.63047192, dom-loss: 78.94302940, src-aux-loss: 96.01853079, tar-aux-loss: 99.64051491
Epoch: [18 ] train-acc: 0.90607143, dom-acc: 0.57607143, val-acc: 0.87500000, val_loss: 0.31007224
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 300.18776703, sen-loss: 28.19734669, dom-loss: 78.48116899, src-aux-loss: 95.52404016, tar-aux-loss: 97.98521221
Epoch: [19 ] train-acc: 0.90857143, dom-acc: 0.59312500, val-acc: 0.87750000, val_loss: 0.31098920
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 299.53665781, sen-loss: 27.94459268, dom-loss: 78.34916842, src-aux-loss: 95.33153635, tar-aux-loss: 97.91136014
Epoch: [20 ] train-acc: 0.90892857, dom-acc: 0.60276786, val-acc: 0.87750000, val_loss: 0.30977613
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 297.52988529, sen-loss: 27.48615207, dom-loss: 78.09013897, src-aux-loss: 94.75145137, tar-aux-loss: 97.20214194
Epoch: [21 ] train-acc: 0.91089286, dom-acc: 0.60732143, val-acc: 0.88500000, val_loss: 0.31245518
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 298.52986670, sen-loss: 27.24033990, dom-loss: 77.90466833, src-aux-loss: 94.21216208, tar-aux-loss: 99.17269719
Epoch: [22 ] train-acc: 0.91357143, dom-acc: 0.60964286, val-acc: 0.87500000, val_loss: 0.30579257
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 295.35536814, sen-loss: 26.91957207, dom-loss: 77.68896437, src-aux-loss: 94.18443072, tar-aux-loss: 96.56240082
Epoch: [23 ] train-acc: 0.91053571, dom-acc: 0.61875000, val-acc: 0.87500000, val_loss: 0.31894308
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 295.14076591, sen-loss: 26.53423662, dom-loss: 77.58179903, src-aux-loss: 93.57979006, tar-aux-loss: 97.44494009
Epoch: [24 ] train-acc: 0.91482143, dom-acc: 0.61660714, val-acc: 0.88000000, val_loss: 0.31110162
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.70006752, sen-loss: 26.23277760, dom-loss: 77.67890781, src-aux-loss: 93.18767899, tar-aux-loss: 98.60070366
Epoch: [25 ] train-acc: 0.91500000, dom-acc: 0.61857143, val-acc: 0.88250000, val_loss: 0.30989948
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.16650772, sen-loss: 25.90115758, dom-loss: 77.67997611, src-aux-loss: 92.86396366, tar-aux-loss: 96.72141010
Epoch: [26 ] train-acc: 0.91571429, dom-acc: 0.60473214, val-acc: 0.88250000, val_loss: 0.30871245
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 291.88241601, sen-loss: 25.73789911, dom-loss: 77.65666580, src-aux-loss: 92.55959344, tar-aux-loss: 95.92825770
Epoch: [27 ] train-acc: 0.91964286, dom-acc: 0.60794643, val-acc: 0.88000000, val_loss: 0.30595550
---------------------------------------------------

Successfully load model from save path: ./work/models/books_electronics_HATN.ckpt
Best Epoch: [ 22] best val accuracy: 0.00000000 best val loss: 0.30579257
Testing accuracy: 0.85083333
./work/attentions/books_electronics_train_HATN.txt
./work/attentions/books_electronics_test_HATN.txt
loading data...
source domain:  books target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 13856
vocab-size:  78006
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'love', 'funny', 'fantastic', 'well', 'enjoyed', 'awesome', 'entertaining', 'classic', 'favorite', 'brilliant', 'fascinating', 'wonderful', 'amazing', 'loved', 'interesting', 'fun', 'nice', 'incredible', 'solid', 'inspiring', 'superb', 'believable', 'riveting', 'useful', 'essential', 'important', 'emotional', 'inspirational', 'sad', 'compelling', 'wonderfully', 'real', 'fabulous', 'hilarious', 'greatest', 'liked', 'refreshing', 'honest', 'makes', 'impressive', 'loves', 'valuable', 'authentic', 'humorous', 'enlightening', 'true', 'simple', 'invaluable', 'heartwarming', 'gives']
['disappointing', 'disappointed', 'boring', 'poorly', 'better', 'horrible', 'repetitive', 'confusing', 'useless', 'hard', 'lacks', 'annoying', 'difficult', 'predictable', 'worst', 'awful', 'tedious', 'misleading', 'pathetic', 'flawed', 'wasted', 'simplistic', 'unnecessary', 'terrible', 'slow', 'laughable', 'frustrating', 'uninteresting', 'trite', 'mediocre', 'ridiculous', 'uninspired', 'waste', 'bad', 'contrived', 'sloppy', 'tiresome', 'shallow', 'silly', 'disjointed', 'utterly', 'outdated', 'superficial', 'weak', 'terribly', 'expensive', 'unbelievable', 'lame', 'unlikeable', 'inaccurate', 'impossible', 'wastes', 'dissapointed', 'unreadable', 'seems', 'hackneyed', 'biased', 'poor', 'deceived', 'unconvincing', 'trying', 'ruined', 'lacking', 'fails', 'finest', 'dangerous', 'overrated', 'dull', 'repetitious', 'proven', 'stilted', 'wrong', 'sadly', 'disapointed', 'irritating', 'clumsy']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
78006
5600 400 6000 15750 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.12460971, sen-loss: 77.67308879, dom-loss: 78.09087020, src-aux-loss: 129.29636967, tar-aux-loss: 119.06428140
Epoch: [1  ] train-acc: 0.65964286, dom-acc: 0.75991071, val-acc: 0.64500000, val_loss: 0.65666765
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 375.39576316, sen-loss: 70.94814122, dom-loss: 74.63588756, src-aux-loss: 119.71411961, tar-aux-loss: 110.09761447
Epoch: [2  ] train-acc: 0.74392857, dom-acc: 0.82937500, val-acc: 0.75000000, val_loss: 0.59454846
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 357.99520850, sen-loss: 63.90694529, dom-loss: 73.30591261, src-aux-loss: 114.90956187, tar-aux-loss: 105.87278968
Epoch: [3  ] train-acc: 0.79267857, dom-acc: 0.81035714, val-acc: 0.80000000, val_loss: 0.52428275
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 344.88889241, sen-loss: 56.29373851, dom-loss: 73.39407879, src-aux-loss: 111.30740350, tar-aux-loss: 103.89367104
Epoch: [4  ] train-acc: 0.82392857, dom-acc: 0.77142857, val-acc: 0.82250000, val_loss: 0.45062912
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 334.37819314, sen-loss: 48.93082148, dom-loss: 74.08152473, src-aux-loss: 108.96997976, tar-aux-loss: 102.39586741
Epoch: [5  ] train-acc: 0.84392857, dom-acc: 0.77089286, val-acc: 0.85500000, val_loss: 0.39020658
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 326.10397315, sen-loss: 42.98430538, dom-loss: 74.62334484, src-aux-loss: 107.40850300, tar-aux-loss: 101.08781934
Epoch: [6  ] train-acc: 0.86642857, dom-acc: 0.76125000, val-acc: 0.86750000, val_loss: 0.34500384
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 318.85643983, sen-loss: 38.48833854, dom-loss: 75.22719502, src-aux-loss: 105.49848157, tar-aux-loss: 99.64242363
Epoch: [7  ] train-acc: 0.87642857, dom-acc: 0.71866071, val-acc: 0.87000000, val_loss: 0.32706189
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 316.00310183, sen-loss: 35.75886849, dom-loss: 75.85558897, src-aux-loss: 104.18613118, tar-aux-loss: 100.20251358
Epoch: [8  ] train-acc: 0.87517857, dom-acc: 0.68008929, val-acc: 0.86750000, val_loss: 0.33778128
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 310.58494091, sen-loss: 33.98545735, dom-loss: 76.53239942, src-aux-loss: 103.03170741, tar-aux-loss: 97.03537476
Epoch: [9  ] train-acc: 0.88625000, dom-acc: 0.65250000, val-acc: 0.87750000, val_loss: 0.32495350
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 310.60240459, sen-loss: 32.68836233, dom-loss: 77.66493958, src-aux-loss: 101.86919862, tar-aux-loss: 98.37990266
Epoch: [10 ] train-acc: 0.89107143, dom-acc: 0.63250000, val-acc: 0.87750000, val_loss: 0.32246995
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 309.53962064, sen-loss: 31.98195764, dom-loss: 78.06221569, src-aux-loss: 101.07514870, tar-aux-loss: 98.42029816
Epoch: [11 ] train-acc: 0.89732143, dom-acc: 0.62526786, val-acc: 0.87750000, val_loss: 0.31992713
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 307.97428751, sen-loss: 31.34245481, dom-loss: 78.86988980, src-aux-loss: 100.31238717, tar-aux-loss: 97.44955611
Epoch: [12 ] train-acc: 0.89428571, dom-acc: 0.60687500, val-acc: 0.87000000, val_loss: 0.32665631
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 306.31842208, sen-loss: 30.86505215, dom-loss: 78.73894906, src-aux-loss: 100.05552423, tar-aux-loss: 96.65889615
Epoch: [13 ] train-acc: 0.89982143, dom-acc: 0.60267857, val-acc: 0.87500000, val_loss: 0.31549287
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 304.18588257, sen-loss: 30.28790130, dom-loss: 78.86950457, src-aux-loss: 99.15446502, tar-aux-loss: 95.87401199
Epoch: [14 ] train-acc: 0.89892857, dom-acc: 0.61187500, val-acc: 0.87500000, val_loss: 0.32244739
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 302.63156319, sen-loss: 29.67891222, dom-loss: 78.57147962, src-aux-loss: 98.27885371, tar-aux-loss: 96.10231704
Epoch: [15 ] train-acc: 0.90035714, dom-acc: 0.61741071, val-acc: 0.88000000, val_loss: 0.31818461
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 302.54381490, sen-loss: 29.28236887, dom-loss: 78.59944963, src-aux-loss: 98.05852091, tar-aux-loss: 96.60347623
Epoch: [16 ] train-acc: 0.90321429, dom-acc: 0.60732143, val-acc: 0.87250000, val_loss: 0.31943005
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 299.18330860, sen-loss: 28.77950162, dom-loss: 78.23553771, src-aux-loss: 97.28937441, tar-aux-loss: 94.87889302
Epoch: [17 ] train-acc: 0.90678571, dom-acc: 0.62062500, val-acc: 0.87250000, val_loss: 0.30994883
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 297.55445671, sen-loss: 28.31110626, dom-loss: 77.60172290, src-aux-loss: 96.91327125, tar-aux-loss: 94.72835630
Epoch: [18 ] train-acc: 0.90857143, dom-acc: 0.62321429, val-acc: 0.87500000, val_loss: 0.30693614
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 297.02871895, sen-loss: 27.94938818, dom-loss: 77.36227810, src-aux-loss: 96.33012116, tar-aux-loss: 95.38693243
Epoch: [19 ] train-acc: 0.90839286, dom-acc: 0.64187500, val-acc: 0.87750000, val_loss: 0.30928850
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 295.29695368, sen-loss: 27.75804673, dom-loss: 77.37924290, src-aux-loss: 96.01889062, tar-aux-loss: 94.14077330
Epoch: [20 ] train-acc: 0.91017857, dom-acc: 0.64348214, val-acc: 0.87750000, val_loss: 0.30826417
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 294.07248950, sen-loss: 27.26083392, dom-loss: 77.11539823, src-aux-loss: 95.49884248, tar-aux-loss: 94.19741440
Epoch: [21 ] train-acc: 0.91107143, dom-acc: 0.64669643, val-acc: 0.87750000, val_loss: 0.31172836
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 293.27788520, sen-loss: 26.95679961, dom-loss: 77.08241838, src-aux-loss: 94.94801867, tar-aux-loss: 94.29064977
Epoch: [22 ] train-acc: 0.91660714, dom-acc: 0.64330357, val-acc: 0.87750000, val_loss: 0.30476683
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 292.63738656, sen-loss: 26.72048242, dom-loss: 76.92376828, src-aux-loss: 94.72497535, tar-aux-loss: 94.26816010
Epoch: [23 ] train-acc: 0.91232143, dom-acc: 0.64696429, val-acc: 0.88000000, val_loss: 0.31746319
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 292.31451678, sen-loss: 26.36541902, dom-loss: 77.18339956, src-aux-loss: 94.32398105, tar-aux-loss: 94.44171631
Epoch: [24 ] train-acc: 0.91767857, dom-acc: 0.63830357, val-acc: 0.88250000, val_loss: 0.30578911
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 290.88688827, sen-loss: 26.02369619, dom-loss: 77.08310884, src-aux-loss: 93.60961789, tar-aux-loss: 94.17046446
Epoch: [25 ] train-acc: 0.91589286, dom-acc: 0.63687500, val-acc: 0.87500000, val_loss: 0.31025928
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 290.14782810, sen-loss: 25.66702924, dom-loss: 77.31199670, src-aux-loss: 93.37303555, tar-aux-loss: 93.79576486
Epoch: [26 ] train-acc: 0.91946429, dom-acc: 0.62732143, val-acc: 0.88500000, val_loss: 0.30592242
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 288.97863388, sen-loss: 25.39796866, dom-loss: 77.49330753, src-aux-loss: 93.09133482, tar-aux-loss: 92.99602276
Epoch: [27 ] train-acc: 0.92178571, dom-acc: 0.62008929, val-acc: 0.88250000, val_loss: 0.30413997
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 289.61928606, sen-loss: 25.04899811, dom-loss: 77.58989185, src-aux-loss: 92.75550169, tar-aux-loss: 94.22489434
Epoch: [28 ] train-acc: 0.91857143, dom-acc: 0.62401786, val-acc: 0.88500000, val_loss: 0.31677896
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 288.24782801, sen-loss: 24.79640497, dom-loss: 77.67519486, src-aux-loss: 92.25590789, tar-aux-loss: 93.52031970
Epoch: [29 ] train-acc: 0.92357143, dom-acc: 0.61562500, val-acc: 0.88000000, val_loss: 0.30263236
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 286.57304287, sen-loss: 24.60614152, dom-loss: 77.81891197, src-aux-loss: 91.75807923, tar-aux-loss: 92.38991034
Epoch: [30 ] train-acc: 0.92464286, dom-acc: 0.61482143, val-acc: 0.87750000, val_loss: 0.31134495
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 285.80287313, sen-loss: 24.18864869, dom-loss: 77.80326968, src-aux-loss: 91.62764525, tar-aux-loss: 92.18330801
Epoch: [31 ] train-acc: 0.92750000, dom-acc: 0.61053571, val-acc: 0.88250000, val_loss: 0.30606046
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 286.04136777, sen-loss: 23.94688199, dom-loss: 77.60082728, src-aux-loss: 91.07151598, tar-aux-loss: 93.42214352
Epoch: [32 ] train-acc: 0.92785714, dom-acc: 0.62008929, val-acc: 0.88250000, val_loss: 0.30425936
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 284.63859892, sen-loss: 23.73567954, dom-loss: 77.62364882, src-aux-loss: 90.73139447, tar-aux-loss: 92.54787594
Epoch: [33 ] train-acc: 0.92892857, dom-acc: 0.61973214, val-acc: 0.88500000, val_loss: 0.30553320
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 282.83459115, sen-loss: 23.45213561, dom-loss: 77.81854975, src-aux-loss: 90.24240994, tar-aux-loss: 91.32149684
Epoch: [34 ] train-acc: 0.92982143, dom-acc: 0.62875000, val-acc: 0.88250000, val_loss: 0.30571672
---------------------------------------------------

Successfully load model from save path: ./work/models/books_kitchen_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.30263236
Testing accuracy: 0.86583333
./work/attentions/books_kitchen_train_HATN.txt
./work/attentions/books_kitchen_test_HATN.txt
loading data...
source domain:  books target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 30180
vocab-size:  98084
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'funny', 'fantastic', 'love', 'well', 'entertaining', 'awesome', 'enjoyed', 'brilliant', 'classic', 'favorite', 'fascinating', 'loved', 'wonderful', 'amazing', 'interesting', 'incredible', 'nice', 'inspiring', 'emotional', 'solid', 'fun', 'sad', 'believable', 'essential', 'amusing', 'superb', 'true', 'riveting', 'useful', 'important', 'inspirational', 'compelling', 'real', 'greatest', 'liked', 'invaluable', 'finest', 'honest', 'wonderfully', 'perfect', 'valuable', 'fabulous', 'hilarious', 'authentic', 'humorous', 'enlightening', 'refreshing', 'loves', 'makes', 'impressive', 'marvelous']
['disappointing', 'boring', 'disappointed', 'poorly', 'horrible', 'better', 'repetitive', 'confusing', 'useless', 'worst', 'lacks', 'annoying', 'misleading', 'hard', 'predictable', 'awful', 'wasted', 'pathetic', 'difficult', 'tedious', 'terrible', 'flawed', 'simplistic', 'unnecessary', 'laughable', 'slow', 'frustrating', 'uninteresting', 'ridiculous', 'trite', 'mediocre', 'uninspired', 'sloppy', 'waste', 'weak', 'much', 'lacking', 'wrong', 'utterly', 'tiresome', 'superficial', 'biased', 'silly', 'contrived', 'unbelievable', 'disjointed', 'bad', 'terribly', 'dangerous', 'lame', 'unlikeable', 'inaccurate', 'outdated', 'dissapointed', 'unreadable', 'hackneyed', 'shallow', 'poor', 'deceived', 'unconvincing', 'expensive', 'ruined', 'fails', 'overrated', 'dull', 'repetitious', 'impossible', 'amateurish', 'instead', 'wastes', 'proven', 'stilted', 'irritating', 'clumsy']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
98084
5600 400 6000 15750 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 400.34999871, sen-loss: 77.67006552, dom-loss: 77.73891622, src-aux-loss: 127.50883019, tar-aux-loss: 117.43218833
Epoch: [1  ] train-acc: 0.67142857, dom-acc: 0.70428571, val-acc: 0.67750000, val_loss: 0.65575433
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 372.42111397, sen-loss: 70.87956494, dom-loss: 75.73771083, src-aux-loss: 118.04186094, tar-aux-loss: 107.76197892
Epoch: [2  ] train-acc: 0.73571429, dom-acc: 0.72821429, val-acc: 0.75500000, val_loss: 0.59504342
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 356.16316605, sen-loss: 63.92011562, dom-loss: 74.92459339, src-aux-loss: 113.39350641, tar-aux-loss: 103.92495054
Epoch: [3  ] train-acc: 0.79267857, dom-acc: 0.69946429, val-acc: 0.79750000, val_loss: 0.52390701
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 342.71236610, sen-loss: 56.31660777, dom-loss: 74.68164831, src-aux-loss: 109.91962147, tar-aux-loss: 101.79448801
Epoch: [4  ] train-acc: 0.82107143, dom-acc: 0.67026786, val-acc: 0.82750000, val_loss: 0.45103943
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 329.25439119, sen-loss: 48.99234450, dom-loss: 74.69271183, src-aux-loss: 107.58093500, tar-aux-loss: 97.98839933
Epoch: [5  ] train-acc: 0.84160714, dom-acc: 0.66464286, val-acc: 0.84750000, val_loss: 0.39106613
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 321.46778870, sen-loss: 43.24551105, dom-loss: 74.86463594, src-aux-loss: 105.70314097, tar-aux-loss: 97.65450341
Epoch: [6  ] train-acc: 0.86160714, dom-acc: 0.68000000, val-acc: 0.86000000, val_loss: 0.34856915
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 312.28967690, sen-loss: 38.64157823, dom-loss: 74.81551898, src-aux-loss: 103.81372553, tar-aux-loss: 95.01885360
Epoch: [7  ] train-acc: 0.87500000, dom-acc: 0.69544643, val-acc: 0.86500000, val_loss: 0.32963401
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 309.24970913, sen-loss: 35.96366869, dom-loss: 74.81759155, src-aux-loss: 102.58158153, tar-aux-loss: 95.88686711
Epoch: [8  ] train-acc: 0.87589286, dom-acc: 0.71589286, val-acc: 0.87250000, val_loss: 0.33310509
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 304.71101189, sen-loss: 34.09301217, dom-loss: 75.26571286, src-aux-loss: 101.56315708, tar-aux-loss: 93.78912860
Epoch: [9  ] train-acc: 0.88821429, dom-acc: 0.69812500, val-acc: 0.87750000, val_loss: 0.32314852
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 302.89196992, sen-loss: 33.00631067, dom-loss: 75.65195960, src-aux-loss: 100.53842270, tar-aux-loss: 93.69527608
Epoch: [10 ] train-acc: 0.88892857, dom-acc: 0.69892857, val-acc: 0.87500000, val_loss: 0.32820269
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 301.84719253, sen-loss: 32.24901231, dom-loss: 75.79135799, src-aux-loss: 99.88614553, tar-aux-loss: 93.92067629
Epoch: [11 ] train-acc: 0.89464286, dom-acc: 0.69303571, val-acc: 0.87250000, val_loss: 0.32439440
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 300.55092955, sen-loss: 31.61183953, dom-loss: 76.31214601, src-aux-loss: 99.19083369, tar-aux-loss: 93.43610930
Epoch: [12 ] train-acc: 0.89482143, dom-acc: 0.67928571, val-acc: 0.86500000, val_loss: 0.32568648
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 297.65357375, sen-loss: 31.05075087, dom-loss: 76.25785375, src-aux-loss: 98.57010448, tar-aux-loss: 91.77486426
Epoch: [13 ] train-acc: 0.89660714, dom-acc: 0.67142857, val-acc: 0.87500000, val_loss: 0.32212725
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 297.74796057, sen-loss: 30.58282214, dom-loss: 76.68658251, src-aux-loss: 98.16752899, tar-aux-loss: 92.31102681
Epoch: [14 ] train-acc: 0.89785714, dom-acc: 0.67687500, val-acc: 0.87750000, val_loss: 0.32759440
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 295.39545178, sen-loss: 29.89280774, dom-loss: 77.04983640, src-aux-loss: 97.40718079, tar-aux-loss: 91.04562759
Epoch: [15 ] train-acc: 0.89857143, dom-acc: 0.66830357, val-acc: 0.87000000, val_loss: 0.32684365
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 296.12686110, sen-loss: 29.54679167, dom-loss: 77.35868031, src-aux-loss: 97.17910844, tar-aux-loss: 92.04228121
Epoch: [16 ] train-acc: 0.90321429, dom-acc: 0.64125000, val-acc: 0.88500000, val_loss: 0.31752372
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 293.47935152, sen-loss: 29.07890856, dom-loss: 77.65376312, src-aux-loss: 96.30501664, tar-aux-loss: 90.44166481
Epoch: [17 ] train-acc: 0.90392857, dom-acc: 0.63857143, val-acc: 0.87750000, val_loss: 0.31533888
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 293.24085689, sen-loss: 28.70649924, dom-loss: 77.70820671, src-aux-loss: 96.04142040, tar-aux-loss: 90.78473043
Epoch: [18 ] train-acc: 0.90589286, dom-acc: 0.62714286, val-acc: 0.88000000, val_loss: 0.31191710
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 292.93490911, sen-loss: 28.28133017, dom-loss: 77.93836439, src-aux-loss: 95.68529993, tar-aux-loss: 91.02991629
Epoch: [19 ] train-acc: 0.90839286, dom-acc: 0.63080357, val-acc: 0.88000000, val_loss: 0.31435549
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 291.35247731, sen-loss: 28.06548995, dom-loss: 78.24371046, src-aux-loss: 95.21378309, tar-aux-loss: 89.82949275
Epoch: [20 ] train-acc: 0.90910714, dom-acc: 0.61250000, val-acc: 0.87750000, val_loss: 0.31104276
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 290.10258508, sen-loss: 27.57160757, dom-loss: 78.26049155, src-aux-loss: 94.50124931, tar-aux-loss: 89.76923645
Epoch: [21 ] train-acc: 0.91053571, dom-acc: 0.62125000, val-acc: 0.88250000, val_loss: 0.31274605
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 290.69205880, sen-loss: 27.34598862, dom-loss: 78.39054978, src-aux-loss: 94.11206216, tar-aux-loss: 90.84345752
Epoch: [22 ] train-acc: 0.91142857, dom-acc: 0.61035714, val-acc: 0.88000000, val_loss: 0.30896088
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 288.97732544, sen-loss: 27.00417117, dom-loss: 78.36824608, src-aux-loss: 93.93144470, tar-aux-loss: 89.67346305
Epoch: [23 ] train-acc: 0.91000000, dom-acc: 0.61580357, val-acc: 0.88000000, val_loss: 0.31869435
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 288.55658984, sen-loss: 26.62117104, dom-loss: 78.44456607, src-aux-loss: 93.40581381, tar-aux-loss: 90.08504099
Epoch: [24 ] train-acc: 0.91482143, dom-acc: 0.61348214, val-acc: 0.88250000, val_loss: 0.31106994
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 287.81692243, sen-loss: 26.32540278, dom-loss: 78.33804750, src-aux-loss: 92.85465837, tar-aux-loss: 90.29881489
Epoch: [25 ] train-acc: 0.91642857, dom-acc: 0.60946429, val-acc: 0.88250000, val_loss: 0.31017330
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 285.30399060, sen-loss: 25.95235638, dom-loss: 78.38825369, src-aux-loss: 92.51432908, tar-aux-loss: 88.44905329
Epoch: [26 ] train-acc: 0.91392857, dom-acc: 0.60348214, val-acc: 0.88000000, val_loss: 0.31328031
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 283.86659813, sen-loss: 25.79217900, dom-loss: 78.32037431, src-aux-loss: 92.21507096, tar-aux-loss: 87.53897345
Epoch: [27 ] train-acc: 0.91892857, dom-acc: 0.60419643, val-acc: 0.88250000, val_loss: 0.30842417
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 285.64333677, sen-loss: 25.41018049, dom-loss: 78.23480242, src-aux-loss: 92.08780801, tar-aux-loss: 89.91054720
Epoch: [28 ] train-acc: 0.91232143, dom-acc: 0.62616071, val-acc: 0.87250000, val_loss: 0.32310694
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 284.32111740, sen-loss: 25.10521428, dom-loss: 78.18232065, src-aux-loss: 91.48041707, tar-aux-loss: 89.55316705
Epoch: [29 ] train-acc: 0.92142857, dom-acc: 0.62169643, val-acc: 0.88250000, val_loss: 0.30643430
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 282.41822934, sen-loss: 24.85696334, dom-loss: 78.04335272, src-aux-loss: 90.99932694, tar-aux-loss: 88.51858562
Epoch: [30 ] train-acc: 0.91946429, dom-acc: 0.62419643, val-acc: 0.87500000, val_loss: 0.31654045
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 280.18568420, sen-loss: 24.48139108, dom-loss: 77.96464646, src-aux-loss: 90.76551014, tar-aux-loss: 86.97413570
Epoch: [31 ] train-acc: 0.92357143, dom-acc: 0.62687500, val-acc: 0.88500000, val_loss: 0.31080645
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 281.86876011, sen-loss: 24.20119131, dom-loss: 77.80008489, src-aux-loss: 90.16666442, tar-aux-loss: 89.70081908
Epoch: [32 ] train-acc: 0.92392857, dom-acc: 0.63294643, val-acc: 0.88000000, val_loss: 0.31222641
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 279.97513056, sen-loss: 23.97714338, dom-loss: 77.73214912, src-aux-loss: 89.84880829, tar-aux-loss: 88.41702777
Epoch: [33 ] train-acc: 0.92553571, dom-acc: 0.63357143, val-acc: 0.87750000, val_loss: 0.31267175
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 278.96359539, sen-loss: 23.78645203, dom-loss: 77.79204935, src-aux-loss: 89.42776096, tar-aux-loss: 87.95733279
Epoch: [34 ] train-acc: 0.92785714, dom-acc: 0.64125000, val-acc: 0.88500000, val_loss: 0.30898058
---------------------------------------------------

Successfully load model from save path: ./work/models/books_video_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.30643430
Testing accuracy: 0.88233333
./work/attentions/books_video_train_HATN.txt
./work/attentions/books_video_test_HATN.txt
loading data...
source domain:  dvd target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 9750
vocab-size:  100530
['best', 'good', 'great', 'excellent', 'funny', 'entertaining', 'enjoyable', 'better', 'awesome', 'fantastic', 'love', 'classic', 'wonderful', 'amazing', 'perfect', 'nice', 'brilliant', 'hilarious', 'loved', 'funniest', 'underrated', 'favorite', 'sad', 'interesting', 'superb', 'terrific', 'greatest', 'finest', 'real', 'solid', 'easy', 'incredible', 'memorable', 'fascinating', 'sappy', 'liked', 'spectacular', 'impressive', 'cute']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'wasted', 'dull', 'waste', 'stupid', 'pathetic', 'laughable', 'unwatchable', 'predictable', 'worse', 'annoying', 'lousy', 'forgettable', 'unfunny', 'uninspired', 'ruined', 'decent', 'slow', 'sucked', 'pointless', 'overrated', 'dismal', 'wrong', 'poorly', 'unoriginal', 'ok', 'mediocre', 'lacking', 'lame', 'dreadful', 'defective', 'sucks', 'crappy', 'atrocious', 'disgusting', 'hated', 'cheap', 'embarrassing', 'ridiculous', 'unconvincing', 'depressing', 'horrendous', 'biased', 'weak', 'disjointed', 'disapointed', 'frustrating', 'contrived', 'honest', 'insipid', 'inferior']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
100530
5600 400 6000 17843 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 407.56794548, sen-loss: 78.27819645, dom-loss: 79.59816492, src-aux-loss: 130.69728738, tar-aux-loss: 118.99429715
Epoch: [1  ] train-acc: 0.64642857, dom-acc: 0.60982143, val-acc: 0.67000000, val_loss: 0.66475844
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 384.24030447, sen-loss: 72.66853946, dom-loss: 77.23402554, src-aux-loss: 122.44866353, tar-aux-loss: 111.88907552
Epoch: [2  ] train-acc: 0.71767857, dom-acc: 0.76866071, val-acc: 0.75000000, val_loss: 0.60796350
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 366.37296414, sen-loss: 66.66693765, dom-loss: 76.03792489, src-aux-loss: 117.08283508, tar-aux-loss: 106.58526528
Epoch: [3  ] train-acc: 0.77803571, dom-acc: 0.79633929, val-acc: 0.80000000, val_loss: 0.54265457
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 353.05966067, sen-loss: 59.84847337, dom-loss: 75.27367991, src-aux-loss: 113.67645001, tar-aux-loss: 104.26105905
Epoch: [4  ] train-acc: 0.81178571, dom-acc: 0.79241071, val-acc: 0.81250000, val_loss: 0.47130916
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 340.70869446, sen-loss: 52.59251520, dom-loss: 74.83773774, src-aux-loss: 110.45288789, tar-aux-loss: 102.82555389
Epoch: [5  ] train-acc: 0.83678571, dom-acc: 0.77598214, val-acc: 0.86000000, val_loss: 0.40734941
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 329.31060219, sen-loss: 46.55560136, dom-loss: 74.62673843, src-aux-loss: 107.65565926, tar-aux-loss: 100.47260261
Epoch: [6  ] train-acc: 0.85446429, dom-acc: 0.76303571, val-acc: 0.88750000, val_loss: 0.34745467
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 320.63338017, sen-loss: 41.97527210, dom-loss: 74.50285602, src-aux-loss: 105.85842633, tar-aux-loss: 98.29682517
Epoch: [7  ] train-acc: 0.86892857, dom-acc: 0.75392857, val-acc: 0.90250000, val_loss: 0.31963614
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 316.61100960, sen-loss: 39.24686894, dom-loss: 74.31199926, src-aux-loss: 104.33416611, tar-aux-loss: 98.71797681
Epoch: [8  ] train-acc: 0.87785714, dom-acc: 0.75660714, val-acc: 0.90250000, val_loss: 0.30383062
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 313.50791311, sen-loss: 37.28483999, dom-loss: 74.60190785, src-aux-loss: 102.80351180, tar-aux-loss: 98.81765401
Epoch: [9  ] train-acc: 0.88089286, dom-acc: 0.75687500, val-acc: 0.90250000, val_loss: 0.29636517
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 308.52907014, sen-loss: 35.77116106, dom-loss: 74.62044907, src-aux-loss: 101.83027118, tar-aux-loss: 96.30718881
Epoch: [10 ] train-acc: 0.88517857, dom-acc: 0.75160714, val-acc: 0.90000000, val_loss: 0.28758857
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 307.11178899, sen-loss: 34.89844729, dom-loss: 74.40558439, src-aux-loss: 100.90723050, tar-aux-loss: 96.90052503
Epoch: [11 ] train-acc: 0.88428571, dom-acc: 0.74151786, val-acc: 0.89500000, val_loss: 0.28361589
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 306.50604773, sen-loss: 34.27538668, dom-loss: 74.75671780, src-aux-loss: 100.26426691, tar-aux-loss: 97.20967638
Epoch: [12 ] train-acc: 0.88803571, dom-acc: 0.73294643, val-acc: 0.90000000, val_loss: 0.29147571
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 302.76399398, sen-loss: 33.42336184, dom-loss: 75.02854019, src-aux-loss: 99.43978447, tar-aux-loss: 94.87230700
Epoch: [13 ] train-acc: 0.89232143, dom-acc: 0.72616071, val-acc: 0.90250000, val_loss: 0.28359216
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 305.03320098, sen-loss: 33.00353692, dom-loss: 75.54893529, src-aux-loss: 99.06354314, tar-aux-loss: 97.41718519
Epoch: [14 ] train-acc: 0.89250000, dom-acc: 0.71946429, val-acc: 0.90000000, val_loss: 0.28244412
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 301.19538379, sen-loss: 32.40715058, dom-loss: 75.54745883, src-aux-loss: 98.13188505, tar-aux-loss: 95.10888791
Epoch: [15 ] train-acc: 0.89196429, dom-acc: 0.71321429, val-acc: 0.90000000, val_loss: 0.28399631
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 301.20784330, sen-loss: 32.02287330, dom-loss: 75.94290328, src-aux-loss: 97.62991315, tar-aux-loss: 95.61215508
Epoch: [16 ] train-acc: 0.89607143, dom-acc: 0.70651786, val-acc: 0.89750000, val_loss: 0.27456066
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 298.98050117, sen-loss: 31.50036211, dom-loss: 75.97472113, src-aux-loss: 96.98056519, tar-aux-loss: 94.52485275
Epoch: [17 ] train-acc: 0.89892857, dom-acc: 0.70151786, val-acc: 0.90500000, val_loss: 0.27356648
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 299.91821742, sen-loss: 31.13832046, dom-loss: 76.10475451, src-aux-loss: 96.38756382, tar-aux-loss: 96.28757870
Epoch: [18 ] train-acc: 0.89946429, dom-acc: 0.69794643, val-acc: 0.90250000, val_loss: 0.27123535
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 297.77806282, sen-loss: 30.95902672, dom-loss: 76.42650694, src-aux-loss: 96.01210111, tar-aux-loss: 94.38042885
Epoch: [19 ] train-acc: 0.89767857, dom-acc: 0.69508929, val-acc: 0.90000000, val_loss: 0.27297503
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 297.27099085, sen-loss: 30.53845602, dom-loss: 76.70570856, src-aux-loss: 95.40533596, tar-aux-loss: 94.62149048
Epoch: [20 ] train-acc: 0.90160714, dom-acc: 0.69000000, val-acc: 0.90000000, val_loss: 0.26775560
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 295.26081300, sen-loss: 30.15505540, dom-loss: 76.57803285, src-aux-loss: 94.98265660, tar-aux-loss: 93.54506785
Epoch: [21 ] train-acc: 0.90089286, dom-acc: 0.68205357, val-acc: 0.89500000, val_loss: 0.26654410
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 296.50806451, sen-loss: 29.98106137, dom-loss: 77.13410437, src-aux-loss: 94.47786403, tar-aux-loss: 94.91503513
Epoch: [22 ] train-acc: 0.90321429, dom-acc: 0.68250000, val-acc: 0.90000000, val_loss: 0.26622370
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 293.23471522, sen-loss: 29.63126161, dom-loss: 76.86902922, src-aux-loss: 93.82965177, tar-aux-loss: 92.90477306
Epoch: [23 ] train-acc: 0.89982143, dom-acc: 0.68312500, val-acc: 0.90750000, val_loss: 0.26952046
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 294.37025952, sen-loss: 29.35477414, dom-loss: 77.15258062, src-aux-loss: 93.78451627, tar-aux-loss: 94.07838744
Epoch: [24 ] train-acc: 0.90196429, dom-acc: 0.67669643, val-acc: 0.90500000, val_loss: 0.26627687
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 292.84768772, sen-loss: 28.87972988, dom-loss: 76.95275152, src-aux-loss: 93.08214074, tar-aux-loss: 93.93306506
Epoch: [25 ] train-acc: 0.90375000, dom-acc: 0.67491071, val-acc: 0.90000000, val_loss: 0.26159433
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 291.78396559, sen-loss: 28.61373810, dom-loss: 77.40639144, src-aux-loss: 92.61400408, tar-aux-loss: 93.14983046
Epoch: [26 ] train-acc: 0.90500000, dom-acc: 0.67392857, val-acc: 0.90000000, val_loss: 0.26110768
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 290.05847096, sen-loss: 28.47550426, dom-loss: 77.48584402, src-aux-loss: 92.16351873, tar-aux-loss: 91.93360269
Epoch: [27 ] train-acc: 0.90517857, dom-acc: 0.66928571, val-acc: 0.90250000, val_loss: 0.26444569
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.24294686, sen-loss: 28.14468496, dom-loss: 77.20203930, src-aux-loss: 91.78185004, tar-aux-loss: 94.11437172
Epoch: [28 ] train-acc: 0.90607143, dom-acc: 0.67276786, val-acc: 0.90750000, val_loss: 0.26752642
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 290.30555320, sen-loss: 27.84839010, dom-loss: 77.43267405, src-aux-loss: 91.41700947, tar-aux-loss: 93.60748005
Epoch: [29 ] train-acc: 0.90821429, dom-acc: 0.66741071, val-acc: 0.90500000, val_loss: 0.25932223
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.58690619, sen-loss: 27.62408859, dom-loss: 77.20877719, src-aux-loss: 90.94671893, tar-aux-loss: 93.80732143
Epoch: [30 ] train-acc: 0.90089286, dom-acc: 0.67312500, val-acc: 0.90000000, val_loss: 0.28011858
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.63968253, sen-loss: 27.45961548, dom-loss: 77.46485615, src-aux-loss: 90.75314766, tar-aux-loss: 91.96206474
Epoch: [31 ] train-acc: 0.90964286, dom-acc: 0.66750000, val-acc: 0.90500000, val_loss: 0.25735584
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 287.45319271, sen-loss: 27.18121418, dom-loss: 77.46084952, src-aux-loss: 90.26660877, tar-aux-loss: 92.54451942
Epoch: [32 ] train-acc: 0.91053571, dom-acc: 0.66178571, val-acc: 0.90750000, val_loss: 0.25627378
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 287.30835867, sen-loss: 26.90726538, dom-loss: 77.33930659, src-aux-loss: 89.97897547, tar-aux-loss: 93.08281100
Epoch: [33 ] train-acc: 0.90892857, dom-acc: 0.67401786, val-acc: 0.91000000, val_loss: 0.26140600
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 286.61787057, sen-loss: 26.64668759, dom-loss: 77.49295187, src-aux-loss: 89.38058090, tar-aux-loss: 93.09764993
Epoch: [34 ] train-acc: 0.91321429, dom-acc: 0.66357143, val-acc: 0.90750000, val_loss: 0.25760880
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 283.92496562, sen-loss: 26.54457492, dom-loss: 77.13882518, src-aux-loss: 89.18282288, tar-aux-loss: 91.05874449
Epoch: [35 ] train-acc: 0.91321429, dom-acc: 0.67312500, val-acc: 0.90750000, val_loss: 0.26058018
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 284.91252494, sen-loss: 26.29088445, dom-loss: 77.36520237, src-aux-loss: 88.73456985, tar-aux-loss: 92.52186650
Epoch: [36 ] train-acc: 0.91500000, dom-acc: 0.66857143, val-acc: 0.90750000, val_loss: 0.25709912
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 282.71344662, sen-loss: 25.93791147, dom-loss: 76.89413023, src-aux-loss: 87.96963286, tar-aux-loss: 91.91177249
Epoch: [37 ] train-acc: 0.91625000, dom-acc: 0.66821429, val-acc: 0.90250000, val_loss: 0.25327212
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 282.76858664, sen-loss: 25.76471759, dom-loss: 77.22698742, src-aux-loss: 87.74062085, tar-aux-loss: 92.03625983
Epoch: [38 ] train-acc: 0.91607143, dom-acc: 0.67464286, val-acc: 0.91000000, val_loss: 0.25671533
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 281.18315959, sen-loss: 25.44288609, dom-loss: 77.28760374, src-aux-loss: 86.95902538, tar-aux-loss: 91.49364507
Epoch: [39 ] train-acc: 0.91696429, dom-acc: 0.67642857, val-acc: 0.90750000, val_loss: 0.25433484
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 280.10060906, sen-loss: 25.23946285, dom-loss: 77.03088915, src-aux-loss: 86.68455148, tar-aux-loss: 91.14570576
Epoch: [40 ] train-acc: 0.92017857, dom-acc: 0.67973214, val-acc: 0.91000000, val_loss: 0.25554851
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 280.13447332, sen-loss: 24.93629752, dom-loss: 76.76473701, src-aux-loss: 86.37872249, tar-aux-loss: 92.05471539
Epoch: [41 ] train-acc: 0.91964286, dom-acc: 0.68187500, val-acc: 0.91250000, val_loss: 0.25403774
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 279.58646321, sen-loss: 24.71015824, dom-loss: 77.06617188, src-aux-loss: 85.62292171, tar-aux-loss: 92.18721241
Epoch: [42 ] train-acc: 0.92142857, dom-acc: 0.68223214, val-acc: 0.91000000, val_loss: 0.25127444
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 278.36424565, sen-loss: 24.52150350, dom-loss: 76.93310469, src-aux-loss: 85.51219982, tar-aux-loss: 91.39743793
Epoch: [43 ] train-acc: 0.92214286, dom-acc: 0.68464286, val-acc: 0.90750000, val_loss: 0.25708744
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 278.39311600, sen-loss: 24.37981894, dom-loss: 76.57759005, src-aux-loss: 85.27735949, tar-aux-loss: 92.15834874
Epoch: [44 ] train-acc: 0.92392857, dom-acc: 0.68607143, val-acc: 0.91250000, val_loss: 0.25076085
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 275.24474525, sen-loss: 24.02376913, dom-loss: 76.66917843, src-aux-loss: 84.66450316, tar-aux-loss: 89.88729405
Epoch: [45 ] train-acc: 0.92214286, dom-acc: 0.68723214, val-acc: 0.90750000, val_loss: 0.26391777
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 276.39332771, sen-loss: 23.89444918, dom-loss: 76.83963168, src-aux-loss: 84.29404712, tar-aux-loss: 91.36519921
Epoch: [46 ] train-acc: 0.92446429, dom-acc: 0.68776786, val-acc: 0.91000000, val_loss: 0.24986802
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 275.19758844, sen-loss: 23.49590302, dom-loss: 76.69509780, src-aux-loss: 83.49983793, tar-aux-loss: 91.50675046
Epoch: [47 ] train-acc: 0.91982143, dom-acc: 0.67901786, val-acc: 0.90750000, val_loss: 0.25103509
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 273.73433709, sen-loss: 23.32306673, dom-loss: 76.62892681, src-aux-loss: 83.07084674, tar-aux-loss: 90.71149611
Epoch: [48 ] train-acc: 0.92750000, dom-acc: 0.68196429, val-acc: 0.91000000, val_loss: 0.25052863
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 274.46107483, sen-loss: 23.06155021, dom-loss: 76.78782290, src-aux-loss: 82.83044738, tar-aux-loss: 91.78125554
Epoch: [49 ] train-acc: 0.92946429, dom-acc: 0.68776786, val-acc: 0.91000000, val_loss: 0.25135854
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 271.61461329, sen-loss: 23.02339017, dom-loss: 76.64202499, src-aux-loss: 82.10968274, tar-aux-loss: 89.83951598
Epoch: [50 ] train-acc: 0.93053571, dom-acc: 0.68982143, val-acc: 0.91000000, val_loss: 0.25667420
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 272.01883984, sen-loss: 22.66226265, dom-loss: 76.70639658, src-aux-loss: 81.69792271, tar-aux-loss: 90.95225900
Epoch: [51 ] train-acc: 0.93089286, dom-acc: 0.68964286, val-acc: 0.91250000, val_loss: 0.25688437
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_books_HATN.ckpt
Best Epoch: [ 46] best val accuracy: 0.00000000 best val loss: 0.24986802
Testing accuracy: 0.88183333
./work/attentions/dvd_books_train_HATN.txt
./work/attentions/dvd_books_test_HATN.txt
loading data...
source domain:  dvd target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 17009
vocab-size:  85442
['good', 'best', 'great', 'excellent', 'funny', 'enjoyable', 'entertaining', 'better', 'awesome', 'fantastic', 'love', 'classic', 'nice', 'amazing', 'wonderful', 'perfect', 'hilarious', 'brilliant', 'funniest', 'underrated', 'loved', 'favorite', 'superb', 'interesting', 'sad', 'greatest', 'terrific', 'finest', 'real', 'solid', 'easy', 'incredible', 'memorable', 'fine', 'fascinating', 'liked', 'spectacular', 'impressive', 'cute']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'wasted', 'dull', 'waste', 'stupid', 'pathetic', 'unwatchable', 'annoying', 'unfunny', 'worse', 'laughable', 'forgettable', 'predictable', 'decent', 'lousy', 'ruined', 'uninspired', 'pointless', 'slow', 'wrong', 'overrated', 'dismal', 'sucks', 'sucked', 'silly', 'poorly', 'cheap', 'unoriginal', 'ok', 'crappy', 'mediocre', 'lacking', 'atrocious', 'lame', 'defective', 'disgusting', 'frustrating', 'okay', 'ridiculous', 'unconvincing', 'depressing', 'horrendous', 'biased', 'weak', 'disapointed', 'dumbest', 'hated', 'contrived', 'lackluster', 'insipid', 'inferior']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
85442
5600 400 6000 17843 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.87192130, sen-loss: 78.28214473, dom-loss: 79.10364890, src-aux-loss: 129.93885612, tar-aux-loss: 117.54727131
Epoch: [1  ] train-acc: 0.64053571, dom-acc: 0.74446429, val-acc: 0.66000000, val_loss: 0.66589087
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 381.16630912, sen-loss: 72.87651330, dom-loss: 75.11712927, src-aux-loss: 121.88873988, tar-aux-loss: 111.28392518
Epoch: [2  ] train-acc: 0.73125000, dom-acc: 0.85964286, val-acc: 0.75000000, val_loss: 0.61133081
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 364.25660229, sen-loss: 67.05090952, dom-loss: 73.36423421, src-aux-loss: 117.13684535, tar-aux-loss: 106.70461369
Epoch: [3  ] train-acc: 0.77767857, dom-acc: 0.85562500, val-acc: 0.80000000, val_loss: 0.54664719
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 351.54877472, sen-loss: 60.17629677, dom-loss: 72.97221005, src-aux-loss: 113.77799219, tar-aux-loss: 104.62227613
Epoch: [4  ] train-acc: 0.80553571, dom-acc: 0.84419643, val-acc: 0.81000000, val_loss: 0.47123736
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 338.82305670, sen-loss: 52.61487687, dom-loss: 73.18241519, src-aux-loss: 110.60316080, tar-aux-loss: 102.42260516
Epoch: [5  ] train-acc: 0.83767857, dom-acc: 0.82937500, val-acc: 0.85750000, val_loss: 0.40323824
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 328.14907646, sen-loss: 46.46415266, dom-loss: 73.93901855, src-aux-loss: 107.64231563, tar-aux-loss: 100.10359156
Epoch: [6  ] train-acc: 0.85875000, dom-acc: 0.81875000, val-acc: 0.88750000, val_loss: 0.34456891
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 321.93991017, sen-loss: 41.70456597, dom-loss: 74.87246966, src-aux-loss: 105.92410094, tar-aux-loss: 99.43877292
Epoch: [7  ] train-acc: 0.86875000, dom-acc: 0.76321429, val-acc: 0.89750000, val_loss: 0.31758446
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 318.30582452, sen-loss: 38.82433204, dom-loss: 75.94071698, src-aux-loss: 104.37003917, tar-aux-loss: 99.17073458
Epoch: [8  ] train-acc: 0.87750000, dom-acc: 0.69339286, val-acc: 0.90250000, val_loss: 0.30085656
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 315.79970455, sen-loss: 37.08092771, dom-loss: 77.33612382, src-aux-loss: 102.98148209, tar-aux-loss: 98.40117192
Epoch: [9  ] train-acc: 0.88196429, dom-acc: 0.64375000, val-acc: 0.90250000, val_loss: 0.29422975
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 314.43905330, sen-loss: 35.72656062, dom-loss: 78.73595828, src-aux-loss: 101.91162282, tar-aux-loss: 98.06491160
Epoch: [10 ] train-acc: 0.88339286, dom-acc: 0.57428571, val-acc: 0.89000000, val_loss: 0.28678927
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 311.34042072, sen-loss: 34.75599766, dom-loss: 79.12783486, src-aux-loss: 100.78928608, tar-aux-loss: 96.66730171
Epoch: [11 ] train-acc: 0.88589286, dom-acc: 0.56107143, val-acc: 0.89250000, val_loss: 0.28333387
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 311.00596404, sen-loss: 34.17596413, dom-loss: 79.71785235, src-aux-loss: 100.21189272, tar-aux-loss: 96.90025634
Epoch: [12 ] train-acc: 0.88714286, dom-acc: 0.55223214, val-acc: 0.90000000, val_loss: 0.29167753
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 309.17621589, sen-loss: 33.41976415, dom-loss: 79.91089040, src-aux-loss: 99.65018672, tar-aux-loss: 96.19537419
Epoch: [13 ] train-acc: 0.89071429, dom-acc: 0.53910714, val-acc: 0.90000000, val_loss: 0.28290027
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 308.35860705, sen-loss: 32.93239915, dom-loss: 79.73359317, src-aux-loss: 98.77539271, tar-aux-loss: 96.91722333
Epoch: [14 ] train-acc: 0.89125000, dom-acc: 0.53866071, val-acc: 0.90000000, val_loss: 0.27786896
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 305.89304781, sen-loss: 32.38771053, dom-loss: 79.27364403, src-aux-loss: 98.27267247, tar-aux-loss: 95.95902216
Epoch: [15 ] train-acc: 0.89053571, dom-acc: 0.55937500, val-acc: 0.90000000, val_loss: 0.28332472
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 304.56897712, sen-loss: 32.07489985, dom-loss: 78.86481893, src-aux-loss: 97.83092493, tar-aux-loss: 95.79833400
Epoch: [16 ] train-acc: 0.89732143, dom-acc: 0.57482143, val-acc: 0.90250000, val_loss: 0.27262056
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 301.97504592, sen-loss: 31.50053929, dom-loss: 78.47714043, src-aux-loss: 96.92280692, tar-aux-loss: 95.07455790
Epoch: [17 ] train-acc: 0.89607143, dom-acc: 0.59464286, val-acc: 0.90000000, val_loss: 0.27304527
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 301.58210969, sen-loss: 31.14709812, dom-loss: 77.96183962, src-aux-loss: 96.49730730, tar-aux-loss: 95.97586292
Epoch: [18 ] train-acc: 0.89928571, dom-acc: 0.61767857, val-acc: 0.90000000, val_loss: 0.27037323
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 299.72302747, sen-loss: 30.97345581, dom-loss: 77.63245171, src-aux-loss: 96.02408123, tar-aux-loss: 95.09303784
Epoch: [19 ] train-acc: 0.89607143, dom-acc: 0.61937500, val-acc: 0.90500000, val_loss: 0.27329087
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 298.17159033, sen-loss: 30.59944603, dom-loss: 77.56759512, src-aux-loss: 95.50481820, tar-aux-loss: 94.49973279
Epoch: [20 ] train-acc: 0.90214286, dom-acc: 0.62383929, val-acc: 0.89500000, val_loss: 0.26563638
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 296.00395608, sen-loss: 30.22519569, dom-loss: 77.51428217, src-aux-loss: 94.97479248, tar-aux-loss: 93.28968638
Epoch: [21 ] train-acc: 0.90196429, dom-acc: 0.63294643, val-acc: 0.89500000, val_loss: 0.26449618
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 296.97333097, sen-loss: 30.03800838, dom-loss: 77.51063865, src-aux-loss: 94.59103328, tar-aux-loss: 94.83365059
Epoch: [22 ] train-acc: 0.90250000, dom-acc: 0.62276786, val-acc: 0.89750000, val_loss: 0.26535523
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 294.80107665, sen-loss: 29.70354373, dom-loss: 77.51019931, src-aux-loss: 93.87392241, tar-aux-loss: 93.71341234
Epoch: [23 ] train-acc: 0.89928571, dom-acc: 0.63294643, val-acc: 0.90500000, val_loss: 0.26992151
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 295.03731704, sen-loss: 29.38932829, dom-loss: 77.58371043, src-aux-loss: 93.87017202, tar-aux-loss: 94.19410771
Epoch: [24 ] train-acc: 0.90410714, dom-acc: 0.61580357, val-acc: 0.90000000, val_loss: 0.26241177
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 294.53979874, sen-loss: 28.99902576, dom-loss: 78.00095052, src-aux-loss: 93.17617428, tar-aux-loss: 94.36365002
Epoch: [25 ] train-acc: 0.90285714, dom-acc: 0.60616071, val-acc: 0.89500000, val_loss: 0.25894964
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.61216784, sen-loss: 28.70706137, dom-loss: 78.15049672, src-aux-loss: 92.71220267, tar-aux-loss: 94.04240769
Epoch: [26 ] train-acc: 0.90500000, dom-acc: 0.59196429, val-acc: 0.90000000, val_loss: 0.25908294
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 291.48473525, sen-loss: 28.58199646, dom-loss: 78.23624563, src-aux-loss: 92.29248655, tar-aux-loss: 92.37400728
Epoch: [27 ] train-acc: 0.90446429, dom-acc: 0.57044643, val-acc: 0.90750000, val_loss: 0.26223630
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.40374303, sen-loss: 28.19500967, dom-loss: 78.46610630, src-aux-loss: 91.84148270, tar-aux-loss: 92.90114486
Epoch: [28 ] train-acc: 0.90089286, dom-acc: 0.57303571, val-acc: 0.91000000, val_loss: 0.27153027
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 292.13719440, sen-loss: 27.97145990, dom-loss: 78.50830126, src-aux-loss: 91.42149132, tar-aux-loss: 94.23594141
Epoch: [29 ] train-acc: 0.90767857, dom-acc: 0.56651786, val-acc: 0.90250000, val_loss: 0.25632372
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.91417837, sen-loss: 27.74041816, dom-loss: 78.51866210, src-aux-loss: 91.20885456, tar-aux-loss: 93.44624376
Epoch: [30 ] train-acc: 0.90267857, dom-acc: 0.58008929, val-acc: 0.91250000, val_loss: 0.27290508
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 288.95158315, sen-loss: 27.49346720, dom-loss: 78.59858203, src-aux-loss: 90.77577406, tar-aux-loss: 92.08376050
Epoch: [31 ] train-acc: 0.90821429, dom-acc: 0.56830357, val-acc: 0.90500000, val_loss: 0.25776353
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.78897119, sen-loss: 27.31731670, dom-loss: 78.43116057, src-aux-loss: 90.43781447, tar-aux-loss: 92.60267866
Epoch: [32 ] train-acc: 0.90964286, dom-acc: 0.57401786, val-acc: 0.89250000, val_loss: 0.25325820
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 288.25803351, sen-loss: 26.98969300, dom-loss: 78.38054866, src-aux-loss: 90.20541769, tar-aux-loss: 92.68237346
Epoch: [33 ] train-acc: 0.90892857, dom-acc: 0.59339286, val-acc: 0.90750000, val_loss: 0.26011831
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 288.21860194, sen-loss: 26.71110208, dom-loss: 78.29294997, src-aux-loss: 89.54395425, tar-aux-loss: 93.67059553
Epoch: [34 ] train-acc: 0.91178571, dom-acc: 0.56446429, val-acc: 0.89750000, val_loss: 0.25352296
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 286.06328034, sen-loss: 26.52441426, dom-loss: 78.00197715, src-aux-loss: 89.33406597, tar-aux-loss: 92.20282376
Epoch: [35 ] train-acc: 0.91160714, dom-acc: 0.60883929, val-acc: 0.90500000, val_loss: 0.25884640
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 285.49459147, sen-loss: 26.27794248, dom-loss: 77.90141582, src-aux-loss: 88.82386559, tar-aux-loss: 92.49136698
Epoch: [36 ] train-acc: 0.91428571, dom-acc: 0.63500000, val-acc: 0.90250000, val_loss: 0.25222537
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 282.96807432, sen-loss: 26.04922777, dom-loss: 77.65833187, src-aux-loss: 88.19698644, tar-aux-loss: 91.06352836
Epoch: [37 ] train-acc: 0.91375000, dom-acc: 0.64375000, val-acc: 0.89750000, val_loss: 0.25065309
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 284.32360601, sen-loss: 25.87470036, dom-loss: 77.40954542, src-aux-loss: 88.12991840, tar-aux-loss: 92.90944260
Epoch: [38 ] train-acc: 0.91660714, dom-acc: 0.64151786, val-acc: 0.90500000, val_loss: 0.25471851
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 282.64951205, sen-loss: 25.50033100, dom-loss: 77.39075327, src-aux-loss: 87.43909520, tar-aux-loss: 92.31933188
Epoch: [39 ] train-acc: 0.91642857, dom-acc: 0.65750000, val-acc: 0.90250000, val_loss: 0.24926917
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 280.12582231, sen-loss: 25.32973093, dom-loss: 77.31731832, src-aux-loss: 87.09529573, tar-aux-loss: 90.38347942
Epoch: [40 ] train-acc: 0.91696429, dom-acc: 0.64651786, val-acc: 0.90000000, val_loss: 0.24972284
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 280.69364095, sen-loss: 25.06117790, dom-loss: 77.22385329, src-aux-loss: 86.59815413, tar-aux-loss: 91.81045586
Epoch: [41 ] train-acc: 0.92107143, dom-acc: 0.67026786, val-acc: 0.90750000, val_loss: 0.25215632
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 280.00584197, sen-loss: 24.77324894, dom-loss: 77.33138216, src-aux-loss: 86.05509549, tar-aux-loss: 91.84611505
Epoch: [42 ] train-acc: 0.92000000, dom-acc: 0.65214286, val-acc: 0.90500000, val_loss: 0.24889278
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 279.04424095, sen-loss: 24.63053307, dom-loss: 77.39338470, src-aux-loss: 85.80020285, tar-aux-loss: 91.22012049
Epoch: [43 ] train-acc: 0.92339286, dom-acc: 0.62169643, val-acc: 0.90500000, val_loss: 0.25224617
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 279.56074739, sen-loss: 24.55422544, dom-loss: 77.48853999, src-aux-loss: 85.75997472, tar-aux-loss: 91.75800675
Epoch: [44 ] train-acc: 0.92392857, dom-acc: 0.62321429, val-acc: 0.90750000, val_loss: 0.24657989
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 277.71261787, sen-loss: 24.08346207, dom-loss: 77.58959132, src-aux-loss: 85.09742928, tar-aux-loss: 90.94213516
Epoch: [45 ] train-acc: 0.92392857, dom-acc: 0.62339286, val-acc: 0.91250000, val_loss: 0.25752977
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 278.82146537, sen-loss: 23.99682412, dom-loss: 77.68963331, src-aux-loss: 84.56995323, tar-aux-loss: 92.56505489
Epoch: [46 ] train-acc: 0.92500000, dom-acc: 0.58294643, val-acc: 0.91000000, val_loss: 0.24708691
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 275.48847747, sen-loss: 23.60729410, dom-loss: 77.73222864, src-aux-loss: 84.04137808, tar-aux-loss: 90.10757720
Epoch: [47 ] train-acc: 0.91982143, dom-acc: 0.59669643, val-acc: 0.91000000, val_loss: 0.24715330
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 276.45788193, sen-loss: 23.46236216, dom-loss: 77.80960965, src-aux-loss: 83.78337249, tar-aux-loss: 91.40253657
Epoch: [48 ] train-acc: 0.92589286, dom-acc: 0.58026786, val-acc: 0.90500000, val_loss: 0.24663131
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 276.65607142, sen-loss: 23.12935110, dom-loss: 77.89340055, src-aux-loss: 83.36404595, tar-aux-loss: 92.26927465
Epoch: [49 ] train-acc: 0.92803571, dom-acc: 0.57955357, val-acc: 0.90750000, val_loss: 0.24863183
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_electronics_HATN.ckpt
Best Epoch: [ 44] best val accuracy: 0.00000000 best val loss: 0.24657989
Testing accuracy: 0.86700000
./work/attentions/dvd_electronics_train_HATN.txt
./work/attentions/dvd_electronics_test_HATN.txt
loading data...
source domain:  dvd target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 13856
vocab-size:  80685
['good', 'best', 'great', 'excellent', 'funny', 'enjoyable', 'entertaining', 'better', 'love', 'awesome', 'fantastic', 'classic', 'nice', 'wonderful', 'amazing', 'perfect', 'hilarious', 'brilliant', 'funniest', 'loved', 'underrated', 'favorite', 'superb', 'sad', 'greatest', 'interesting', 'terrific', 'finest', 'real', 'solid', 'easy', 'incredible', 'memorable', 'fine', 'simplistic', 'fascinating', 'liked', 'impressive', 'cute']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'wasted', 'dull', 'waste', 'stupid', 'pathetic', 'worse', 'unwatchable', 'unfunny', 'forgettable', 'predictable', 'annoying', 'lousy', 'laughable', 'decent', 'ruined', 'uninspired', 'slow', 'sucked', 'wrong', 'pointless', 'overrated', 'dismal', 'sucks', 'poorly', 'cheap', 'unoriginal', 'ok', 'silly', 'mediocre', 'crappy', 'defective', 'lacking', 'atrocious', 'lame', 'disgusting', 'hated', 'frustrating', 'contrived', 'unconvincing', 'dreadful', 'depressing', 'horrendous', 'disjointed', 'disapointed', 'dumbest', 'okay', 'ridiculous', 'insipid', 'inferior']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
80685
5600 400 6000 17843 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 401.10570884, sen-loss: 78.27029330, dom-loss: 78.90471780, src-aux-loss: 130.80547833, tar-aux-loss: 113.12521994
Epoch: [1  ] train-acc: 0.63821429, dom-acc: 0.73133929, val-acc: 0.65500000, val_loss: 0.66621661
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 376.13527608, sen-loss: 72.87624776, dom-loss: 75.33228737, src-aux-loss: 122.58196115, tar-aux-loss: 105.34478122
Epoch: [2  ] train-acc: 0.72750000, dom-acc: 0.84232143, val-acc: 0.75000000, val_loss: 0.61133593
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 358.58751488, sen-loss: 67.04204214, dom-loss: 73.71018630, src-aux-loss: 117.56075615, tar-aux-loss: 100.27453041
Epoch: [3  ] train-acc: 0.77875000, dom-acc: 0.84464286, val-acc: 0.79250000, val_loss: 0.54626912
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 345.41331029, sen-loss: 60.13835281, dom-loss: 73.30742955, src-aux-loss: 113.95714700, tar-aux-loss: 98.01038092
Epoch: [4  ] train-acc: 0.80428571, dom-acc: 0.81812500, val-acc: 0.81000000, val_loss: 0.47257707
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 333.38471985, sen-loss: 52.61855757, dom-loss: 73.63156617, src-aux-loss: 110.67362034, tar-aux-loss: 96.46097672
Epoch: [5  ] train-acc: 0.83857143, dom-acc: 0.78285714, val-acc: 0.85250000, val_loss: 0.40293339
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 323.19915867, sen-loss: 46.47857437, dom-loss: 73.89196396, src-aux-loss: 107.92329776, tar-aux-loss: 94.90532291
Epoch: [6  ] train-acc: 0.85392857, dom-acc: 0.73223214, val-acc: 0.88750000, val_loss: 0.34620011
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 314.86738539, sen-loss: 41.72810319, dom-loss: 74.70730656, src-aux-loss: 105.97693759, tar-aux-loss: 92.45503795
Epoch: [7  ] train-acc: 0.87107143, dom-acc: 0.69455357, val-acc: 0.90000000, val_loss: 0.31874043
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 312.91625929, sen-loss: 39.02248044, dom-loss: 75.83069956, src-aux-loss: 104.38665473, tar-aux-loss: 93.67642468
Epoch: [8  ] train-acc: 0.87553571, dom-acc: 0.66589286, val-acc: 0.90000000, val_loss: 0.30551097
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 308.51437569, sen-loss: 37.08146894, dom-loss: 76.54331011, src-aux-loss: 102.80837387, tar-aux-loss: 92.08122236
Epoch: [9  ] train-acc: 0.88214286, dom-acc: 0.64723214, val-acc: 0.90250000, val_loss: 0.29602155
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 307.03164792, sen-loss: 35.70888761, dom-loss: 77.95733380, src-aux-loss: 101.86447060, tar-aux-loss: 91.50095397
Epoch: [10 ] train-acc: 0.88464286, dom-acc: 0.62482143, val-acc: 0.89500000, val_loss: 0.28653392
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 304.87768316, sen-loss: 34.78515600, dom-loss: 77.90506661, src-aux-loss: 100.65643299, tar-aux-loss: 91.53102911
Epoch: [11 ] train-acc: 0.88428571, dom-acc: 0.61642857, val-acc: 0.89000000, val_loss: 0.28311616
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 303.33240938, sen-loss: 34.15217417, dom-loss: 78.24522376, src-aux-loss: 99.97528583, tar-aux-loss: 90.95972598
Epoch: [12 ] train-acc: 0.88375000, dom-acc: 0.62178571, val-acc: 0.89250000, val_loss: 0.29356742
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 301.86265063, sen-loss: 33.45964633, dom-loss: 78.37985688, src-aux-loss: 99.36212730, tar-aux-loss: 90.66101998
Epoch: [13 ] train-acc: 0.89142857, dom-acc: 0.61607143, val-acc: 0.90750000, val_loss: 0.27885288
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 299.96360850, sen-loss: 32.94391908, dom-loss: 78.27958375, src-aux-loss: 98.63003480, tar-aux-loss: 90.11007088
Epoch: [14 ] train-acc: 0.89089286, dom-acc: 0.62571429, val-acc: 0.90000000, val_loss: 0.27900067
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 298.55192161, sen-loss: 32.38294125, dom-loss: 77.81893122, src-aux-loss: 97.77957267, tar-aux-loss: 90.57047552
Epoch: [15 ] train-acc: 0.89142857, dom-acc: 0.63839286, val-acc: 0.90250000, val_loss: 0.27812308
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 297.68594265, sen-loss: 32.06383491, dom-loss: 77.74906230, src-aux-loss: 97.40751326, tar-aux-loss: 90.46553338
Epoch: [16 ] train-acc: 0.89428571, dom-acc: 0.64133929, val-acc: 0.90500000, val_loss: 0.27228433
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 293.70069504, sen-loss: 31.45179909, dom-loss: 77.30432504, src-aux-loss: 96.49558556, tar-aux-loss: 88.44898421
Epoch: [17 ] train-acc: 0.89714286, dom-acc: 0.65428571, val-acc: 0.90500000, val_loss: 0.27100128
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 293.50388002, sen-loss: 31.09545927, dom-loss: 76.75619864, src-aux-loss: 95.90757394, tar-aux-loss: 89.74464786
Epoch: [18 ] train-acc: 0.89910714, dom-acc: 0.65160714, val-acc: 0.91000000, val_loss: 0.26776859
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 291.91557360, sen-loss: 31.00117219, dom-loss: 76.88420796, src-aux-loss: 95.65608007, tar-aux-loss: 88.37411398
Epoch: [19 ] train-acc: 0.89571429, dom-acc: 0.65732143, val-acc: 0.90500000, val_loss: 0.27036121
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 292.09867787, sen-loss: 30.59172915, dom-loss: 76.98189294, src-aux-loss: 95.05367720, tar-aux-loss: 89.47137779
Epoch: [20 ] train-acc: 0.90232143, dom-acc: 0.65928571, val-acc: 0.90000000, val_loss: 0.26291364
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 289.64030886, sen-loss: 30.18377048, dom-loss: 76.79796439, src-aux-loss: 94.52315617, tar-aux-loss: 88.13541776
Epoch: [21 ] train-acc: 0.90250000, dom-acc: 0.65687500, val-acc: 0.90000000, val_loss: 0.26226810
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 288.99074340, sen-loss: 29.93838640, dom-loss: 77.09494764, src-aux-loss: 94.08661807, tar-aux-loss: 87.87079161
Epoch: [22 ] train-acc: 0.90321429, dom-acc: 0.65053571, val-acc: 0.90000000, val_loss: 0.26216364
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 288.12925148, sen-loss: 29.66425909, dom-loss: 76.95308763, src-aux-loss: 93.51491255, tar-aux-loss: 87.99699104
Epoch: [23 ] train-acc: 0.89660714, dom-acc: 0.64830357, val-acc: 0.90750000, val_loss: 0.27154979
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 288.84569216, sen-loss: 29.34668805, dom-loss: 77.47376984, src-aux-loss: 93.25563151, tar-aux-loss: 88.76960260
Epoch: [24 ] train-acc: 0.90410714, dom-acc: 0.63553571, val-acc: 0.90250000, val_loss: 0.25976768
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 285.95064092, sen-loss: 28.88400061, dom-loss: 77.60900581, src-aux-loss: 92.64118612, tar-aux-loss: 86.81644768
Epoch: [25 ] train-acc: 0.90410714, dom-acc: 0.62526786, val-acc: 0.90000000, val_loss: 0.25642917
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 286.60274720, sen-loss: 28.57872320, dom-loss: 77.86154866, src-aux-loss: 92.19228899, tar-aux-loss: 87.97018707
Epoch: [26 ] train-acc: 0.90500000, dom-acc: 0.62017857, val-acc: 0.90500000, val_loss: 0.25607222
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 284.68617034, sen-loss: 28.55369661, dom-loss: 78.21984941, src-aux-loss: 91.65528852, tar-aux-loss: 86.25733531
Epoch: [27 ] train-acc: 0.90285714, dom-acc: 0.61437500, val-acc: 0.90500000, val_loss: 0.26224667
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 286.41988683, sen-loss: 28.17229515, dom-loss: 78.37883091, src-aux-loss: 91.25581062, tar-aux-loss: 88.61294794
Epoch: [28 ] train-acc: 0.90178571, dom-acc: 0.60883929, val-acc: 0.90500000, val_loss: 0.26962888
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 284.04723120, sen-loss: 27.84871592, dom-loss: 78.58067697, src-aux-loss: 90.85005617, tar-aux-loss: 86.76778263
Epoch: [29 ] train-acc: 0.90714286, dom-acc: 0.59598214, val-acc: 0.90750000, val_loss: 0.25431141
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 285.24867201, sen-loss: 27.63857824, dom-loss: 78.45456910, src-aux-loss: 90.48453563, tar-aux-loss: 88.67098892
Epoch: [30 ] train-acc: 0.90178571, dom-acc: 0.60982143, val-acc: 0.90750000, val_loss: 0.27310333
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 282.46157026, sen-loss: 27.46060532, dom-loss: 78.59512264, src-aux-loss: 90.03700143, tar-aux-loss: 86.36884117
Epoch: [31 ] train-acc: 0.90982143, dom-acc: 0.60303571, val-acc: 0.90750000, val_loss: 0.25333142
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 281.46412778, sen-loss: 27.22495605, dom-loss: 78.36834216, src-aux-loss: 89.90047234, tar-aux-loss: 85.97035694
Epoch: [32 ] train-acc: 0.91125000, dom-acc: 0.60294643, val-acc: 0.90250000, val_loss: 0.24991672
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 280.88898110, sen-loss: 26.89290032, dom-loss: 78.22211736, src-aux-loss: 89.36885774, tar-aux-loss: 86.40510350
Epoch: [33 ] train-acc: 0.90875000, dom-acc: 0.62178571, val-acc: 0.90750000, val_loss: 0.25327253
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 280.38712358, sen-loss: 26.65739335, dom-loss: 78.17537528, src-aux-loss: 88.84490573, tar-aux-loss: 86.70945072
Epoch: [34 ] train-acc: 0.91250000, dom-acc: 0.61794643, val-acc: 0.90750000, val_loss: 0.24975783
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 279.55983591, sen-loss: 26.47279681, dom-loss: 77.78290927, src-aux-loss: 88.76384282, tar-aux-loss: 86.54028547
Epoch: [35 ] train-acc: 0.91303571, dom-acc: 0.64178571, val-acc: 0.90750000, val_loss: 0.25255629
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 278.18497062, sen-loss: 26.25584172, dom-loss: 77.45305908, src-aux-loss: 88.07624090, tar-aux-loss: 86.39982909
Epoch: [36 ] train-acc: 0.91625000, dom-acc: 0.64758929, val-acc: 0.90750000, val_loss: 0.24946544
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 276.69911075, sen-loss: 25.94707404, dom-loss: 77.13983560, src-aux-loss: 87.47001511, tar-aux-loss: 86.14218670
Epoch: [37 ] train-acc: 0.91410714, dom-acc: 0.65500000, val-acc: 0.90250000, val_loss: 0.24692585
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 275.91891479, sen-loss: 25.72118246, dom-loss: 76.81623524, src-aux-loss: 87.29954618, tar-aux-loss: 86.08195037
Epoch: [38 ] train-acc: 0.91625000, dom-acc: 0.67357143, val-acc: 0.90750000, val_loss: 0.25293112
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 275.40121031, sen-loss: 25.41679749, dom-loss: 76.83184481, src-aux-loss: 86.79190850, tar-aux-loss: 86.36066002
Epoch: [39 ] train-acc: 0.91750000, dom-acc: 0.68455357, val-acc: 0.91250000, val_loss: 0.24542677
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 272.65397120, sen-loss: 25.24180940, dom-loss: 76.60144490, src-aux-loss: 86.26637667, tar-aux-loss: 84.54433787
Epoch: [40 ] train-acc: 0.91892857, dom-acc: 0.68928571, val-acc: 0.90500000, val_loss: 0.24461530
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 272.10837150, sen-loss: 25.10659654, dom-loss: 76.47783017, src-aux-loss: 85.95991236, tar-aux-loss: 84.56403297
Epoch: [41 ] train-acc: 0.92071429, dom-acc: 0.67875000, val-acc: 0.91000000, val_loss: 0.25178841
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 273.27555990, sen-loss: 24.69280121, dom-loss: 76.66030133, src-aux-loss: 85.20176834, tar-aux-loss: 86.72068751
Epoch: [42 ] train-acc: 0.92035714, dom-acc: 0.67910714, val-acc: 0.90750000, val_loss: 0.24528381
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 270.89769554, sen-loss: 24.51724114, dom-loss: 76.79478675, src-aux-loss: 85.09105790, tar-aux-loss: 84.49460894
Epoch: [43 ] train-acc: 0.92035714, dom-acc: 0.66955357, val-acc: 0.90750000, val_loss: 0.24808675
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 273.54390454, sen-loss: 24.38571017, dom-loss: 77.06463528, src-aux-loss: 84.95676172, tar-aux-loss: 87.13679636
Epoch: [44 ] train-acc: 0.92303571, dom-acc: 0.65803571, val-acc: 0.90750000, val_loss: 0.24350920
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 269.44287276, sen-loss: 24.04555922, dom-loss: 77.11804563, src-aux-loss: 84.09819651, tar-aux-loss: 84.18107170
Epoch: [45 ] train-acc: 0.92250000, dom-acc: 0.64562500, val-acc: 0.91250000, val_loss: 0.25572735
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 270.65319669, sen-loss: 23.90080810, dom-loss: 77.56337368, src-aux-loss: 83.76330814, tar-aux-loss: 85.42570585
Epoch: [46 ] train-acc: 0.92267857, dom-acc: 0.61750000, val-acc: 0.91000000, val_loss: 0.24326494
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 270.12815547, sen-loss: 23.51488788, dom-loss: 77.75964034, src-aux-loss: 83.17837602, tar-aux-loss: 85.67525047
Epoch: [47 ] train-acc: 0.91857143, dom-acc: 0.58383929, val-acc: 0.91000000, val_loss: 0.24564412
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 268.78579926, sen-loss: 23.40951211, dom-loss: 78.02395797, src-aux-loss: 82.83629185, tar-aux-loss: 84.51603740
Epoch: [48 ] train-acc: 0.92660714, dom-acc: 0.57660714, val-acc: 0.91500000, val_loss: 0.24313425
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 269.99804759, sen-loss: 23.04092554, dom-loss: 78.41255468, src-aux-loss: 82.38827610, tar-aux-loss: 86.15629280
Epoch: [49 ] train-acc: 0.92767857, dom-acc: 0.54857143, val-acc: 0.91000000, val_loss: 0.24711153
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 266.80089688, sen-loss: 22.99287470, dom-loss: 78.51956886, src-aux-loss: 81.79319137, tar-aux-loss: 83.49526268
Epoch: [50 ] train-acc: 0.92821429, dom-acc: 0.53133929, val-acc: 0.91250000, val_loss: 0.25161842
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 268.68775606, sen-loss: 22.71795758, dom-loss: 78.49748629, src-aux-loss: 81.41457933, tar-aux-loss: 86.05773193
Epoch: [51 ] train-acc: 0.93017857, dom-acc: 0.54116071, val-acc: 0.91000000, val_loss: 0.24949713
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 265.80777240, sen-loss: 22.43549941, dom-loss: 78.40650368, src-aux-loss: 81.00567344, tar-aux-loss: 83.96009451
Epoch: [52 ] train-acc: 0.93160714, dom-acc: 0.58803571, val-acc: 0.91250000, val_loss: 0.24783678
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 265.42285979, sen-loss: 22.34440528, dom-loss: 78.46083009, src-aux-loss: 80.26377922, tar-aux-loss: 84.35384566
Epoch: [53 ] train-acc: 0.93232143, dom-acc: 0.55357143, val-acc: 0.91250000, val_loss: 0.24502361
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_kitchen_HATN.ckpt
Best Epoch: [ 48] best val accuracy: 0.00000000 best val loss: 0.24313425
Testing accuracy: 0.86733333
./work/attentions/dvd_kitchen_train_HATN.txt
./work/attentions/dvd_kitchen_test_HATN.txt
loading data...
source domain:  dvd target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 30180
vocab-size:  91852
['best', 'good', 'great', 'excellent', 'funny', 'enjoyable', 'entertaining', 'love', 'awesome', 'fantastic', 'better', 'classic', 'nice', 'wonderful', 'amazing', 'hilarious', 'brilliant', 'funniest', 'perfect', 'loved', 'underrated', 'favorite', 'interesting', 'superb', 'sad', 'terrific', 'greatest', 'finest', 'real', 'solid', 'incredible', 'easy', 'memorable', 'cute', 'believable', 'fascinating', 'liked']
['worst', 'boring', 'horrible', 'bad', 'awful', 'poor', 'disappointing', 'terrible', 'disappointed', 'dull', 'waste', 'wasted', 'stupid', 'pathetic', 'worse', 'unwatchable', 'forgettable', 'unfunny', 'predictable', 'annoying', 'laughable', 'lousy', 'ruined', 'uninspired', 'decent', 'slow', 'sucked', 'pointless', 'overrated', 'dismal', 'wrong', 'ok', 'silly', 'poorly', 'unoriginal', 'mediocre', 'lacking', 'sucks', 'crappy', 'lame', 'cheap', 'atrocious', 'disgusting', 'hated', 'ridiculous', 'contrived', 'defective', 'unconvincing', 'depressing', 'horrendous', 'biased', 'disjointed', 'disapointed', 'dumbest', 'frustrating', 'wasting', 'insipid', 'dreadful']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
91852
5600 400 6000 17843 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 403.93258619, sen-loss: 78.27746826, dom-loss: 79.40023720, src-aux-loss: 130.79169554, tar-aux-loss: 115.46318495
Epoch: [1  ] train-acc: 0.65428571, dom-acc: 0.46375000, val-acc: 0.67250000, val_loss: 0.66451252
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 384.33644128, sen-loss: 72.63998455, dom-loss: 78.79198712, src-aux-loss: 123.17289650, tar-aux-loss: 109.73157018
Epoch: [2  ] train-acc: 0.73053571, dom-acc: 0.48812500, val-acc: 0.74250000, val_loss: 0.60747492
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 368.82059145, sen-loss: 66.62954903, dom-loss: 78.62589657, src-aux-loss: 118.39977777, tar-aux-loss: 105.16537035
Epoch: [3  ] train-acc: 0.77875000, dom-acc: 0.52803571, val-acc: 0.79750000, val_loss: 0.54122412
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 355.28851461, sen-loss: 59.65381509, dom-loss: 78.41188502, src-aux-loss: 114.87252039, tar-aux-loss: 102.35029411
Epoch: [4  ] train-acc: 0.80946429, dom-acc: 0.58535714, val-acc: 0.81000000, val_loss: 0.46796021
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 341.20906305, sen-loss: 52.31642407, dom-loss: 78.05567724, src-aux-loss: 111.41381085, tar-aux-loss: 99.42315006
Epoch: [5  ] train-acc: 0.84232143, dom-acc: 0.62258929, val-acc: 0.85500000, val_loss: 0.39930192
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 330.31100988, sen-loss: 46.43199047, dom-loss: 77.80148578, src-aux-loss: 108.12356317, tar-aux-loss: 97.95397192
Epoch: [6  ] train-acc: 0.85035714, dom-acc: 0.62517857, val-acc: 0.87750000, val_loss: 0.34727579
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 321.95085454, sen-loss: 42.06361917, dom-loss: 77.72240323, src-aux-loss: 106.38303190, tar-aux-loss: 95.78180093
Epoch: [7  ] train-acc: 0.87053571, dom-acc: 0.62883929, val-acc: 0.90250000, val_loss: 0.32094717
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 317.37776613, sen-loss: 39.36129189, dom-loss: 77.44915074, src-aux-loss: 104.73870623, tar-aux-loss: 95.82861632
Epoch: [8  ] train-acc: 0.87660714, dom-acc: 0.63276786, val-acc: 0.89750000, val_loss: 0.30301067
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 313.27263832, sen-loss: 37.27876472, dom-loss: 77.45046365, src-aux-loss: 103.24486953, tar-aux-loss: 95.29854017
Epoch: [9  ] train-acc: 0.88160714, dom-acc: 0.64142857, val-acc: 0.90250000, val_loss: 0.29337788
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 310.08711791, sen-loss: 35.81076096, dom-loss: 77.58328503, src-aux-loss: 102.45318592, tar-aux-loss: 94.23988670
Epoch: [10 ] train-acc: 0.88375000, dom-acc: 0.64401786, val-acc: 0.90000000, val_loss: 0.28668219
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 306.49213338, sen-loss: 34.84571056, dom-loss: 76.97772616, src-aux-loss: 101.17171091, tar-aux-loss: 93.49698645
Epoch: [11 ] train-acc: 0.88446429, dom-acc: 0.64848214, val-acc: 0.89250000, val_loss: 0.28307903
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 305.23398161, sen-loss: 34.17711996, dom-loss: 76.93464625, src-aux-loss: 100.47968376, tar-aux-loss: 93.64253277
Epoch: [12 ] train-acc: 0.88625000, dom-acc: 0.67008929, val-acc: 0.90500000, val_loss: 0.29055348
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 302.65626621, sen-loss: 33.44413263, dom-loss: 76.83591652, src-aux-loss: 99.92316163, tar-aux-loss: 92.45305580
Epoch: [13 ] train-acc: 0.89142857, dom-acc: 0.66419643, val-acc: 0.90750000, val_loss: 0.27831709
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 302.70673656, sen-loss: 33.07054791, dom-loss: 76.85316437, src-aux-loss: 99.24454659, tar-aux-loss: 93.53847891
Epoch: [14 ] train-acc: 0.89178571, dom-acc: 0.66625000, val-acc: 0.90250000, val_loss: 0.27902824
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 299.89370632, sen-loss: 32.31603050, dom-loss: 76.92830676, src-aux-loss: 98.42820013, tar-aux-loss: 92.22116786
Epoch: [15 ] train-acc: 0.89357143, dom-acc: 0.67187500, val-acc: 0.90250000, val_loss: 0.27813804
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 298.77549052, sen-loss: 31.97493178, dom-loss: 76.92994273, src-aux-loss: 98.01607740, tar-aux-loss: 91.85453951
Epoch: [16 ] train-acc: 0.89714286, dom-acc: 0.66875000, val-acc: 0.90500000, val_loss: 0.27176547
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 296.02266240, sen-loss: 31.47270647, dom-loss: 76.64833444, src-aux-loss: 97.22584510, tar-aux-loss: 90.67577767
Epoch: [17 ] train-acc: 0.89571429, dom-acc: 0.67151786, val-acc: 0.90500000, val_loss: 0.27266341
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 296.51059842, sen-loss: 31.13120659, dom-loss: 76.58454090, src-aux-loss: 96.76719546, tar-aux-loss: 92.02765572
Epoch: [18 ] train-acc: 0.89910714, dom-acc: 0.66803571, val-acc: 0.90250000, val_loss: 0.26895651
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 295.30575371, sen-loss: 31.01356896, dom-loss: 76.71321410, src-aux-loss: 96.31555307, tar-aux-loss: 91.26341558
Epoch: [19 ] train-acc: 0.89767857, dom-acc: 0.66848214, val-acc: 0.90750000, val_loss: 0.27052143
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 294.48893547, sen-loss: 30.58272092, dom-loss: 76.81338847, src-aux-loss: 95.74680436, tar-aux-loss: 91.34602207
Epoch: [20 ] train-acc: 0.90178571, dom-acc: 0.66964286, val-acc: 0.90250000, val_loss: 0.26534855
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 292.13289046, sen-loss: 30.13069218, dom-loss: 76.74509490, src-aux-loss: 95.26301229, tar-aux-loss: 89.99409169
Epoch: [21 ] train-acc: 0.90107143, dom-acc: 0.66919643, val-acc: 0.89750000, val_loss: 0.26463696
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 292.06030393, sen-loss: 29.95311366, dom-loss: 76.71616518, src-aux-loss: 94.66406685, tar-aux-loss: 90.72695798
Epoch: [22 ] train-acc: 0.90357143, dom-acc: 0.67071429, val-acc: 0.90250000, val_loss: 0.26383713
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 289.28141308, sen-loss: 29.58724250, dom-loss: 76.40643728, src-aux-loss: 94.06660092, tar-aux-loss: 89.22113240
Epoch: [23 ] train-acc: 0.90000000, dom-acc: 0.67892857, val-acc: 0.90250000, val_loss: 0.26810867
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 290.79562354, sen-loss: 29.38672810, dom-loss: 76.69761604, src-aux-loss: 93.86384475, tar-aux-loss: 90.84743547
Epoch: [24 ] train-acc: 0.90464286, dom-acc: 0.67017857, val-acc: 0.90250000, val_loss: 0.26223734
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 288.66592503, sen-loss: 28.93041234, dom-loss: 76.62746108, src-aux-loss: 93.14533663, tar-aux-loss: 89.96271384
Epoch: [25 ] train-acc: 0.90482143, dom-acc: 0.66741071, val-acc: 0.90250000, val_loss: 0.25965843
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 287.77825499, sen-loss: 28.63407721, dom-loss: 76.76226664, src-aux-loss: 92.67659509, tar-aux-loss: 89.70531601
Epoch: [26 ] train-acc: 0.90553571, dom-acc: 0.67160714, val-acc: 0.90250000, val_loss: 0.26113561
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 284.73207808, sen-loss: 28.53601803, dom-loss: 76.48020995, src-aux-loss: 92.23622823, tar-aux-loss: 87.47962195
Epoch: [27 ] train-acc: 0.90285714, dom-acc: 0.67151786, val-acc: 0.90250000, val_loss: 0.26719677
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 285.38232422, sen-loss: 28.22828953, dom-loss: 76.46427166, src-aux-loss: 91.75417942, tar-aux-loss: 88.93558449
Epoch: [28 ] train-acc: 0.90160714, dom-acc: 0.68044643, val-acc: 0.90250000, val_loss: 0.27490732
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 285.37907624, sen-loss: 27.91613337, dom-loss: 76.67240447, src-aux-loss: 91.33160573, tar-aux-loss: 89.45893353
Epoch: [29 ] train-acc: 0.90875000, dom-acc: 0.66875000, val-acc: 0.90250000, val_loss: 0.25609124
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 284.79146290, sen-loss: 27.66501822, dom-loss: 76.32808560, src-aux-loss: 90.86016756, tar-aux-loss: 89.93819207
Epoch: [30 ] train-acc: 0.90125000, dom-acc: 0.68339286, val-acc: 0.90500000, val_loss: 0.27440596
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 282.14215469, sen-loss: 27.41017951, dom-loss: 76.61512691, src-aux-loss: 90.57906389, tar-aux-loss: 87.53778547
Epoch: [31 ] train-acc: 0.91035714, dom-acc: 0.67205357, val-acc: 0.90750000, val_loss: 0.25555539
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 282.20677257, sen-loss: 27.24618977, dom-loss: 76.51052999, src-aux-loss: 90.25113165, tar-aux-loss: 88.19892144
Epoch: [32 ] train-acc: 0.90946429, dom-acc: 0.66767857, val-acc: 0.90750000, val_loss: 0.25439161
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 281.76815009, sen-loss: 26.96443589, dom-loss: 76.20889860, src-aux-loss: 89.84938663, tar-aux-loss: 88.74543011
Epoch: [33 ] train-acc: 0.91178571, dom-acc: 0.67526786, val-acc: 0.90500000, val_loss: 0.25516656
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 281.11240077, sen-loss: 26.79357992, dom-loss: 76.37778771, src-aux-loss: 89.20915890, tar-aux-loss: 88.73187441
Epoch: [34 ] train-acc: 0.91196429, dom-acc: 0.66839286, val-acc: 0.90250000, val_loss: 0.25460652
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 279.05173731, sen-loss: 26.52918576, dom-loss: 76.57733941, src-aux-loss: 88.78393739, tar-aux-loss: 87.16127765
Epoch: [35 ] train-acc: 0.91321429, dom-acc: 0.67982143, val-acc: 0.90750000, val_loss: 0.25797987
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 278.64797664, sen-loss: 26.30938492, dom-loss: 76.52488452, src-aux-loss: 88.10622782, tar-aux-loss: 87.70748127
Epoch: [36 ] train-acc: 0.91428571, dom-acc: 0.67562500, val-acc: 0.90500000, val_loss: 0.25432974
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 276.02966809, sen-loss: 26.01295420, dom-loss: 76.30448741, src-aux-loss: 87.46462351, tar-aux-loss: 86.24760276
Epoch: [37 ] train-acc: 0.91517857, dom-acc: 0.66294643, val-acc: 0.90500000, val_loss: 0.25078344
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 277.99815488, sen-loss: 25.81323069, dom-loss: 76.55213130, src-aux-loss: 87.42340016, tar-aux-loss: 88.20939428
Epoch: [38 ] train-acc: 0.91589286, dom-acc: 0.66767857, val-acc: 0.91000000, val_loss: 0.25395089
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 275.51002884, sen-loss: 25.45797323, dom-loss: 76.61027217, src-aux-loss: 86.62063402, tar-aux-loss: 86.82114989
Epoch: [39 ] train-acc: 0.91696429, dom-acc: 0.66785714, val-acc: 0.90500000, val_loss: 0.25008124
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 273.75565481, sen-loss: 25.25716935, dom-loss: 76.44544172, src-aux-loss: 86.17173213, tar-aux-loss: 85.88131160
Epoch: [40 ] train-acc: 0.91785714, dom-acc: 0.66803571, val-acc: 0.90250000, val_loss: 0.24978159
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 273.33630514, sen-loss: 25.02148453, dom-loss: 76.06922686, src-aux-loss: 85.54100543, tar-aux-loss: 86.70458823
Epoch: [41 ] train-acc: 0.92017857, dom-acc: 0.66937500, val-acc: 0.91000000, val_loss: 0.25320181
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 272.88040757, sen-loss: 24.78832345, dom-loss: 76.63531822, src-aux-loss: 84.95436782, tar-aux-loss: 86.50239807
Epoch: [42 ] train-acc: 0.92000000, dom-acc: 0.66848214, val-acc: 0.90250000, val_loss: 0.24969266
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 271.97864771, sen-loss: 24.66638225, dom-loss: 76.33009613, src-aux-loss: 84.66048944, tar-aux-loss: 86.32168001
Epoch: [43 ] train-acc: 0.92232143, dom-acc: 0.66651786, val-acc: 0.90500000, val_loss: 0.25259429
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 273.17260265, sen-loss: 24.49071417, dom-loss: 76.51258475, src-aux-loss: 84.57486737, tar-aux-loss: 87.59443611
Epoch: [44 ] train-acc: 0.92267857, dom-acc: 0.66767857, val-acc: 0.90250000, val_loss: 0.24766158
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 269.26902473, sen-loss: 24.05566034, dom-loss: 76.25550574, src-aux-loss: 83.50409752, tar-aux-loss: 85.45376235
Epoch: [45 ] train-acc: 0.92125000, dom-acc: 0.67625000, val-acc: 0.91000000, val_loss: 0.25641429
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 269.50915956, sen-loss: 23.97924732, dom-loss: 76.34758818, src-aux-loss: 83.15668306, tar-aux-loss: 86.02564085
Epoch: [46 ] train-acc: 0.92428571, dom-acc: 0.66410714, val-acc: 0.90500000, val_loss: 0.24807122
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 268.56379104, sen-loss: 23.56999294, dom-loss: 76.41242677, src-aux-loss: 82.52086794, tar-aux-loss: 86.06050324
Epoch: [47 ] train-acc: 0.92232143, dom-acc: 0.65946429, val-acc: 0.90000000, val_loss: 0.24841289
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 266.66832113, sen-loss: 23.36091588, dom-loss: 76.24665970, src-aux-loss: 82.03156072, tar-aux-loss: 85.02918541
Epoch: [48 ] train-acc: 0.92500000, dom-acc: 0.66223214, val-acc: 0.90500000, val_loss: 0.24728830
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 266.70940566, sen-loss: 23.07525633, dom-loss: 76.38404185, src-aux-loss: 81.50363591, tar-aux-loss: 85.74647164
Epoch: [49 ] train-acc: 0.92946429, dom-acc: 0.67044643, val-acc: 0.90500000, val_loss: 0.24941345
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 263.93023181, sen-loss: 22.98655814, dom-loss: 76.13338161, src-aux-loss: 80.59128726, tar-aux-loss: 84.21900165
Epoch: [50 ] train-acc: 0.92750000, dom-acc: 0.67151786, val-acc: 0.91000000, val_loss: 0.25504979
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 265.23082376, sen-loss: 22.71349864, dom-loss: 76.49490225, src-aux-loss: 80.20927703, tar-aux-loss: 85.81314617
Epoch: [51 ] train-acc: 0.92857143, dom-acc: 0.67410714, val-acc: 0.91250000, val_loss: 0.25483409
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 264.20435452, sen-loss: 22.35333999, dom-loss: 76.29128903, src-aux-loss: 79.64138091, tar-aux-loss: 85.91834372
Epoch: [52 ] train-acc: 0.93053571, dom-acc: 0.67669643, val-acc: 0.90250000, val_loss: 0.24969390
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 262.42718649, sen-loss: 22.30455743, dom-loss: 76.40159160, src-aux-loss: 78.89310604, tar-aux-loss: 84.82793289
Epoch: [53 ] train-acc: 0.93160714, dom-acc: 0.66794643, val-acc: 0.90250000, val_loss: 0.24672864
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [54 ] loss: 260.94487095, sen-loss: 21.91970753, dom-loss: 76.29128128, src-aux-loss: 78.38766184, tar-aux-loss: 84.34622097
Epoch: [54 ] train-acc: 0.93232143, dom-acc: 0.66526786, val-acc: 0.90500000, val_loss: 0.24834622
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [55 ] loss: 261.10137296, sen-loss: 21.92343717, dom-loss: 76.36321229, src-aux-loss: 77.95707908, tar-aux-loss: 84.85764396
Epoch: [55 ] train-acc: 0.93303571, dom-acc: 0.66205357, val-acc: 0.90500000, val_loss: 0.25624168
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [56 ] loss: 259.15718365, sen-loss: 21.51496523, dom-loss: 76.29781371, src-aux-loss: 77.29739261, tar-aux-loss: 84.04701263
Epoch: [56 ] train-acc: 0.93232143, dom-acc: 0.66464286, val-acc: 0.90750000, val_loss: 0.24409372
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [57 ] loss: 260.50255716, sen-loss: 21.30886737, dom-loss: 76.34573931, src-aux-loss: 76.60593617, tar-aux-loss: 86.24201268
Epoch: [57 ] train-acc: 0.93750000, dom-acc: 0.66625000, val-acc: 0.90250000, val_loss: 0.24926087
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [58 ] loss: 256.00850618, sen-loss: 21.03897883, dom-loss: 76.24901503, src-aux-loss: 76.05814135, tar-aux-loss: 82.66237134
Epoch: [58 ] train-acc: 0.93267857, dom-acc: 0.67580357, val-acc: 0.90500000, val_loss: 0.26301605
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [59 ] loss: 257.19031167, sen-loss: 20.73785468, dom-loss: 76.20310223, src-aux-loss: 75.46818927, tar-aux-loss: 84.78116608
Epoch: [59 ] train-acc: 0.93464286, dom-acc: 0.67830357, val-acc: 0.91250000, val_loss: 0.26725245
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [60 ] loss: 255.37326682, sen-loss: 20.59254695, dom-loss: 76.34915185, src-aux-loss: 74.55795246, tar-aux-loss: 83.87361503
Epoch: [60 ] train-acc: 0.94142857, dom-acc: 0.67160714, val-acc: 0.90250000, val_loss: 0.25876787
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [61 ] loss: 253.08890653, sen-loss: 20.35018814, dom-loss: 76.43878084, src-aux-loss: 73.53766632, tar-aux-loss: 82.76227260
Epoch: [61 ] train-acc: 0.94107143, dom-acc: 0.66616071, val-acc: 0.90500000, val_loss: 0.25085989
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_video_HATN.ckpt
Best Epoch: [ 56] best val accuracy: 0.00000000 best val loss: 0.24409372
Testing accuracy: 0.88783333
./work/attentions/dvd_video_train_HATN.txt
./work/attentions/dvd_video_test_HATN.txt
loading data...
source domain:  electronics target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 9750
vocab-size:  83050
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'happy', 'fantastic', 'awesome', 'solid', 'amazing', 'highly', 'satisfied', 'reliable', 'love', 'quick', 'worth', 'durable', 'recommend', 'outstanding', 'perfectly', 'impressive', 'fast', 'pleased', 'simple', 'exactly', 'old', 'recommended', 'nice', 'decent', 'impressed', 'useful', 'arrived', 'cheaper', 'inexpensive', 'awsome', 'far', 'slick', 'effective', 'expected', 'loves', 'advertised']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'useless', 'frustrating', 'bad', 'worst', 'awful', 'failed', 'unreliable', 'cheap', 'wrong', 'horrible', 'died', 'hard', 'broke', 'impossible', 'expensive', 'poorly', 'defective', 'overpriced', 'returning', 'unacceptable', 'misleading', 'quit', 'broken', 'short', 'terrible', 'worthless', 'flimsy', 'ridiculous', 'beware', 'missing', 'slow', 'save', 'lousy', 'lasted', 'much', 'uncomfortable', 'difficult', 'unhappy', 'trying', 'low', 'cannot', 'flawed', 'incompatible', 'waste', 'acceptable', 'average', 'sad', 'lose', 'nowhere', 'waiting', 'weak', 'tried', 'mediocre', 'inferior']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
83050
5600 400 6000 23009 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 414.91665077, sen-loss: 77.46936399, dom-loss: 77.51447046, src-aux-loss: 133.19489425, tar-aux-loss: 126.73792267
Epoch: [1  ] train-acc: 0.71071429, dom-acc: 0.83562500, val-acc: 0.70750000, val_loss: 0.65235901
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 384.94443440, sen-loss: 68.90058196, dom-loss: 74.10296762, src-aux-loss: 123.90180129, tar-aux-loss: 118.03908348
Epoch: [2  ] train-acc: 0.75232143, dom-acc: 0.85187500, val-acc: 0.76500000, val_loss: 0.57044691
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 364.86911559, sen-loss: 58.40622276, dom-loss: 72.90797675, src-aux-loss: 119.06403613, tar-aux-loss: 114.49088126
Epoch: [3  ] train-acc: 0.80178571, dom-acc: 0.70758929, val-acc: 0.84000000, val_loss: 0.45170465
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 348.34800792, sen-loss: 48.55218986, dom-loss: 72.66990495, src-aux-loss: 115.54862422, tar-aux-loss: 111.57728904
Epoch: [4  ] train-acc: 0.82589286, dom-acc: 0.59517857, val-acc: 0.86000000, val_loss: 0.38169178
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 340.97270536, sen-loss: 44.20218398, dom-loss: 73.27382898, src-aux-loss: 113.35035110, tar-aux-loss: 110.14634299
Epoch: [5  ] train-acc: 0.84500000, dom-acc: 0.57714286, val-acc: 0.87000000, val_loss: 0.34469810
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 335.28003573, sen-loss: 40.74573630, dom-loss: 74.32882476, src-aux-loss: 111.35493869, tar-aux-loss: 108.85053593
Epoch: [6  ] train-acc: 0.85928571, dom-acc: 0.56839286, val-acc: 0.88250000, val_loss: 0.31644434
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 330.79982257, sen-loss: 38.09245420, dom-loss: 75.45531559, src-aux-loss: 109.58862388, tar-aux-loss: 107.66342711
Epoch: [7  ] train-acc: 0.86517857, dom-acc: 0.53553571, val-acc: 0.88750000, val_loss: 0.29712254
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 326.65505433, sen-loss: 36.21985176, dom-loss: 76.23227763, src-aux-loss: 108.28972429, tar-aux-loss: 105.91319990
Epoch: [8  ] train-acc: 0.87839286, dom-acc: 0.50392857, val-acc: 0.90000000, val_loss: 0.28111845
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 325.02863431, sen-loss: 34.87315591, dom-loss: 77.06368518, src-aux-loss: 107.03301734, tar-aux-loss: 106.05877537
Epoch: [9  ] train-acc: 0.88339286, dom-acc: 0.47839286, val-acc: 0.90250000, val_loss: 0.26656985
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 323.30711317, sen-loss: 33.56959939, dom-loss: 77.97351402, src-aux-loss: 106.02128696, tar-aux-loss: 105.74271250
Epoch: [10 ] train-acc: 0.88767857, dom-acc: 0.45830357, val-acc: 0.91250000, val_loss: 0.25677651
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 321.17406654, sen-loss: 32.50170328, dom-loss: 78.32113415, src-aux-loss: 105.56825668, tar-aux-loss: 104.78297222
Epoch: [11 ] train-acc: 0.88732143, dom-acc: 0.43776786, val-acc: 0.92000000, val_loss: 0.24730885
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 318.87832189, sen-loss: 31.78992650, dom-loss: 78.65287805, src-aux-loss: 104.33166891, tar-aux-loss: 104.10384905
Epoch: [12 ] train-acc: 0.89696429, dom-acc: 0.42000000, val-acc: 0.91750000, val_loss: 0.23835912
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 317.66772056, sen-loss: 30.93214589, dom-loss: 78.83983219, src-aux-loss: 103.67931992, tar-aux-loss: 104.21642435
Epoch: [13 ] train-acc: 0.89500000, dom-acc: 0.43178571, val-acc: 0.91500000, val_loss: 0.23540713
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 316.36458802, sen-loss: 30.41148655, dom-loss: 79.07590318, src-aux-loss: 103.00971103, tar-aux-loss: 103.86748689
Epoch: [14 ] train-acc: 0.90160714, dom-acc: 0.42857143, val-acc: 0.91500000, val_loss: 0.23187923
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 313.91486859, sen-loss: 29.72655772, dom-loss: 79.02846360, src-aux-loss: 102.40683514, tar-aux-loss: 102.75301230
Epoch: [15 ] train-acc: 0.90392857, dom-acc: 0.43937500, val-acc: 0.92000000, val_loss: 0.22513993
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 312.54387069, sen-loss: 29.07946174, dom-loss: 78.86375022, src-aux-loss: 102.18341857, tar-aux-loss: 102.41723973
Epoch: [16 ] train-acc: 0.90696429, dom-acc: 0.44366071, val-acc: 0.91750000, val_loss: 0.22189844
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 310.57149029, sen-loss: 28.55863738, dom-loss: 78.77457064, src-aux-loss: 101.45656663, tar-aux-loss: 101.78171617
Epoch: [17 ] train-acc: 0.90767857, dom-acc: 0.45312500, val-acc: 0.92000000, val_loss: 0.22080739
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 310.51512980, sen-loss: 28.33130130, dom-loss: 78.73236030, src-aux-loss: 100.81406593, tar-aux-loss: 102.63740075
Epoch: [18 ] train-acc: 0.90767857, dom-acc: 0.47928571, val-acc: 0.92750000, val_loss: 0.22304150
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 308.38610983, sen-loss: 27.68195418, dom-loss: 78.60009480, src-aux-loss: 100.34656233, tar-aux-loss: 101.75749737
Epoch: [19 ] train-acc: 0.90982143, dom-acc: 0.49687500, val-acc: 0.92250000, val_loss: 0.21122475
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 307.09635592, sen-loss: 27.34431359, dom-loss: 78.39644945, src-aux-loss: 99.77332389, tar-aux-loss: 101.58226979
Epoch: [20 ] train-acc: 0.91232143, dom-acc: 0.51928571, val-acc: 0.92750000, val_loss: 0.21568263
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 307.01436281, sen-loss: 27.04080529, dom-loss: 78.02555096, src-aux-loss: 99.67431718, tar-aux-loss: 102.27369076
Epoch: [21 ] train-acc: 0.91375000, dom-acc: 0.50160714, val-acc: 0.92250000, val_loss: 0.20689550
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 304.83267140, sen-loss: 26.63010534, dom-loss: 77.96391767, src-aux-loss: 98.98592281, tar-aux-loss: 101.25272548
Epoch: [22 ] train-acc: 0.91285714, dom-acc: 0.51401786, val-acc: 0.92250000, val_loss: 0.20532155
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 303.34985280, sen-loss: 26.16972403, dom-loss: 77.68744409, src-aux-loss: 98.38835478, tar-aux-loss: 101.10433042
Epoch: [23 ] train-acc: 0.91482143, dom-acc: 0.53133929, val-acc: 0.92750000, val_loss: 0.20489441
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 302.94379115, sen-loss: 25.78894259, dom-loss: 77.51137835, src-aux-loss: 98.45804036, tar-aux-loss: 101.18543088
Epoch: [24 ] train-acc: 0.92035714, dom-acc: 0.52803571, val-acc: 0.92750000, val_loss: 0.20583627
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 303.21760607, sen-loss: 25.54957877, dom-loss: 77.62832606, src-aux-loss: 97.71927929, tar-aux-loss: 102.32042176
Epoch: [25 ] train-acc: 0.91839286, dom-acc: 0.54794643, val-acc: 0.93000000, val_loss: 0.20245980
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 299.86283088, sen-loss: 25.31410191, dom-loss: 77.57108557, src-aux-loss: 97.31995243, tar-aux-loss: 99.65769160
Epoch: [26 ] train-acc: 0.92196429, dom-acc: 0.52705357, val-acc: 0.92750000, val_loss: 0.20312031
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 300.68041110, sen-loss: 24.92167133, dom-loss: 77.37654376, src-aux-loss: 96.83806342, tar-aux-loss: 101.54413337
Epoch: [27 ] train-acc: 0.92107143, dom-acc: 0.52312500, val-acc: 0.93000000, val_loss: 0.19868024
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 299.38782883, sen-loss: 24.71055554, dom-loss: 77.51083249, src-aux-loss: 96.54485035, tar-aux-loss: 100.62159026
Epoch: [28 ] train-acc: 0.92339286, dom-acc: 0.52008929, val-acc: 0.93000000, val_loss: 0.19919731
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 298.78276324, sen-loss: 24.52910987, dom-loss: 77.52622402, src-aux-loss: 96.15808964, tar-aux-loss: 100.56933939
Epoch: [29 ] train-acc: 0.92142857, dom-acc: 0.53892857, val-acc: 0.93000000, val_loss: 0.19794501
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 297.51741004, sen-loss: 24.08956189, dom-loss: 77.46414095, src-aux-loss: 95.75590515, tar-aux-loss: 100.20780253
Epoch: [30 ] train-acc: 0.92696429, dom-acc: 0.52366071, val-acc: 0.93250000, val_loss: 0.20046669
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 297.06969666, sen-loss: 23.87357350, dom-loss: 77.65181702, src-aux-loss: 95.23384935, tar-aux-loss: 100.31045574
Epoch: [31 ] train-acc: 0.92660714, dom-acc: 0.49339286, val-acc: 0.93500000, val_loss: 0.20118803
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 297.57894349, sen-loss: 23.53077799, dom-loss: 77.90735346, src-aux-loss: 95.05971962, tar-aux-loss: 101.08109224
Epoch: [32 ] train-acc: 0.92482143, dom-acc: 0.52723214, val-acc: 0.92250000, val_loss: 0.19725342
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 296.34906602, sen-loss: 23.30541220, dom-loss: 77.94509971, src-aux-loss: 94.66774863, tar-aux-loss: 100.43080473
Epoch: [33 ] train-acc: 0.92946429, dom-acc: 0.51464286, val-acc: 0.93750000, val_loss: 0.19664586
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 296.00457788, sen-loss: 23.10177906, dom-loss: 77.91736889, src-aux-loss: 94.41642505, tar-aux-loss: 100.56900364
Epoch: [34 ] train-acc: 0.93178571, dom-acc: 0.48767857, val-acc: 0.93500000, val_loss: 0.19412163
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 294.16763306, sen-loss: 22.76554669, dom-loss: 78.17030203, src-aux-loss: 93.85496151, tar-aux-loss: 99.37682331
Epoch: [35 ] train-acc: 0.93000000, dom-acc: 0.47375000, val-acc: 0.94000000, val_loss: 0.19674639
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 294.07561064, sen-loss: 22.44540618, dom-loss: 78.21914721, src-aux-loss: 93.62274224, tar-aux-loss: 99.78831637
Epoch: [36 ] train-acc: 0.93160714, dom-acc: 0.46625000, val-acc: 0.93500000, val_loss: 0.19095409
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 294.27199483, sen-loss: 22.36937609, dom-loss: 78.12284881, src-aux-loss: 93.30659789, tar-aux-loss: 100.47317123
Epoch: [37 ] train-acc: 0.93339286, dom-acc: 0.46633929, val-acc: 0.93500000, val_loss: 0.19067259
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 292.70457864, sen-loss: 21.95785180, dom-loss: 78.34625357, src-aux-loss: 92.59419280, tar-aux-loss: 99.80628103
Epoch: [38 ] train-acc: 0.93500000, dom-acc: 0.48071429, val-acc: 0.94000000, val_loss: 0.19229743
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 290.98294425, sen-loss: 21.85393988, dom-loss: 78.25102007, src-aux-loss: 92.13569403, tar-aux-loss: 98.74229020
Epoch: [39 ] train-acc: 0.93678571, dom-acc: 0.45035714, val-acc: 0.94000000, val_loss: 0.18948089
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 291.31993389, sen-loss: 21.46249090, dom-loss: 78.20588052, src-aux-loss: 91.88583761, tar-aux-loss: 99.76572382
Epoch: [40 ] train-acc: 0.93660714, dom-acc: 0.46508929, val-acc: 0.94000000, val_loss: 0.18938749
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 290.90937400, sen-loss: 21.31766924, dom-loss: 78.18697077, src-aux-loss: 91.53299057, tar-aux-loss: 99.87174445
Epoch: [41 ] train-acc: 0.93821429, dom-acc: 0.45910714, val-acc: 0.94250000, val_loss: 0.18860972
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 289.58403850, sen-loss: 21.02636121, dom-loss: 78.09999102, src-aux-loss: 91.12045133, tar-aux-loss: 99.33723503
Epoch: [42 ] train-acc: 0.94053571, dom-acc: 0.46946429, val-acc: 0.94000000, val_loss: 0.18814762
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 288.27564073, sen-loss: 20.84444462, dom-loss: 77.95507282, src-aux-loss: 90.46931124, tar-aux-loss: 99.00681120
Epoch: [43 ] train-acc: 0.94142857, dom-acc: 0.45383929, val-acc: 0.93750000, val_loss: 0.18849057
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 287.82667685, sen-loss: 20.63000155, dom-loss: 77.91990501, src-aux-loss: 90.15043074, tar-aux-loss: 99.12633878
Epoch: [44 ] train-acc: 0.94250000, dom-acc: 0.50000000, val-acc: 0.93250000, val_loss: 0.18884231
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 286.68842816, sen-loss: 20.35316505, dom-loss: 77.75007236, src-aux-loss: 90.02347165, tar-aux-loss: 98.56171787
Epoch: [45 ] train-acc: 0.94392857, dom-acc: 0.50357143, val-acc: 0.93750000, val_loss: 0.18620098
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 286.59197283, sen-loss: 20.14399169, dom-loss: 77.60731840, src-aux-loss: 89.52297878, tar-aux-loss: 99.31768411
Epoch: [46 ] train-acc: 0.94464286, dom-acc: 0.47973214, val-acc: 0.93000000, val_loss: 0.18771449
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 285.12645388, sen-loss: 19.84900928, dom-loss: 77.50404596, src-aux-loss: 88.91791421, tar-aux-loss: 98.85548234
Epoch: [47 ] train-acc: 0.94250000, dom-acc: 0.49258929, val-acc: 0.93750000, val_loss: 0.19055562
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 285.26392746, sen-loss: 19.65485734, dom-loss: 77.42931229, src-aux-loss: 88.58214140, tar-aux-loss: 99.59761775
Epoch: [48 ] train-acc: 0.94696429, dom-acc: 0.53785714, val-acc: 0.93750000, val_loss: 0.18796688
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 283.46434450, sen-loss: 19.36865673, dom-loss: 77.35109180, src-aux-loss: 88.07926846, tar-aux-loss: 98.66532683
Epoch: [49 ] train-acc: 0.94696429, dom-acc: 0.49464286, val-acc: 0.93250000, val_loss: 0.18584740
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 282.25250053, sen-loss: 19.19616216, dom-loss: 77.30739373, src-aux-loss: 87.74717730, tar-aux-loss: 98.00176656
Epoch: [50 ] train-acc: 0.94875000, dom-acc: 0.52142857, val-acc: 0.93500000, val_loss: 0.18697566
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 281.46932101, sen-loss: 18.95641022, dom-loss: 77.30393887, src-aux-loss: 87.09200972, tar-aux-loss: 98.11695993
Epoch: [51 ] train-acc: 0.94839286, dom-acc: 0.55267857, val-acc: 0.93500000, val_loss: 0.18804298
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 281.40698957, sen-loss: 18.65781247, dom-loss: 77.27668256, src-aux-loss: 86.82846946, tar-aux-loss: 98.64402413
Epoch: [52 ] train-acc: 0.94446429, dom-acc: 0.47142857, val-acc: 0.93750000, val_loss: 0.19270699
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 282.01327038, sen-loss: 18.50221466, dom-loss: 77.26196676, src-aux-loss: 86.67405039, tar-aux-loss: 99.57503819
Epoch: [53 ] train-acc: 0.95017857, dom-acc: 0.50330357, val-acc: 0.93500000, val_loss: 0.18811561
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [54 ] loss: 280.65167332, sen-loss: 18.18701817, dom-loss: 77.29040188, src-aux-loss: 86.12735629, tar-aux-loss: 99.04689813
Epoch: [54 ] train-acc: 0.94857143, dom-acc: 0.48794643, val-acc: 0.93750000, val_loss: 0.19041562
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_books_HATN.ckpt
Best Epoch: [ 49] best val accuracy: 0.00000000 best val loss: 0.18584740
Testing accuracy: 0.83900000
./work/attentions/electronics_books_train_HATN.txt
./work/attentions/electronics_books_test_HATN.txt
loading data...
source domain:  electronics target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 11843
vocab-size:  85442
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'fantastic', 'happy', 'awesome', 'solid', 'amazing', 'highly', 'satisfied', 'durable', 'reliable', 'worth', 'quick', 'outstanding', 'perfectly', 'love', 'impressive', 'recommend', 'fast', 'pleased', 'simple', 'exactly', 'nice', 'recommended', 'impressed', 'cheaper', 'inexpensive', 'decent', 'old', 'awsome', 'far', 'useful', 'slick', 'effective', 'expected']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'useless', 'frustrating', 'bad', 'worst', 'awful', 'horrible', 'unreliable', 'cheap', 'failed', 'hard', 'died', 'impossible', 'broke', 'expensive', 'wrong', 'poorly', 'overpriced', 'unacceptable', 'defective', 'returning', 'misleading', 'quit', 'terrible', 'short', 'worthless', 'broken', 'flimsy', 'beware', 'save', 'ridiculous', 'lousy', 'lasted', 'much', 'missing', 'unhappy', 'trying', 'slow', 'low', 'uncomfortable', 'waste', 'difficult', 'flawed', 'sent', 'incompatible', 'tried', 'cannot', 'acceptable', 'average', 'sad', 'lose', 'waiting', 'mediocre', 'inferior']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
85442
5600 400 6000 23009 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 410.90430689, sen-loss: 77.50069112, dom-loss: 76.64833099, src-aux-loss: 133.93938923, tar-aux-loss: 122.81589478
Epoch: [1  ] train-acc: 0.71160714, dom-acc: 0.84750000, val-acc: 0.71000000, val_loss: 0.65236002
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 380.95669270, sen-loss: 68.97051537, dom-loss: 73.34834492, src-aux-loss: 124.48159379, tar-aux-loss: 114.15623820
Epoch: [2  ] train-acc: 0.75589286, dom-acc: 0.82116071, val-acc: 0.77750000, val_loss: 0.57010347
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.60329413, sen-loss: 58.41565111, dom-loss: 72.09382755, src-aux-loss: 119.73017389, tar-aux-loss: 110.36364216
Epoch: [3  ] train-acc: 0.80285714, dom-acc: 0.70267857, val-acc: 0.84000000, val_loss: 0.45125207
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 344.35452509, sen-loss: 48.58976302, dom-loss: 72.00785434, src-aux-loss: 115.83535790, tar-aux-loss: 107.92155075
Epoch: [4  ] train-acc: 0.82678571, dom-acc: 0.60348214, val-acc: 0.86000000, val_loss: 0.38118467
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 336.35415864, sen-loss: 44.27851856, dom-loss: 72.81095487, src-aux-loss: 113.76193905, tar-aux-loss: 105.50274366
Epoch: [5  ] train-acc: 0.84714286, dom-acc: 0.58732143, val-acc: 0.87500000, val_loss: 0.34594426
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 330.90284824, sen-loss: 40.87965010, dom-loss: 73.60703069, src-aux-loss: 111.63881898, tar-aux-loss: 104.77734911
Epoch: [6  ] train-acc: 0.85607143, dom-acc: 0.56678571, val-acc: 0.88000000, val_loss: 0.31725982
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 326.63113761, sen-loss: 38.22318412, dom-loss: 74.85907423, src-aux-loss: 109.65894443, tar-aux-loss: 103.88993466
Epoch: [7  ] train-acc: 0.86500000, dom-acc: 0.54062500, val-acc: 0.89500000, val_loss: 0.29709759
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 322.65238643, sen-loss: 36.39463751, dom-loss: 75.73027498, src-aux-loss: 108.36881667, tar-aux-loss: 102.15865678
Epoch: [8  ] train-acc: 0.87482143, dom-acc: 0.50455357, val-acc: 0.89750000, val_loss: 0.28406459
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 320.95383739, sen-loss: 35.07926954, dom-loss: 77.04797441, src-aux-loss: 107.06151712, tar-aux-loss: 101.76507646
Epoch: [9  ] train-acc: 0.88142857, dom-acc: 0.48071429, val-acc: 0.90250000, val_loss: 0.26752838
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 319.51759815, sen-loss: 33.72462589, dom-loss: 77.86440706, src-aux-loss: 106.07379460, tar-aux-loss: 101.85477024
Epoch: [10 ] train-acc: 0.88767857, dom-acc: 0.44991071, val-acc: 0.91000000, val_loss: 0.25715056
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 316.94971061, sen-loss: 32.55295962, dom-loss: 78.50795090, src-aux-loss: 105.51668352, tar-aux-loss: 100.37211639
Epoch: [11 ] train-acc: 0.89232143, dom-acc: 0.43303571, val-acc: 0.91500000, val_loss: 0.24789435
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 315.06363869, sen-loss: 31.86483459, dom-loss: 78.99145341, src-aux-loss: 104.39343596, tar-aux-loss: 99.81391490
Epoch: [12 ] train-acc: 0.89571429, dom-acc: 0.41125000, val-acc: 0.91250000, val_loss: 0.23964804
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.56624055, sen-loss: 31.02613434, dom-loss: 78.97381479, src-aux-loss: 103.72256243, tar-aux-loss: 100.84372801
Epoch: [13 ] train-acc: 0.89357143, dom-acc: 0.42750000, val-acc: 0.91750000, val_loss: 0.23559822
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 312.35397458, sen-loss: 30.42100805, dom-loss: 79.03267699, src-aux-loss: 102.97585154, tar-aux-loss: 99.92443681
Epoch: [14 ] train-acc: 0.90142857, dom-acc: 0.40508929, val-acc: 0.91250000, val_loss: 0.23183253
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 309.78171587, sen-loss: 29.81370571, dom-loss: 78.84554327, src-aux-loss: 102.29105687, tar-aux-loss: 98.83140916
Epoch: [15 ] train-acc: 0.90178571, dom-acc: 0.42964286, val-acc: 0.91500000, val_loss: 0.22521104
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 308.58029842, sen-loss: 29.15927680, dom-loss: 78.56533962, src-aux-loss: 101.94686735, tar-aux-loss: 98.90881461
Epoch: [16 ] train-acc: 0.90678571, dom-acc: 0.43669643, val-acc: 0.91750000, val_loss: 0.22202908
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 305.79542589, sen-loss: 28.60670254, dom-loss: 78.07192910, src-aux-loss: 101.23902464, tar-aux-loss: 97.87776989
Epoch: [17 ] train-acc: 0.90696429, dom-acc: 0.44160714, val-acc: 0.92000000, val_loss: 0.22049814
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 305.72753382, sen-loss: 28.33982854, dom-loss: 78.07188928, src-aux-loss: 100.65988612, tar-aux-loss: 98.65593123
Epoch: [18 ] train-acc: 0.90928571, dom-acc: 0.47205357, val-acc: 0.92000000, val_loss: 0.21846688
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 302.81326270, sen-loss: 27.78054265, dom-loss: 77.59784633, src-aux-loss: 100.17028135, tar-aux-loss: 97.26459354
Epoch: [19 ] train-acc: 0.91125000, dom-acc: 0.47991071, val-acc: 0.91750000, val_loss: 0.21183674
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 301.70421576, sen-loss: 27.37525728, dom-loss: 77.22176534, src-aux-loss: 99.51784152, tar-aux-loss: 97.58935177
Epoch: [20 ] train-acc: 0.91267857, dom-acc: 0.48919643, val-acc: 0.92750000, val_loss: 0.21419451
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 302.81434298, sen-loss: 27.02979960, dom-loss: 77.23491198, src-aux-loss: 99.24173641, tar-aux-loss: 99.30789375
Epoch: [21 ] train-acc: 0.91392857, dom-acc: 0.50758929, val-acc: 0.92000000, val_loss: 0.20738487
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 299.13586712, sen-loss: 26.63611862, dom-loss: 77.03428054, src-aux-loss: 98.70463455, tar-aux-loss: 96.76083302
Epoch: [22 ] train-acc: 0.91267857, dom-acc: 0.51142857, val-acc: 0.91250000, val_loss: 0.20697872
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 298.94436884, sen-loss: 26.19388115, dom-loss: 76.75690359, src-aux-loss: 98.27497560, tar-aux-loss: 97.71860796
Epoch: [23 ] train-acc: 0.91464286, dom-acc: 0.52919643, val-acc: 0.92000000, val_loss: 0.20507240
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 297.52000141, sen-loss: 25.84748386, dom-loss: 76.71593165, src-aux-loss: 98.25196874, tar-aux-loss: 96.70461696
Epoch: [24 ] train-acc: 0.91857143, dom-acc: 0.52258929, val-acc: 0.92500000, val_loss: 0.20512721
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 298.35564899, sen-loss: 25.52885327, dom-loss: 76.85900569, src-aux-loss: 97.40191531, tar-aux-loss: 98.56587523
Epoch: [25 ] train-acc: 0.91696429, dom-acc: 0.51571429, val-acc: 0.92750000, val_loss: 0.20255421
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.46783566, sen-loss: 25.35046443, dom-loss: 76.93295389, src-aux-loss: 96.88467604, tar-aux-loss: 96.29974192
Epoch: [26 ] train-acc: 0.92142857, dom-acc: 0.51080357, val-acc: 0.92500000, val_loss: 0.20235132
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 296.20990157, sen-loss: 24.99185050, dom-loss: 77.00002581, src-aux-loss: 96.44607425, tar-aux-loss: 97.77195078
Epoch: [27 ] train-acc: 0.92107143, dom-acc: 0.50937500, val-acc: 0.92750000, val_loss: 0.19902542
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 295.31528115, sen-loss: 24.79276767, dom-loss: 77.21494305, src-aux-loss: 96.24207771, tar-aux-loss: 97.06549346
Epoch: [28 ] train-acc: 0.92321429, dom-acc: 0.49589286, val-acc: 0.92500000, val_loss: 0.19914344
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 294.98074126, sen-loss: 24.59432178, dom-loss: 77.47599691, src-aux-loss: 95.85264355, tar-aux-loss: 97.05777991
Epoch: [29 ] train-acc: 0.92142857, dom-acc: 0.49660714, val-acc: 0.93000000, val_loss: 0.19725236
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 292.75754786, sen-loss: 24.15401644, dom-loss: 77.61437893, src-aux-loss: 95.13108408, tar-aux-loss: 95.85806739
Epoch: [30 ] train-acc: 0.92500000, dom-acc: 0.47473214, val-acc: 0.93000000, val_loss: 0.20263335
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 292.54704642, sen-loss: 23.96705013, dom-loss: 77.89531356, src-aux-loss: 94.74383658, tar-aux-loss: 95.94084615
Epoch: [31 ] train-acc: 0.92696429, dom-acc: 0.45785714, val-acc: 0.93250000, val_loss: 0.19974247
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 294.41596437, sen-loss: 23.56897029, dom-loss: 78.22619087, src-aux-loss: 94.52901524, tar-aux-loss: 98.09178853
Epoch: [32 ] train-acc: 0.92410714, dom-acc: 0.46687500, val-acc: 0.93000000, val_loss: 0.19587533
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 292.33409500, sen-loss: 23.38848500, dom-loss: 78.47452116, src-aux-loss: 94.08189213, tar-aux-loss: 96.38919795
Epoch: [33 ] train-acc: 0.92946429, dom-acc: 0.45133929, val-acc: 0.92750000, val_loss: 0.19600953
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 291.40035033, sen-loss: 23.13477661, dom-loss: 78.46014744, src-aux-loss: 93.86798745, tar-aux-loss: 95.93743813
Epoch: [34 ] train-acc: 0.92910714, dom-acc: 0.44250000, val-acc: 0.92750000, val_loss: 0.19282073
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 290.44653058, sen-loss: 22.80298615, dom-loss: 78.67667365, src-aux-loss: 93.18458176, tar-aux-loss: 95.78229064
Epoch: [35 ] train-acc: 0.92767857, dom-acc: 0.42750000, val-acc: 0.93500000, val_loss: 0.19792487
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 290.61462188, sen-loss: 22.56619359, dom-loss: 78.74870044, src-aux-loss: 93.02867746, tar-aux-loss: 96.27104992
Epoch: [36 ] train-acc: 0.93160714, dom-acc: 0.43214286, val-acc: 0.93500000, val_loss: 0.19120458
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 289.19999242, sen-loss: 22.42061897, dom-loss: 78.67351866, src-aux-loss: 92.53231376, tar-aux-loss: 95.57354027
Epoch: [37 ] train-acc: 0.93285714, dom-acc: 0.43580357, val-acc: 0.93500000, val_loss: 0.19008195
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 290.07396197, sen-loss: 22.06030604, dom-loss: 78.60766506, src-aux-loss: 92.00835240, tar-aux-loss: 97.39763969
Epoch: [38 ] train-acc: 0.93410714, dom-acc: 0.44035714, val-acc: 0.93750000, val_loss: 0.19321142
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 288.63850498, sen-loss: 21.97521312, dom-loss: 78.43751496, src-aux-loss: 91.55493194, tar-aux-loss: 96.67084599
Epoch: [39 ] train-acc: 0.93571429, dom-acc: 0.42901786, val-acc: 0.93500000, val_loss: 0.18961717
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 285.44823241, sen-loss: 21.54620810, dom-loss: 78.30183154, src-aux-loss: 91.20229173, tar-aux-loss: 94.39790207
Epoch: [40 ] train-acc: 0.93750000, dom-acc: 0.44589286, val-acc: 0.93500000, val_loss: 0.18893197
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 285.36413097, sen-loss: 21.47890744, dom-loss: 78.10324788, src-aux-loss: 90.76573092, tar-aux-loss: 95.01624447
Epoch: [41 ] train-acc: 0.93642857, dom-acc: 0.45366071, val-acc: 0.93750000, val_loss: 0.18838631
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 284.71149039, sen-loss: 21.08127251, dom-loss: 77.85643792, src-aux-loss: 90.45950055, tar-aux-loss: 95.31428081
Epoch: [42 ] train-acc: 0.93964286, dom-acc: 0.46214286, val-acc: 0.93500000, val_loss: 0.18763743
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 284.13993025, sen-loss: 20.95725458, dom-loss: 77.50610089, src-aux-loss: 89.91609472, tar-aux-loss: 95.76048106
Epoch: [43 ] train-acc: 0.94196429, dom-acc: 0.46053571, val-acc: 0.93750000, val_loss: 0.18830366
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 282.51285791, sen-loss: 20.74908914, dom-loss: 77.42152011, src-aux-loss: 89.48561621, tar-aux-loss: 94.85663182
Epoch: [44 ] train-acc: 0.94250000, dom-acc: 0.49026786, val-acc: 0.93750000, val_loss: 0.18728550
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 282.91979527, sen-loss: 20.50725352, dom-loss: 77.20170474, src-aux-loss: 89.37070560, tar-aux-loss: 95.84013206
Epoch: [45 ] train-acc: 0.94125000, dom-acc: 0.49741071, val-acc: 0.93250000, val_loss: 0.18585165
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 281.31972432, sen-loss: 20.24504281, dom-loss: 77.08512402, src-aux-loss: 88.91970766, tar-aux-loss: 95.06984991
Epoch: [46 ] train-acc: 0.94214286, dom-acc: 0.49544643, val-acc: 0.93000000, val_loss: 0.18859924
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 279.85648155, sen-loss: 19.95142219, dom-loss: 76.76149988, src-aux-loss: 88.40042019, tar-aux-loss: 94.74314004
Epoch: [47 ] train-acc: 0.93946429, dom-acc: 0.49901786, val-acc: 0.92750000, val_loss: 0.19382365
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 280.74355388, sen-loss: 19.75520165, dom-loss: 76.94670236, src-aux-loss: 87.93406129, tar-aux-loss: 96.10758954
Epoch: [48 ] train-acc: 0.94428571, dom-acc: 0.52910714, val-acc: 0.93250000, val_loss: 0.18662904
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 279.64242697, sen-loss: 19.49941507, dom-loss: 76.80009151, src-aux-loss: 87.67455828, tar-aux-loss: 95.66836226
Epoch: [49 ] train-acc: 0.94428571, dom-acc: 0.48535714, val-acc: 0.93250000, val_loss: 0.18441352
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 277.75042772, sen-loss: 19.33332407, dom-loss: 76.98501778, src-aux-loss: 87.20175934, tar-aux-loss: 94.23032683
Epoch: [50 ] train-acc: 0.94517857, dom-acc: 0.51741071, val-acc: 0.93250000, val_loss: 0.18590090
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 277.42245579, sen-loss: 19.05903429, dom-loss: 77.01802397, src-aux-loss: 86.68406886, tar-aux-loss: 94.66132778
Epoch: [51 ] train-acc: 0.94660714, dom-acc: 0.50214286, val-acc: 0.93000000, val_loss: 0.18591683
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 275.53152752, sen-loss: 18.76458213, dom-loss: 77.01779908, src-aux-loss: 86.23943067, tar-aux-loss: 93.50971603
Epoch: [52 ] train-acc: 0.94535714, dom-acc: 0.48321429, val-acc: 0.93500000, val_loss: 0.18882975
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 278.37482715, sen-loss: 18.63188127, dom-loss: 77.21980828, src-aux-loss: 85.90103221, tar-aux-loss: 96.62210476
Epoch: [53 ] train-acc: 0.94821429, dom-acc: 0.49723214, val-acc: 0.93500000, val_loss: 0.18661919
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [54 ] loss: 276.90309501, sen-loss: 18.30410579, dom-loss: 77.22335422, src-aux-loss: 85.57072711, tar-aux-loss: 95.80490869
Epoch: [54 ] train-acc: 0.94696429, dom-acc: 0.47142857, val-acc: 0.93250000, val_loss: 0.19014537
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_dvd_HATN.ckpt
Best Epoch: [ 49] best val accuracy: 0.00000000 best val loss: 0.18441352
Testing accuracy: 0.83566667
./work/attentions/electronics_dvd_train_HATN.txt
./work/attentions/electronics_dvd_test_HATN.txt
loading data...
source domain:  electronics target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 13856
vocab-size:  49470
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'happy', 'works', 'awesome', 'fantastic', 'solid', 'love', 'highly', 'amazing', 'satisfied', 'reliable', 'worth', 'durable', 'far', 'quick', 'perfectly', 'recommend', 'outstanding', 'pleased', 'impressive', 'old', 'simple', 'exactly', 'fast', 'expected', 'recommended', 'decent', 'impressed', 'nice', 'useful', 'cheaper', 'arrived', 'inexpensive', 'awsome', 'true', 'slick', 'effective']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'frustrating', 'useless', 'worst', 'bad', 'unreliable', 'awful', 'horrible', 'cheap', 'failed', 'broke', 'hard', 'wrong', 'died', 'impossible', 'defective', 'poorly', 'expensive', 'returning', 'unacceptable', 'overpriced', 'misleading', 'quit', 'broken', 'short', 'terrible', 'worthless', 'flimsy', 'ridiculous', 'beware', 'uncomfortable', 'much', 'trying', 'slow', 'save', 'lousy', 'lasted', 'missing', 'unhappy', 'average', 'low', 'waste', 'difficult', 'flawed', 'lose', 'incompatible', 'tried', 'sad', 'waiting', 'cannot', 'unable', 'mediocre', 'inferior']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
49470
5600 400 6000 23009 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 408.67565393, sen-loss: 77.44668818, dom-loss: 78.12022561, src-aux-loss: 134.47961122, tar-aux-loss: 118.62912995
Epoch: [1  ] train-acc: 0.71339286, dom-acc: 0.71223214, val-acc: 0.71500000, val_loss: 0.65199035
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 379.80406857, sen-loss: 68.84664756, dom-loss: 75.80244923, src-aux-loss: 125.02089220, tar-aux-loss: 110.13408202
Epoch: [2  ] train-acc: 0.75625000, dom-acc: 0.77312500, val-acc: 0.75750000, val_loss: 0.56796676
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 358.76806927, sen-loss: 58.23886201, dom-loss: 74.76500720, src-aux-loss: 119.65478081, tar-aux-loss: 106.10941887
Epoch: [3  ] train-acc: 0.80196429, dom-acc: 0.77267857, val-acc: 0.83750000, val_loss: 0.45076850
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 341.58429718, sen-loss: 48.48101607, dom-loss: 74.32168293, src-aux-loss: 115.92927527, tar-aux-loss: 102.85232282
Epoch: [4  ] train-acc: 0.82892857, dom-acc: 0.73562500, val-acc: 0.85750000, val_loss: 0.38135830
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 334.49392653, sen-loss: 44.05347289, dom-loss: 74.42846382, src-aux-loss: 113.74222571, tar-aux-loss: 102.26976514
Epoch: [5  ] train-acc: 0.84714286, dom-acc: 0.74294643, val-acc: 0.87000000, val_loss: 0.34485638
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 326.12202764, sen-loss: 40.69871728, dom-loss: 74.17618257, src-aux-loss: 111.53645271, tar-aux-loss: 99.71067345
Epoch: [6  ] train-acc: 0.85839286, dom-acc: 0.72419643, val-acc: 0.88000000, val_loss: 0.31651023
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 321.85307407, sen-loss: 38.00068578, dom-loss: 74.47468084, src-aux-loss: 109.63721728, tar-aux-loss: 99.74049067
Epoch: [7  ] train-acc: 0.86571429, dom-acc: 0.71473214, val-acc: 0.88250000, val_loss: 0.29750922
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 319.22835493, sen-loss: 36.23033130, dom-loss: 74.74586225, src-aux-loss: 108.42916179, tar-aux-loss: 99.82299960
Epoch: [8  ] train-acc: 0.87642857, dom-acc: 0.72785714, val-acc: 0.90000000, val_loss: 0.28558391
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 314.10947895, sen-loss: 34.89603442, dom-loss: 74.90749538, src-aux-loss: 107.06377226, tar-aux-loss: 97.24217719
Epoch: [9  ] train-acc: 0.88267857, dom-acc: 0.71366071, val-acc: 0.90250000, val_loss: 0.27010155
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 312.36398149, sen-loss: 33.52149877, dom-loss: 75.31107700, src-aux-loss: 105.93626577, tar-aux-loss: 97.59514093
Epoch: [10 ] train-acc: 0.89017857, dom-acc: 0.70633929, val-acc: 0.91000000, val_loss: 0.25639585
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 310.48306179, sen-loss: 32.47601156, dom-loss: 75.60798234, src-aux-loss: 105.61696863, tar-aux-loss: 96.78209937
Epoch: [11 ] train-acc: 0.89160714, dom-acc: 0.69232143, val-acc: 0.92000000, val_loss: 0.24651456
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 307.66291857, sen-loss: 31.76331346, dom-loss: 75.84237385, src-aux-loss: 104.20273215, tar-aux-loss: 95.85449952
Epoch: [12 ] train-acc: 0.89589286, dom-acc: 0.69026786, val-acc: 0.92000000, val_loss: 0.23811151
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 307.10167718, sen-loss: 30.94542325, dom-loss: 76.06126636, src-aux-loss: 103.54310018, tar-aux-loss: 96.55188817
Epoch: [13 ] train-acc: 0.89178571, dom-acc: 0.67125000, val-acc: 0.91750000, val_loss: 0.23705290
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 305.61285925, sen-loss: 30.33657429, dom-loss: 76.20880675, src-aux-loss: 102.89717454, tar-aux-loss: 96.17030418
Epoch: [14 ] train-acc: 0.90250000, dom-acc: 0.68383929, val-acc: 0.91250000, val_loss: 0.22945641
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 303.17618918, sen-loss: 29.72324231, dom-loss: 76.67593974, src-aux-loss: 102.25683010, tar-aux-loss: 94.52017820
Epoch: [15 ] train-acc: 0.90464286, dom-acc: 0.67160714, val-acc: 0.91750000, val_loss: 0.22355697
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 303.68725467, sen-loss: 29.17497908, dom-loss: 76.98424447, src-aux-loss: 102.05310392, tar-aux-loss: 95.47492731
Epoch: [16 ] train-acc: 0.90696429, dom-acc: 0.66875000, val-acc: 0.92500000, val_loss: 0.22252849
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 300.34711838, sen-loss: 28.57748878, dom-loss: 77.00426418, src-aux-loss: 101.32120830, tar-aux-loss: 93.44415778
Epoch: [17 ] train-acc: 0.90928571, dom-acc: 0.66937500, val-acc: 0.93000000, val_loss: 0.22113757
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 299.33739042, sen-loss: 28.25060859, dom-loss: 77.01197702, src-aux-loss: 100.65517569, tar-aux-loss: 93.41962957
Epoch: [18 ] train-acc: 0.91178571, dom-acc: 0.66687500, val-acc: 0.93000000, val_loss: 0.21563068
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 298.20340347, sen-loss: 27.66045213, dom-loss: 77.26903665, src-aux-loss: 100.16066802, tar-aux-loss: 93.11324620
Epoch: [19 ] train-acc: 0.91089286, dom-acc: 0.65901786, val-acc: 0.91750000, val_loss: 0.20956264
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 299.03067183, sen-loss: 27.33817488, dom-loss: 77.44894385, src-aux-loss: 99.54419494, tar-aux-loss: 94.69935894
Epoch: [20 ] train-acc: 0.91250000, dom-acc: 0.66410714, val-acc: 0.93250000, val_loss: 0.21434695
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 298.22209764, sen-loss: 26.99881334, dom-loss: 77.63183826, src-aux-loss: 99.38681364, tar-aux-loss: 94.20463240
Epoch: [21 ] train-acc: 0.91303571, dom-acc: 0.65303571, val-acc: 0.92000000, val_loss: 0.20489796
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 295.90336156, sen-loss: 26.63610727, dom-loss: 77.68485194, src-aux-loss: 98.77593577, tar-aux-loss: 92.80646783
Epoch: [22 ] train-acc: 0.91285714, dom-acc: 0.64714286, val-acc: 0.92000000, val_loss: 0.20363787
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 294.37108612, sen-loss: 26.11151091, dom-loss: 77.65860677, src-aux-loss: 98.24979669, tar-aux-loss: 92.35116965
Epoch: [23 ] train-acc: 0.91714286, dom-acc: 0.64535714, val-acc: 0.92750000, val_loss: 0.20279537
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 296.36896539, sen-loss: 25.79555013, dom-loss: 77.97528374, src-aux-loss: 98.30979544, tar-aux-loss: 94.28833568
Epoch: [24 ] train-acc: 0.91946429, dom-acc: 0.64687500, val-acc: 0.92750000, val_loss: 0.20367077
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 294.30517197, sen-loss: 25.50609743, dom-loss: 77.74959439, src-aux-loss: 97.48414445, tar-aux-loss: 93.56533557
Epoch: [25 ] train-acc: 0.92035714, dom-acc: 0.65589286, val-acc: 0.93000000, val_loss: 0.20044631
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 292.79886270, sen-loss: 25.33079839, dom-loss: 77.91951364, src-aux-loss: 97.14778936, tar-aux-loss: 92.40076256
Epoch: [26 ] train-acc: 0.92125000, dom-acc: 0.65723214, val-acc: 0.92750000, val_loss: 0.20204005
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 292.53025675, sen-loss: 24.88981557, dom-loss: 77.83506411, src-aux-loss: 96.60632133, tar-aux-loss: 93.19905525
Epoch: [27 ] train-acc: 0.92178571, dom-acc: 0.64696429, val-acc: 0.93000000, val_loss: 0.19688429
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.90577650, sen-loss: 24.73767423, dom-loss: 78.01057297, src-aux-loss: 96.46330202, tar-aux-loss: 92.69422793
Epoch: [28 ] train-acc: 0.92428571, dom-acc: 0.65910714, val-acc: 0.92750000, val_loss: 0.19814688
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 291.51046634, sen-loss: 24.50871278, dom-loss: 77.70550346, src-aux-loss: 95.90688336, tar-aux-loss: 93.38936681
Epoch: [29 ] train-acc: 0.92392857, dom-acc: 0.65035714, val-acc: 0.93000000, val_loss: 0.19550326
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.16470599, sen-loss: 24.07471412, dom-loss: 77.85487306, src-aux-loss: 95.65256065, tar-aux-loss: 91.58255714
Epoch: [30 ] train-acc: 0.92732143, dom-acc: 0.66446429, val-acc: 0.92500000, val_loss: 0.19852340
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 289.55093336, sen-loss: 23.91404254, dom-loss: 77.92104793, src-aux-loss: 95.10113341, tar-aux-loss: 92.61470985
Epoch: [31 ] train-acc: 0.92964286, dom-acc: 0.65482143, val-acc: 0.93250000, val_loss: 0.19703737
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 287.84091043, sen-loss: 23.54875538, dom-loss: 77.40778148, src-aux-loss: 94.90995312, tar-aux-loss: 91.97441846
Epoch: [32 ] train-acc: 0.92482143, dom-acc: 0.65437500, val-acc: 0.93500000, val_loss: 0.19496517
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 286.96870542, sen-loss: 23.35516805, dom-loss: 77.64324433, src-aux-loss: 94.55223316, tar-aux-loss: 91.41806024
Epoch: [33 ] train-acc: 0.93000000, dom-acc: 0.66812500, val-acc: 0.93500000, val_loss: 0.19474655
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 287.34499025, sen-loss: 23.13755804, dom-loss: 77.64694178, src-aux-loss: 94.28068739, tar-aux-loss: 92.27980310
Epoch: [34 ] train-acc: 0.93107143, dom-acc: 0.66651786, val-acc: 0.93000000, val_loss: 0.19325219
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 286.37498498, sen-loss: 22.80822465, dom-loss: 77.51059389, src-aux-loss: 93.67773783, tar-aux-loss: 92.37842840
Epoch: [35 ] train-acc: 0.93125000, dom-acc: 0.66803571, val-acc: 0.93750000, val_loss: 0.19387671
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 285.91575742, sen-loss: 22.51902984, dom-loss: 77.40832406, src-aux-loss: 93.62565529, tar-aux-loss: 92.36274958
Epoch: [36 ] train-acc: 0.93321429, dom-acc: 0.66919643, val-acc: 0.93500000, val_loss: 0.18962000
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 284.01158714, sen-loss: 22.40875778, dom-loss: 77.32878035, src-aux-loss: 93.00345021, tar-aux-loss: 91.27059811
Epoch: [37 ] train-acc: 0.93428571, dom-acc: 0.65955357, val-acc: 0.93750000, val_loss: 0.18959279
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 282.87549496, sen-loss: 22.02791502, dom-loss: 77.04053754, src-aux-loss: 92.56364214, tar-aux-loss: 91.24339980
Epoch: [38 ] train-acc: 0.93160714, dom-acc: 0.67178571, val-acc: 0.93500000, val_loss: 0.19568451
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 283.09836364, sen-loss: 21.92052564, dom-loss: 77.33034438, src-aux-loss: 92.11399752, tar-aux-loss: 91.73349583
Epoch: [39 ] train-acc: 0.93553571, dom-acc: 0.67000000, val-acc: 0.93750000, val_loss: 0.18804803
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 281.19051981, sen-loss: 21.56163723, dom-loss: 77.15777278, src-aux-loss: 91.88379598, tar-aux-loss: 90.58731347
Epoch: [40 ] train-acc: 0.93660714, dom-acc: 0.66687500, val-acc: 0.93750000, val_loss: 0.18815669
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 281.17472291, sen-loss: 21.45652606, dom-loss: 77.11675394, src-aux-loss: 91.48434442, tar-aux-loss: 91.11709857
Epoch: [41 ] train-acc: 0.93678571, dom-acc: 0.67678571, val-acc: 0.93750000, val_loss: 0.18813436
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 280.64109349, sen-loss: 21.04201988, dom-loss: 76.88328797, src-aux-loss: 91.11214250, tar-aux-loss: 91.60364425
Epoch: [42 ] train-acc: 0.93910714, dom-acc: 0.67571429, val-acc: 0.94000000, val_loss: 0.18788901
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 278.97635388, sen-loss: 20.90360066, dom-loss: 77.03016901, src-aux-loss: 90.52355951, tar-aux-loss: 90.51902395
Epoch: [43 ] train-acc: 0.94071429, dom-acc: 0.67160714, val-acc: 0.93750000, val_loss: 0.18839899
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 279.48873401, sen-loss: 20.75863350, dom-loss: 76.92674255, src-aux-loss: 90.34300074, tar-aux-loss: 91.46035689
Epoch: [44 ] train-acc: 0.94232143, dom-acc: 0.67446429, val-acc: 0.94000000, val_loss: 0.18852721
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 278.20728779, sen-loss: 20.49400412, dom-loss: 76.74086726, src-aux-loss: 89.89974201, tar-aux-loss: 91.07267636
Epoch: [45 ] train-acc: 0.94285714, dom-acc: 0.67241071, val-acc: 0.93750000, val_loss: 0.18680906
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 277.82536125, sen-loss: 20.19743829, dom-loss: 76.89626348, src-aux-loss: 89.58239317, tar-aux-loss: 91.14926654
Epoch: [46 ] train-acc: 0.94446429, dom-acc: 0.66892857, val-acc: 0.93250000, val_loss: 0.18749636
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 276.37426734, sen-loss: 19.94927250, dom-loss: 77.02386039, src-aux-loss: 89.25578707, tar-aux-loss: 90.14534605
Epoch: [47 ] train-acc: 0.93964286, dom-acc: 0.66250000, val-acc: 0.93750000, val_loss: 0.19319251
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 275.70489240, sen-loss: 19.81778448, dom-loss: 76.44802207, src-aux-loss: 88.83686477, tar-aux-loss: 90.60222173
Epoch: [48 ] train-acc: 0.94446429, dom-acc: 0.67125000, val-acc: 0.93500000, val_loss: 0.18818209
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 276.80306721, sen-loss: 19.53631870, dom-loss: 76.92013800, src-aux-loss: 88.31262726, tar-aux-loss: 92.03398263
Epoch: [49 ] train-acc: 0.94446429, dom-acc: 0.66991071, val-acc: 0.93500000, val_loss: 0.18647398
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 273.31808066, sen-loss: 19.34710139, dom-loss: 76.77755541, src-aux-loss: 87.83725375, tar-aux-loss: 89.35617173
Epoch: [50 ] train-acc: 0.94607143, dom-acc: 0.66830357, val-acc: 0.93000000, val_loss: 0.18774435
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 273.87894225, sen-loss: 19.13961564, dom-loss: 76.75836259, src-aux-loss: 87.36792457, tar-aux-loss: 90.61303967
Epoch: [51 ] train-acc: 0.94571429, dom-acc: 0.67392857, val-acc: 0.93250000, val_loss: 0.18883538
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 272.60755754, sen-loss: 18.84584802, dom-loss: 76.72798061, src-aux-loss: 87.05389714, tar-aux-loss: 89.97983217
Epoch: [52 ] train-acc: 0.94428571, dom-acc: 0.67517857, val-acc: 0.93500000, val_loss: 0.19124945
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 274.24185824, sen-loss: 18.66261177, dom-loss: 76.95950335, src-aux-loss: 86.81753969, tar-aux-loss: 91.80220354
Epoch: [53 ] train-acc: 0.95000000, dom-acc: 0.67419643, val-acc: 0.93500000, val_loss: 0.18893899
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [54 ] loss: 272.07882595, sen-loss: 18.31189102, dom-loss: 76.93533492, src-aux-loss: 86.38383591, tar-aux-loss: 90.44776464
Epoch: [54 ] train-acc: 0.94642857, dom-acc: 0.66491071, val-acc: 0.93250000, val_loss: 0.19171774
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_kitchen_HATN.ckpt
Best Epoch: [ 49] best val accuracy: 0.00000000 best val loss: 0.18647398
Testing accuracy: 0.90116667
./work/attentions/electronics_kitchen_train_HATN.txt
./work/attentions/electronics_kitchen_test_HATN.txt
loading data...
source domain:  electronics target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 30180
vocab-size:  83059
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'fantastic', 'happy', 'awesome', 'solid', 'amazing', 'highly', 'satisfied', 'durable', 'reliable', 'worth', 'quick', 'perfectly', 'outstanding', 'love', 'impressive', 'recommend', 'fast', 'pleased', 'simple', 'nice', 'recommended', 'exactly', 'impressed', 'cheaper', 'inexpensive', 'decent', 'old', 'awsome', 'far', 'useful', 'slick', 'effective', 'expected', 'advertised']
['returned', 'poor', 'return', 'stopped', 'disappointed', 'disappointing', 'useless', 'frustrating', 'bad', 'worst', 'awful', 'horrible', 'unreliable', 'failed', 'cheap', 'hard', 'died', 'impossible', 'broke', 'expensive', 'wrong', 'poorly', 'defective', 'overpriced', 'unacceptable', 'returning', 'misleading', 'terrible', 'quit', 'broken', 'short', 'worthless', 'much', 'beware', 'flimsy', 'save', 'ridiculous', 'lousy', 'lasted', 'missing', 'unhappy', 'trying', 'slow', 'low', 'uncomfortable', 'cannot', 'waste', 'difficult', 'flawed', 'sent', 'incompatible', 'tried', 'acceptable', 'average', 'lose', 'nowhere', 'waiting', 'unable', 'mediocre', 'sad', 'inferior']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
83059
5600 400 6000 23009 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 411.63538623, sen-loss: 77.46719193, dom-loss: 76.48920590, src-aux-loss: 133.23636186, tar-aux-loss: 124.44262522
Epoch: [1  ] train-acc: 0.71375000, dom-acc: 0.86285714, val-acc: 0.71250000, val_loss: 0.65222234
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 382.16699553, sen-loss: 68.99286824, dom-loss: 73.06212431, src-aux-loss: 123.90921623, tar-aux-loss: 116.20278651
Epoch: [2  ] train-acc: 0.75339286, dom-acc: 0.84901786, val-acc: 0.77500000, val_loss: 0.57068050
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 362.33343506, sen-loss: 58.48173451, dom-loss: 71.73214573, src-aux-loss: 119.14204746, tar-aux-loss: 112.97750860
Epoch: [3  ] train-acc: 0.80428571, dom-acc: 0.73401786, val-acc: 0.84000000, val_loss: 0.45325842
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 345.44973016, sen-loss: 48.59703055, dom-loss: 71.82452774, src-aux-loss: 115.57638770, tar-aux-loss: 109.45178217
Epoch: [4  ] train-acc: 0.82517857, dom-acc: 0.62250000, val-acc: 0.85500000, val_loss: 0.38321149
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 338.62162709, sen-loss: 44.38147199, dom-loss: 72.70447403, src-aux-loss: 113.19322896, tar-aux-loss: 108.34245265
Epoch: [5  ] train-acc: 0.84625000, dom-acc: 0.59866071, val-acc: 0.87250000, val_loss: 0.34719583
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 333.60768652, sen-loss: 41.02640158, dom-loss: 74.14360386, src-aux-loss: 111.17561680, tar-aux-loss: 107.26206487
Epoch: [6  ] train-acc: 0.85785714, dom-acc: 0.58991071, val-acc: 0.88250000, val_loss: 0.31749117
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 328.16108084, sen-loss: 38.22997814, dom-loss: 75.45764601, src-aux-loss: 109.20778495, tar-aux-loss: 105.26567245
Epoch: [7  ] train-acc: 0.86553571, dom-acc: 0.54044643, val-acc: 0.89000000, val_loss: 0.29699945
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 325.63748288, sen-loss: 36.34697674, dom-loss: 76.42955875, src-aux-loss: 108.10227120, tar-aux-loss: 104.75867593
Epoch: [8  ] train-acc: 0.87660714, dom-acc: 0.49258929, val-acc: 0.89750000, val_loss: 0.28268948
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 322.92155719, sen-loss: 35.03317453, dom-loss: 77.63465321, src-aux-loss: 106.65408385, tar-aux-loss: 103.59964466
Epoch: [9  ] train-acc: 0.88285714, dom-acc: 0.46678571, val-acc: 0.90250000, val_loss: 0.26675731
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 322.01305509, sen-loss: 33.69945090, dom-loss: 78.69105059, src-aux-loss: 105.63323665, tar-aux-loss: 103.98931885
Epoch: [10 ] train-acc: 0.88785714, dom-acc: 0.43437500, val-acc: 0.91250000, val_loss: 0.25609076
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 319.07203579, sen-loss: 32.60831939, dom-loss: 79.36781967, src-aux-loss: 105.09829187, tar-aux-loss: 101.99760324
Epoch: [11 ] train-acc: 0.89053571, dom-acc: 0.40812500, val-acc: 0.92000000, val_loss: 0.24657261
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 317.51319742, sen-loss: 31.84649999, dom-loss: 79.58461493, src-aux-loss: 103.94190508, tar-aux-loss: 102.14017612
Epoch: [12 ] train-acc: 0.89535714, dom-acc: 0.39178571, val-acc: 0.92000000, val_loss: 0.23829377
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.64215446, sen-loss: 30.99581261, dom-loss: 79.58707619, src-aux-loss: 103.16858375, tar-aux-loss: 100.89068341
Epoch: [13 ] train-acc: 0.89428571, dom-acc: 0.40053571, val-acc: 0.92250000, val_loss: 0.23484749
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 314.03867841, sen-loss: 30.48369522, dom-loss: 79.50200278, src-aux-loss: 102.65340185, tar-aux-loss: 101.39957803
Epoch: [14 ] train-acc: 0.90125000, dom-acc: 0.39294643, val-acc: 0.91500000, val_loss: 0.23058359
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 311.67446518, sen-loss: 29.73817970, dom-loss: 78.99679238, src-aux-loss: 101.92196006, tar-aux-loss: 101.01753360
Epoch: [15 ] train-acc: 0.90357143, dom-acc: 0.42017857, val-acc: 0.91500000, val_loss: 0.22481279
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 309.44190788, sen-loss: 29.15933439, dom-loss: 78.54743326, src-aux-loss: 101.68658781, tar-aux-loss: 100.04855347
Epoch: [16 ] train-acc: 0.90375000, dom-acc: 0.43598214, val-acc: 0.91500000, val_loss: 0.21996784
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 306.36641335, sen-loss: 28.63001402, dom-loss: 78.15307266, src-aux-loss: 100.91268224, tar-aux-loss: 98.67064202
Epoch: [17 ] train-acc: 0.90857143, dom-acc: 0.46116071, val-acc: 0.92250000, val_loss: 0.22116493
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 306.06962705, sen-loss: 28.27443181, dom-loss: 77.84359848, src-aux-loss: 100.21713364, tar-aux-loss: 99.73446447
Epoch: [18 ] train-acc: 0.90928571, dom-acc: 0.49232143, val-acc: 0.92250000, val_loss: 0.21799968
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 303.81729078, sen-loss: 27.68255559, dom-loss: 77.62949526, src-aux-loss: 99.72013110, tar-aux-loss: 98.78511065
Epoch: [19 ] train-acc: 0.91160714, dom-acc: 0.51000000, val-acc: 0.92000000, val_loss: 0.21084438
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 303.39723158, sen-loss: 27.33907832, dom-loss: 77.13687444, src-aux-loss: 99.06541145, tar-aux-loss: 99.85586554
Epoch: [20 ] train-acc: 0.91178571, dom-acc: 0.51294643, val-acc: 0.92500000, val_loss: 0.21372263
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 301.74188709, sen-loss: 26.96297412, dom-loss: 76.92067498, src-aux-loss: 98.99481994, tar-aux-loss: 98.86341822
Epoch: [21 ] train-acc: 0.91196429, dom-acc: 0.51125000, val-acc: 0.92500000, val_loss: 0.20671946
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 300.13985729, sen-loss: 26.57616191, dom-loss: 76.90489167, src-aux-loss: 98.14456958, tar-aux-loss: 98.51423413
Epoch: [22 ] train-acc: 0.91321429, dom-acc: 0.51901786, val-acc: 0.92250000, val_loss: 0.20529585
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 297.38245988, sen-loss: 26.16548172, dom-loss: 76.76977187, src-aux-loss: 97.73419213, tar-aux-loss: 96.71301198
Epoch: [23 ] train-acc: 0.91339286, dom-acc: 0.52107143, val-acc: 0.92500000, val_loss: 0.20430627
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 300.67127848, sen-loss: 25.76639929, dom-loss: 76.90385604, src-aux-loss: 97.74083346, tar-aux-loss: 100.26018804
Epoch: [24 ] train-acc: 0.92035714, dom-acc: 0.51928571, val-acc: 0.92750000, val_loss: 0.20449799
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 298.89957452, sen-loss: 25.49264000, dom-loss: 77.23100126, src-aux-loss: 96.88647163, tar-aux-loss: 99.28946155
Epoch: [25 ] train-acc: 0.91732143, dom-acc: 0.51446429, val-acc: 0.93250000, val_loss: 0.20249206
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.60786486, sen-loss: 25.27348486, dom-loss: 77.36369461, src-aux-loss: 96.43157572, tar-aux-loss: 96.53911078
Epoch: [26 ] train-acc: 0.92285714, dom-acc: 0.49196429, val-acc: 0.92750000, val_loss: 0.20307232
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 295.08950186, sen-loss: 24.93383253, dom-loss: 77.55521691, src-aux-loss: 95.86053133, tar-aux-loss: 96.73992199
Epoch: [27 ] train-acc: 0.92142857, dom-acc: 0.49464286, val-acc: 0.92500000, val_loss: 0.19955719
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 296.06772947, sen-loss: 24.76556754, dom-loss: 77.73480189, src-aux-loss: 95.67107385, tar-aux-loss: 97.89628619
Epoch: [28 ] train-acc: 0.92357143, dom-acc: 0.46821429, val-acc: 0.92500000, val_loss: 0.20066090
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 296.34303904, sen-loss: 24.54158561, dom-loss: 78.11446917, src-aux-loss: 95.18204957, tar-aux-loss: 98.50493455
Epoch: [29 ] train-acc: 0.92089286, dom-acc: 0.46455357, val-acc: 0.93250000, val_loss: 0.19846047
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 292.62697935, sen-loss: 24.05463459, dom-loss: 78.37339371, src-aux-loss: 94.65547222, tar-aux-loss: 95.54348058
Epoch: [30 ] train-acc: 0.92750000, dom-acc: 0.45642857, val-acc: 0.93000000, val_loss: 0.20142528
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 293.20353127, sen-loss: 23.87890016, dom-loss: 78.55445617, src-aux-loss: 94.25185609, tar-aux-loss: 96.51831895
Epoch: [31 ] train-acc: 0.92750000, dom-acc: 0.43125000, val-acc: 0.92500000, val_loss: 0.19808796
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 293.88246346, sen-loss: 23.49128792, dom-loss: 78.80966026, src-aux-loss: 93.88986260, tar-aux-loss: 97.69165272
Epoch: [32 ] train-acc: 0.92375000, dom-acc: 0.44071429, val-acc: 0.93750000, val_loss: 0.19729432
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 293.28191996, sen-loss: 23.29720638, dom-loss: 78.87438279, src-aux-loss: 93.49515200, tar-aux-loss: 97.61517757
Epoch: [33 ] train-acc: 0.92857143, dom-acc: 0.43616071, val-acc: 0.93000000, val_loss: 0.19877069
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 291.46129751, sen-loss: 23.05537694, dom-loss: 78.85414428, src-aux-loss: 93.23384702, tar-aux-loss: 96.31792945
Epoch: [34 ] train-acc: 0.93089286, dom-acc: 0.43508929, val-acc: 0.93000000, val_loss: 0.19535193
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 291.10373902, sen-loss: 22.74313594, dom-loss: 78.75843430, src-aux-loss: 92.63999718, tar-aux-loss: 96.96217149
Epoch: [35 ] train-acc: 0.92875000, dom-acc: 0.42419643, val-acc: 0.93000000, val_loss: 0.19848774
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 289.12674046, sen-loss: 22.43099584, dom-loss: 78.64915657, src-aux-loss: 92.30393863, tar-aux-loss: 95.74264997
Epoch: [36 ] train-acc: 0.93125000, dom-acc: 0.43875000, val-acc: 0.93750000, val_loss: 0.19205037
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 288.56046963, sen-loss: 22.32085878, dom-loss: 78.50016040, src-aux-loss: 91.90154678, tar-aux-loss: 95.83790231
Epoch: [37 ] train-acc: 0.93446429, dom-acc: 0.44642857, val-acc: 0.94000000, val_loss: 0.19170251
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 287.79089737, sen-loss: 21.91988385, dom-loss: 78.22592866, src-aux-loss: 91.27801722, tar-aux-loss: 96.36706835
Epoch: [38 ] train-acc: 0.93410714, dom-acc: 0.44026786, val-acc: 0.93000000, val_loss: 0.19385317
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 286.98248005, sen-loss: 21.80013360, dom-loss: 77.99616057, src-aux-loss: 90.74064213, tar-aux-loss: 96.44554341
Epoch: [39 ] train-acc: 0.93607143, dom-acc: 0.44928571, val-acc: 0.93750000, val_loss: 0.19109958
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 285.15178943, sen-loss: 21.48462560, dom-loss: 77.79063332, src-aux-loss: 90.53521156, tar-aux-loss: 95.34132040
Epoch: [40 ] train-acc: 0.93785714, dom-acc: 0.47241071, val-acc: 0.93750000, val_loss: 0.19090381
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 284.24522758, sen-loss: 21.31035988, dom-loss: 77.55450970, src-aux-loss: 90.09675670, tar-aux-loss: 95.28360087
Epoch: [41 ] train-acc: 0.93607143, dom-acc: 0.48383929, val-acc: 0.93750000, val_loss: 0.19192718
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 283.49275732, sen-loss: 20.96498336, dom-loss: 77.18306065, src-aux-loss: 89.65918308, tar-aux-loss: 95.68552899
Epoch: [42 ] train-acc: 0.93910714, dom-acc: 0.48892857, val-acc: 0.93750000, val_loss: 0.18979003
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 282.27549219, sen-loss: 20.80445898, dom-loss: 77.11128151, src-aux-loss: 88.92964131, tar-aux-loss: 95.43011051
Epoch: [43 ] train-acc: 0.94035714, dom-acc: 0.50214286, val-acc: 0.93250000, val_loss: 0.19007929
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 281.18405914, sen-loss: 20.65209184, dom-loss: 76.95802957, src-aux-loss: 88.65139404, tar-aux-loss: 94.92254436
Epoch: [44 ] train-acc: 0.94107143, dom-acc: 0.52910714, val-acc: 0.93500000, val_loss: 0.18992035
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 280.13710737, sen-loss: 20.42779072, dom-loss: 76.92808038, src-aux-loss: 88.39708263, tar-aux-loss: 94.38415343
Epoch: [45 ] train-acc: 0.94125000, dom-acc: 0.52258929, val-acc: 0.93500000, val_loss: 0.18741521
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 279.97192812, sen-loss: 20.15285540, dom-loss: 76.98355424, src-aux-loss: 87.95386124, tar-aux-loss: 94.88165575
Epoch: [46 ] train-acc: 0.94375000, dom-acc: 0.51651786, val-acc: 0.93000000, val_loss: 0.18909182
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 278.55200315, sen-loss: 19.87771924, dom-loss: 76.81351382, src-aux-loss: 87.33769661, tar-aux-loss: 94.52307308
Epoch: [47 ] train-acc: 0.93839286, dom-acc: 0.51250000, val-acc: 0.93000000, val_loss: 0.19542865
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 279.49468613, sen-loss: 19.67526980, dom-loss: 77.16176498, src-aux-loss: 87.07796800, tar-aux-loss: 95.57968193
Epoch: [48 ] train-acc: 0.94464286, dom-acc: 0.54223214, val-acc: 0.93250000, val_loss: 0.18960626
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 277.97051430, sen-loss: 19.39814046, dom-loss: 77.17014301, src-aux-loss: 86.60626769, tar-aux-loss: 94.79596215
Epoch: [49 ] train-acc: 0.94553571, dom-acc: 0.50651786, val-acc: 0.93250000, val_loss: 0.18765095
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 275.77528787, sen-loss: 19.22886594, dom-loss: 77.33384079, src-aux-loss: 86.00845492, tar-aux-loss: 93.20412666
Epoch: [50 ] train-acc: 0.94660714, dom-acc: 0.51071429, val-acc: 0.93500000, val_loss: 0.18850684
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_video_HATN.ckpt
Best Epoch: [ 45] best val accuracy: 0.00000000 best val loss: 0.18741521
Testing accuracy: 0.83833333
./work/attentions/electronics_video_train_HATN.txt
./work/attentions/electronics_video_test_HATN.txt
loading data...
source domain:  kitchen target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 9750
vocab-size:  78006
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'wonderful', 'pleased', 'loves', 'fantastic', 'amazing', 'highly', 'awesome', 'satisfied', 'fabulous', 'attractive', 'nice', 'well', 'favorite', 'terrific', 'incredible', 'fast', 'quick', 'simple', 'loved', 'outstanding', 'useful', 'solid', 'elegant', 'better', 'tough', 'perfectly', 'decent', 'surprised', 'far']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'useless', 'broke', 'horrible', 'bad', 'poorly', 'broken', 'defective', 'save', 'impossible', 'returned', 'flimsy', 'awful', 'expensive', 'wrong', 'dissapointed', 'dangerous', 'ruined', 'overpriced', 'difficult', 'failed', 'worthless', 'worked', 'frustrating', 'stopped', 'misleading', 'unhappy', 'flawed', 'hard', 'quit', 'disapointed', 'worse', 'sad', 'messy', 'ugly', 'unacceptable', 'return', 'weak', 'lousy', 'ridiculous', 'shattered', 'hated', 'overrated', 'shocked', 'waiting', 'stuck', 'leaky']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
78006
5600 400 6000 19856 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 415.78650117, sen-loss: 76.78798854, dom-loss: 77.93006712, src-aux-loss: 140.43823802, tar-aux-loss: 120.63020933
Epoch: [1  ] train-acc: 0.76303571, dom-acc: 0.80125000, val-acc: 0.76250000, val_loss: 0.63788205
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 385.06474638, sen-loss: 66.53886801, dom-loss: 74.65668219, src-aux-loss: 131.30018395, tar-aux-loss: 112.56901306
Epoch: [2  ] train-acc: 0.78160714, dom-acc: 0.83062500, val-acc: 0.79500000, val_loss: 0.53520089
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 360.86073327, sen-loss: 54.67158735, dom-loss: 73.49242866, src-aux-loss: 124.95082277, tar-aux-loss: 107.74589437
Epoch: [3  ] train-acc: 0.83660714, dom-acc: 0.71633929, val-acc: 0.84000000, val_loss: 0.42147204
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 342.76445603, sen-loss: 43.75372970, dom-loss: 73.25744200, src-aux-loss: 120.67224532, tar-aux-loss: 105.08103937
Epoch: [4  ] train-acc: 0.85767857, dom-acc: 0.59214286, val-acc: 0.86500000, val_loss: 0.35345954
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 335.62231207, sen-loss: 38.66635102, dom-loss: 73.96684241, src-aux-loss: 117.80793035, tar-aux-loss: 105.18118864
Epoch: [5  ] train-acc: 0.87946429, dom-acc: 0.57410714, val-acc: 0.88500000, val_loss: 0.33027095
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 327.42443776, sen-loss: 35.83501710, dom-loss: 74.31089389, src-aux-loss: 115.33755881, tar-aux-loss: 101.94096738
Epoch: [6  ] train-acc: 0.88821429, dom-acc: 0.54598214, val-acc: 0.89250000, val_loss: 0.31552616
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 324.23826933, sen-loss: 33.54095723, dom-loss: 75.59658098, src-aux-loss: 113.44641495, tar-aux-loss: 101.65431428
Epoch: [7  ] train-acc: 0.89017857, dom-acc: 0.52535714, val-acc: 0.90250000, val_loss: 0.30595994
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 321.34102678, sen-loss: 32.00210524, dom-loss: 76.21040267, src-aux-loss: 112.35264826, tar-aux-loss: 100.77587157
Epoch: [8  ] train-acc: 0.89767857, dom-acc: 0.50571429, val-acc: 0.91000000, val_loss: 0.29893139
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 319.47700906, sen-loss: 30.35207646, dom-loss: 77.12907243, src-aux-loss: 110.83486164, tar-aux-loss: 101.16099918
Epoch: [9  ] train-acc: 0.90160714, dom-acc: 0.47633929, val-acc: 0.89500000, val_loss: 0.30875337
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 316.06053329, sen-loss: 29.52397417, dom-loss: 77.47185075, src-aux-loss: 109.96605355, tar-aux-loss: 99.09865505
Epoch: [10 ] train-acc: 0.90517857, dom-acc: 0.46241071, val-acc: 0.91000000, val_loss: 0.29359493
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 314.82300448, sen-loss: 28.54158062, dom-loss: 77.98173136, src-aux-loss: 109.14899546, tar-aux-loss: 99.15069789
Epoch: [11 ] train-acc: 0.90750000, dom-acc: 0.45133929, val-acc: 0.91500000, val_loss: 0.28743446
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 313.72288013, sen-loss: 27.81913271, dom-loss: 78.12094271, src-aux-loss: 108.08918494, tar-aux-loss: 99.69361734
Epoch: [12 ] train-acc: 0.90642857, dom-acc: 0.44357143, val-acc: 0.92500000, val_loss: 0.28500989
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 310.56307268, sen-loss: 27.21615744, dom-loss: 78.29501963, src-aux-loss: 107.16742408, tar-aux-loss: 97.88447052
Epoch: [13 ] train-acc: 0.90964286, dom-acc: 0.45258929, val-acc: 0.89500000, val_loss: 0.30611926
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 311.31398940, sen-loss: 26.64248847, dom-loss: 78.68801647, src-aux-loss: 106.52835917, tar-aux-loss: 99.45512658
Epoch: [14 ] train-acc: 0.91160714, dom-acc: 0.45955357, val-acc: 0.90750000, val_loss: 0.29494864
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 308.37720609, sen-loss: 26.41826456, dom-loss: 78.68857616, src-aux-loss: 105.71673822, tar-aux-loss: 97.55362636
Epoch: [15 ] train-acc: 0.91625000, dom-acc: 0.46508929, val-acc: 0.91500000, val_loss: 0.28158081
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 307.39858031, sen-loss: 25.60202240, dom-loss: 78.77131826, src-aux-loss: 105.05312371, tar-aux-loss: 97.97211552
Epoch: [16 ] train-acc: 0.91517857, dom-acc: 0.47696429, val-acc: 0.91250000, val_loss: 0.28301996
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 305.07739234, sen-loss: 25.16143130, dom-loss: 78.68968582, src-aux-loss: 104.31114447, tar-aux-loss: 96.91512996
Epoch: [17 ] train-acc: 0.91821429, dom-acc: 0.47633929, val-acc: 0.91000000, val_loss: 0.28836536
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 305.67226243, sen-loss: 24.81852747, dom-loss: 78.69048351, src-aux-loss: 103.76510191, tar-aux-loss: 98.39815021
Epoch: [18 ] train-acc: 0.91946429, dom-acc: 0.49303571, val-acc: 0.91000000, val_loss: 0.28489676
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 302.84530783, sen-loss: 24.60740496, dom-loss: 78.60550350, src-aux-loss: 103.22162366, tar-aux-loss: 96.41077578
Epoch: [19 ] train-acc: 0.92178571, dom-acc: 0.49125000, val-acc: 0.91500000, val_loss: 0.28288081
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 301.57544231, sen-loss: 24.15526286, dom-loss: 78.63291061, src-aux-loss: 102.57864457, tar-aux-loss: 96.20862323
Epoch: [20 ] train-acc: 0.92285714, dom-acc: 0.51607143, val-acc: 0.92250000, val_loss: 0.27533233
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 302.02810168, sen-loss: 23.92511515, dom-loss: 78.20033133, src-aux-loss: 102.06046349, tar-aux-loss: 97.84219265
Epoch: [21 ] train-acc: 0.92375000, dom-acc: 0.49982143, val-acc: 0.92250000, val_loss: 0.27269652
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 300.11670089, sen-loss: 23.71421605, dom-loss: 78.10065341, src-aux-loss: 101.36056530, tar-aux-loss: 96.94126779
Epoch: [22 ] train-acc: 0.92482143, dom-acc: 0.49339286, val-acc: 0.91250000, val_loss: 0.27598432
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 297.89275074, sen-loss: 23.26964224, dom-loss: 78.01186746, src-aux-loss: 100.91037065, tar-aux-loss: 95.70087093
Epoch: [23 ] train-acc: 0.92696429, dom-acc: 0.50794643, val-acc: 0.91500000, val_loss: 0.27330506
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 297.89929581, sen-loss: 23.11266409, dom-loss: 77.78973645, src-aux-loss: 100.45918536, tar-aux-loss: 96.53771073
Epoch: [24 ] train-acc: 0.92732143, dom-acc: 0.50991071, val-acc: 0.91750000, val_loss: 0.28014067
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 296.92589378, sen-loss: 22.70493775, dom-loss: 77.67935741, src-aux-loss: 99.60166037, tar-aux-loss: 96.93993890
Epoch: [25 ] train-acc: 0.92803571, dom-acc: 0.53830357, val-acc: 0.91750000, val_loss: 0.27154601
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 294.99874234, sen-loss: 22.49585907, dom-loss: 77.47795486, src-aux-loss: 99.47985822, tar-aux-loss: 95.54507107
Epoch: [26 ] train-acc: 0.93000000, dom-acc: 0.51026786, val-acc: 0.91500000, val_loss: 0.27556056
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 293.60350728, sen-loss: 22.33138334, dom-loss: 77.32339847, src-aux-loss: 98.86414492, tar-aux-loss: 95.08458048
Epoch: [27 ] train-acc: 0.93071429, dom-acc: 0.51651786, val-acc: 0.91250000, val_loss: 0.27470863
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 293.82884908, sen-loss: 22.24600222, dom-loss: 77.15353650, src-aux-loss: 98.34991807, tar-aux-loss: 96.07939225
Epoch: [28 ] train-acc: 0.92982143, dom-acc: 0.51991071, val-acc: 0.92000000, val_loss: 0.26723957
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 292.32634568, sen-loss: 21.95758750, dom-loss: 77.27719700, src-aux-loss: 97.70930976, tar-aux-loss: 95.38225323
Epoch: [29 ] train-acc: 0.93160714, dom-acc: 0.53651786, val-acc: 0.91750000, val_loss: 0.26874432
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 291.56608152, sen-loss: 21.70474271, dom-loss: 76.97436607, src-aux-loss: 97.27902222, tar-aux-loss: 95.60794920
Epoch: [30 ] train-acc: 0.93267857, dom-acc: 0.52294643, val-acc: 0.91750000, val_loss: 0.27530274
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 290.49981022, sen-loss: 21.55276182, dom-loss: 76.95234185, src-aux-loss: 96.81646901, tar-aux-loss: 95.17823935
Epoch: [31 ] train-acc: 0.93357143, dom-acc: 0.52714286, val-acc: 0.91750000, val_loss: 0.27366132
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 289.23379707, sen-loss: 21.30419807, dom-loss: 77.06175125, src-aux-loss: 96.29269856, tar-aux-loss: 94.57514936
Epoch: [32 ] train-acc: 0.93517857, dom-acc: 0.52866071, val-acc: 0.91750000, val_loss: 0.27408230
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 288.52870703, sen-loss: 21.01197468, dom-loss: 76.77639985, src-aux-loss: 95.67879575, tar-aux-loss: 95.06153625
Epoch: [33 ] train-acc: 0.93517857, dom-acc: 0.49830357, val-acc: 0.91750000, val_loss: 0.27364990
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_books_HATN.ckpt
Best Epoch: [ 28] best val accuracy: 0.00000000 best val loss: 0.26723957
Testing accuracy: 0.85216667
./work/attentions/kitchen_books_train_HATN.txt
./work/attentions/kitchen_books_test_HATN.txt
loading data...
source domain:  kitchen target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 11843
vocab-size:  80685
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'pleased', 'wonderful', 'loves', 'fantastic', 'highly', 'amazing', 'awesome', 'satisfied', 'fabulous', 'nice', 'attractive', 'well', 'terrific', 'fast', 'favorite', 'incredible', 'quick', 'useful', 'solid', 'simple', 'outstanding', 'elegant', 'tough', 'easier', 'perfectly', 'decent', 'surprised', 'durable', 'far', 'impressed']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'useless', 'broke', 'horrible', 'poorly', 'bad', 'broken', 'defective', 'save', 'returned', 'flimsy', 'expensive', 'awful', 'impossible', 'wrong', 'dangerous', 'dissapointed', 'failed', 'overpriced', 'stopped', 'ruined', 'difficult', 'worthless', 'hard', 'frustrating', 'misleading', 'unhappy', 'better', 'quit', 'disapointed', 'flawed', 'worse', 'worked', 'sad', 'return', 'messy', 'weak', 'lousy', 'stuck', 'shattered', 'embarrassed', 'fell', 'overrated', 'shocked', 'waiting', 'unacceptable', 'leaky']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
80685
5600 400 6000 19856 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 419.56657028, sen-loss: 76.82041907, dom-loss: 77.09147990, src-aux-loss: 141.91199160, tar-aux-loss: 123.74267900
Epoch: [1  ] train-acc: 0.76267857, dom-acc: 0.81500000, val-acc: 0.76500000, val_loss: 0.63841271
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 388.84997988, sen-loss: 66.73009771, dom-loss: 73.89278096, src-aux-loss: 132.99456817, tar-aux-loss: 115.23253286
Epoch: [2  ] train-acc: 0.77321429, dom-acc: 0.79017857, val-acc: 0.79000000, val_loss: 0.53873354
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 365.15632200, sen-loss: 54.91961387, dom-loss: 72.73048550, src-aux-loss: 126.93974715, tar-aux-loss: 110.56647682
Epoch: [3  ] train-acc: 0.83964286, dom-acc: 0.68366071, val-acc: 0.83750000, val_loss: 0.42391887
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.81608653, sen-loss: 43.89347224, dom-loss: 72.66374612, src-aux-loss: 122.81308085, tar-aux-loss: 108.44578910
Epoch: [4  ] train-acc: 0.85535714, dom-acc: 0.58562500, val-acc: 0.86500000, val_loss: 0.35514107
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 339.60632920, sen-loss: 39.06601599, dom-loss: 73.90987647, src-aux-loss: 119.96415013, tar-aux-loss: 106.66628712
Epoch: [5  ] train-acc: 0.87785714, dom-acc: 0.58098214, val-acc: 0.88250000, val_loss: 0.33082357
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 332.59416533, sen-loss: 35.98239508, dom-loss: 74.03769523, src-aux-loss: 117.74136186, tar-aux-loss: 104.83271259
Epoch: [6  ] train-acc: 0.88375000, dom-acc: 0.54633929, val-acc: 0.88750000, val_loss: 0.31476882
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 329.76350784, sen-loss: 33.75123106, dom-loss: 75.61330521, src-aux-loss: 115.69644648, tar-aux-loss: 104.70252490
Epoch: [7  ] train-acc: 0.88964286, dom-acc: 0.52419643, val-acc: 0.89750000, val_loss: 0.30665347
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 325.99805570, sen-loss: 32.11540949, dom-loss: 76.37289906, src-aux-loss: 114.73267263, tar-aux-loss: 102.77707452
Epoch: [8  ] train-acc: 0.89660714, dom-acc: 0.49464286, val-acc: 0.90000000, val_loss: 0.30020839
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 325.02356696, sen-loss: 30.45836864, dom-loss: 77.69941705, src-aux-loss: 112.81475729, tar-aux-loss: 104.05102330
Epoch: [9  ] train-acc: 0.90053571, dom-acc: 0.45758929, val-acc: 0.89250000, val_loss: 0.30015972
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 320.53003740, sen-loss: 29.53947051, dom-loss: 77.70180035, src-aux-loss: 111.96613503, tar-aux-loss: 101.32263148
Epoch: [10 ] train-acc: 0.90535714, dom-acc: 0.45241071, val-acc: 0.90250000, val_loss: 0.29294398
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 319.02863455, sen-loss: 28.59632264, dom-loss: 78.29755038, src-aux-loss: 111.10663456, tar-aux-loss: 101.02812636
Epoch: [11 ] train-acc: 0.90517857, dom-acc: 0.43142857, val-acc: 0.90500000, val_loss: 0.28804407
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 317.27131343, sen-loss: 27.84041658, dom-loss: 78.46557474, src-aux-loss: 109.87199301, tar-aux-loss: 101.09332985
Epoch: [12 ] train-acc: 0.90678571, dom-acc: 0.43616071, val-acc: 0.90250000, val_loss: 0.28615466
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.99754882, sen-loss: 27.40939507, dom-loss: 78.42612433, src-aux-loss: 108.98132968, tar-aux-loss: 100.18069923
Epoch: [13 ] train-acc: 0.90910714, dom-acc: 0.42866071, val-acc: 0.89500000, val_loss: 0.30487537
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 313.38255095, sen-loss: 26.58561804, dom-loss: 78.39286810, src-aux-loss: 108.14926344, tar-aux-loss: 100.25480145
Epoch: [14 ] train-acc: 0.91107143, dom-acc: 0.42383929, val-acc: 0.90250000, val_loss: 0.29630765
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 312.04103637, sen-loss: 26.44434264, dom-loss: 78.30118060, src-aux-loss: 107.59149355, tar-aux-loss: 99.70401669
Epoch: [15 ] train-acc: 0.91357143, dom-acc: 0.44330357, val-acc: 0.91750000, val_loss: 0.28147066
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 310.92440915, sen-loss: 25.62318054, dom-loss: 78.31825900, src-aux-loss: 106.70358968, tar-aux-loss: 100.27937865
Epoch: [16 ] train-acc: 0.91428571, dom-acc: 0.45125000, val-acc: 0.91000000, val_loss: 0.28229558
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 308.02151752, sen-loss: 25.22578991, dom-loss: 77.95004946, src-aux-loss: 106.16676247, tar-aux-loss: 98.67891556
Epoch: [17 ] train-acc: 0.91589286, dom-acc: 0.45196429, val-acc: 0.90250000, val_loss: 0.28462836
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 308.43591189, sen-loss: 24.78225386, dom-loss: 77.89509302, src-aux-loss: 105.75166219, tar-aux-loss: 100.00690275
Epoch: [18 ] train-acc: 0.91821429, dom-acc: 0.47267857, val-acc: 0.90750000, val_loss: 0.28488991
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 304.98673654, sen-loss: 24.60878575, dom-loss: 77.46947855, src-aux-loss: 104.94178832, tar-aux-loss: 97.96668553
Epoch: [19 ] train-acc: 0.91803571, dom-acc: 0.46714286, val-acc: 0.91250000, val_loss: 0.28142595
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 304.17672467, sen-loss: 24.16800839, dom-loss: 77.39268637, src-aux-loss: 104.39211267, tar-aux-loss: 98.22391778
Epoch: [20 ] train-acc: 0.92053571, dom-acc: 0.48508929, val-acc: 0.92000000, val_loss: 0.27660176
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 304.34731340, sen-loss: 23.98269422, dom-loss: 77.48171151, src-aux-loss: 103.84659576, tar-aux-loss: 99.03631192
Epoch: [21 ] train-acc: 0.92285714, dom-acc: 0.49776786, val-acc: 0.91750000, val_loss: 0.27452117
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 302.27361226, sen-loss: 23.71021958, dom-loss: 77.17495227, src-aux-loss: 103.14313942, tar-aux-loss: 98.24530113
Epoch: [22 ] train-acc: 0.92339286, dom-acc: 0.49107143, val-acc: 0.92000000, val_loss: 0.27548867
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 301.28215265, sen-loss: 23.19145539, dom-loss: 77.11703157, src-aux-loss: 102.60298878, tar-aux-loss: 98.37067693
Epoch: [23 ] train-acc: 0.92464286, dom-acc: 0.50383929, val-acc: 0.92000000, val_loss: 0.27492121
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 300.35244393, sen-loss: 23.10471640, dom-loss: 77.00827771, src-aux-loss: 102.42162299, tar-aux-loss: 97.81782824
Epoch: [24 ] train-acc: 0.92589286, dom-acc: 0.50196429, val-acc: 0.91500000, val_loss: 0.27815750
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 299.32669520, sen-loss: 22.64375163, dom-loss: 77.12841654, src-aux-loss: 101.50344580, tar-aux-loss: 98.05108106
Epoch: [25 ] train-acc: 0.92571429, dom-acc: 0.50741071, val-acc: 0.92250000, val_loss: 0.27502006
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 298.06605053, sen-loss: 22.46112262, dom-loss: 76.84122729, src-aux-loss: 101.12912691, tar-aux-loss: 97.63457447
Epoch: [26 ] train-acc: 0.92803571, dom-acc: 0.49785714, val-acc: 0.92250000, val_loss: 0.27640623
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_dvd_HATN.ckpt
Best Epoch: [ 21] best val accuracy: 0.00000000 best val loss: 0.27452117
Testing accuracy: 0.84666667
./work/attentions/kitchen_dvd_train_HATN.txt
./work/attentions/kitchen_dvd_test_HATN.txt
loading data...
source domain:  kitchen target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 17009
vocab-size:  49470
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'wonderful', 'pleased', 'loves', 'fantastic', 'amazing', 'highly', 'awesome', 'satisfied', 'attractive', 'fabulous', 'nice', 'well', 'terrific', 'loved', 'favorite', 'incredible', 'fast', 'quick', 'simple', 'outstanding', 'useful', 'solid', 'elegant', 'tough', 'perfectly', 'decent', 'surprised', 'pretty']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'broke', 'useless', 'horrible', 'poorly', 'returned', 'bad', 'broken', 'flimsy', 'defective', 'save', 'impossible', 'awful', 'wrong', 'expensive', 'dissapointed', 'dangerous', 'stopped', 'failed', 'ruined', 'overpriced', 'difficult', 'worthless', 'hard', 'quit', 'frustrating', 'misleading', 'unhappy', 'flawed', 'sad', 'disapointed', 'worse', 'worked', 'better', 'messy', 'return', 'ugly', 'unacceptable', 'weak', 'lousy', 'ridiculous', 'inconsistent', 'shattered', 'hated', 'frustrated', 'overrated', 'shocked', 'waiting', 'stuck', 'sadly', 'leaky']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
49470
5600 400 6000 19856 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 421.84886813, sen-loss: 76.83221215, dom-loss: 78.85544682, src-aux-loss: 142.71307051, tar-aux-loss: 123.44814014
Epoch: [1  ] train-acc: 0.76446429, dom-acc: 0.64544643, val-acc: 0.76500000, val_loss: 0.63783073
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 391.61762643, sen-loss: 66.52682394, dom-loss: 76.28158826, src-aux-loss: 133.83025813, tar-aux-loss: 114.97895485
Epoch: [2  ] train-acc: 0.78303571, dom-acc: 0.66767857, val-acc: 0.80500000, val_loss: 0.53454810
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 367.86897159, sen-loss: 54.61691469, dom-loss: 75.04459691, src-aux-loss: 127.38314670, tar-aux-loss: 110.82431436
Epoch: [3  ] train-acc: 0.84339286, dom-acc: 0.61008929, val-acc: 0.85250000, val_loss: 0.42018652
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 348.66887236, sen-loss: 43.61754115, dom-loss: 74.95475620, src-aux-loss: 123.23392147, tar-aux-loss: 106.86265367
Epoch: [4  ] train-acc: 0.85750000, dom-acc: 0.62848214, val-acc: 0.86250000, val_loss: 0.35055682
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 339.24629402, sen-loss: 38.66384792, dom-loss: 74.69934595, src-aux-loss: 120.05971658, tar-aux-loss: 105.82338095
Epoch: [5  ] train-acc: 0.87928571, dom-acc: 0.71830357, val-acc: 0.88000000, val_loss: 0.32923856
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 332.46794510, sen-loss: 35.67994452, dom-loss: 74.67380440, src-aux-loss: 117.74212843, tar-aux-loss: 104.37206924
Epoch: [6  ] train-acc: 0.88517857, dom-acc: 0.66598214, val-acc: 0.89250000, val_loss: 0.31283402
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 329.26635790, sen-loss: 33.71228713, dom-loss: 74.85446525, src-aux-loss: 116.34058094, tar-aux-loss: 104.35902363
Epoch: [7  ] train-acc: 0.89125000, dom-acc: 0.64098214, val-acc: 0.89750000, val_loss: 0.30487669
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 323.64434886, sen-loss: 32.05070777, dom-loss: 75.03456312, src-aux-loss: 114.96456230, tar-aux-loss: 101.59451610
Epoch: [8  ] train-acc: 0.89678571, dom-acc: 0.66375000, val-acc: 0.90250000, val_loss: 0.29904094
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 321.40752387, sen-loss: 30.41868855, dom-loss: 75.37792265, src-aux-loss: 113.34215206, tar-aux-loss: 102.26875931
Epoch: [9  ] train-acc: 0.90142857, dom-acc: 0.66732143, val-acc: 0.89500000, val_loss: 0.30136660
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 319.14128780, sen-loss: 29.42878355, dom-loss: 75.60909021, src-aux-loss: 112.18085098, tar-aux-loss: 101.92256349
Epoch: [10 ] train-acc: 0.90375000, dom-acc: 0.64017857, val-acc: 0.90750000, val_loss: 0.29247555
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 315.52169275, sen-loss: 28.61175836, dom-loss: 75.87838417, src-aux-loss: 111.65008873, tar-aux-loss: 99.38145965
Epoch: [11 ] train-acc: 0.90696429, dom-acc: 0.66553571, val-acc: 0.90750000, val_loss: 0.28697664
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 313.96657467, sen-loss: 27.76329269, dom-loss: 76.06292206, src-aux-loss: 110.36018753, tar-aux-loss: 99.78017133
Epoch: [12 ] train-acc: 0.90910714, dom-acc: 0.65250000, val-acc: 0.91000000, val_loss: 0.28480071
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 311.63010120, sen-loss: 27.27816597, dom-loss: 76.42783767, src-aux-loss: 109.45039427, tar-aux-loss: 98.47370261
Epoch: [13 ] train-acc: 0.90625000, dom-acc: 0.65848214, val-acc: 0.89750000, val_loss: 0.30787665
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 311.26902008, sen-loss: 26.56915569, dom-loss: 76.54214996, src-aux-loss: 108.48780179, tar-aux-loss: 99.66991365
Epoch: [14 ] train-acc: 0.91178571, dom-acc: 0.64875000, val-acc: 0.90250000, val_loss: 0.29158565
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 309.56728888, sen-loss: 26.34540760, dom-loss: 76.81501824, src-aux-loss: 107.88839304, tar-aux-loss: 98.51847196
Epoch: [15 ] train-acc: 0.91625000, dom-acc: 0.63526786, val-acc: 0.91250000, val_loss: 0.28133675
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 306.90102625, sen-loss: 25.54548217, dom-loss: 76.88979000, src-aux-loss: 106.84262419, tar-aux-loss: 97.62313020
Epoch: [16 ] train-acc: 0.91482143, dom-acc: 0.64571429, val-acc: 0.91250000, val_loss: 0.28170305
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 306.32560349, sen-loss: 25.12959248, dom-loss: 77.11906779, src-aux-loss: 106.29002684, tar-aux-loss: 97.78691518
Epoch: [17 ] train-acc: 0.91589286, dom-acc: 0.63366071, val-acc: 0.91000000, val_loss: 0.29072705
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 306.76987410, sen-loss: 24.67638747, dom-loss: 77.30349529, src-aux-loss: 105.82450497, tar-aux-loss: 98.96548575
Epoch: [18 ] train-acc: 0.91892857, dom-acc: 0.65714286, val-acc: 0.91500000, val_loss: 0.28179437
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 303.61328602, sen-loss: 24.58680084, dom-loss: 77.46884161, src-aux-loss: 105.05623251, tar-aux-loss: 96.50140947
Epoch: [19 ] train-acc: 0.91910714, dom-acc: 0.62991071, val-acc: 0.92000000, val_loss: 0.27972111
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 303.62983894, sen-loss: 24.10512818, dom-loss: 77.52747935, src-aux-loss: 104.53156734, tar-aux-loss: 97.46566337
Epoch: [20 ] train-acc: 0.92232143, dom-acc: 0.65446429, val-acc: 0.92000000, val_loss: 0.27608380
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 302.70196033, sen-loss: 23.84032794, dom-loss: 77.69092715, src-aux-loss: 104.00250876, tar-aux-loss: 97.16819751
Epoch: [21 ] train-acc: 0.92125000, dom-acc: 0.62562500, val-acc: 0.91500000, val_loss: 0.27282304
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 301.92128086, sen-loss: 23.64233633, dom-loss: 77.86575496, src-aux-loss: 103.24676806, tar-aux-loss: 97.16642231
Epoch: [22 ] train-acc: 0.92339286, dom-acc: 0.60151786, val-acc: 0.92000000, val_loss: 0.27893475
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 300.27744603, sen-loss: 23.18312149, dom-loss: 77.86116803, src-aux-loss: 102.87338442, tar-aux-loss: 96.35977250
Epoch: [23 ] train-acc: 0.92571429, dom-acc: 0.63383929, val-acc: 0.92250000, val_loss: 0.27451763
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 299.54890871, sen-loss: 22.99032806, dom-loss: 78.02686226, src-aux-loss: 102.29169941, tar-aux-loss: 96.24001920
Epoch: [24 ] train-acc: 0.92589286, dom-acc: 0.62142857, val-acc: 0.92000000, val_loss: 0.28184330
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 299.05225468, sen-loss: 22.62958740, dom-loss: 78.08367175, src-aux-loss: 101.65843594, tar-aux-loss: 96.68055987
Epoch: [25 ] train-acc: 0.92750000, dom-acc: 0.64035714, val-acc: 0.92000000, val_loss: 0.27244860
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 299.01869941, sen-loss: 22.45550966, dom-loss: 78.25572026, src-aux-loss: 101.46335655, tar-aux-loss: 96.84411418
Epoch: [26 ] train-acc: 0.92767857, dom-acc: 0.58392857, val-acc: 0.92500000, val_loss: 0.27726570
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 296.40495038, sen-loss: 22.34125089, dom-loss: 78.18500185, src-aux-loss: 100.85823345, tar-aux-loss: 95.02046514
Epoch: [27 ] train-acc: 0.92732143, dom-acc: 0.59312500, val-acc: 0.92000000, val_loss: 0.27394542
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 297.88299632, sen-loss: 22.24634687, dom-loss: 78.39341760, src-aux-loss: 100.45089096, tar-aux-loss: 96.79234070
Epoch: [28 ] train-acc: 0.92875000, dom-acc: 0.57785714, val-acc: 0.92250000, val_loss: 0.27073407
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 296.06148243, sen-loss: 21.91955815, dom-loss: 78.31658298, src-aux-loss: 99.87803864, tar-aux-loss: 95.94730091
Epoch: [29 ] train-acc: 0.93017857, dom-acc: 0.59982143, val-acc: 0.92250000, val_loss: 0.27029318
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 294.92126107, sen-loss: 21.60104825, dom-loss: 78.45618790, src-aux-loss: 99.49583054, tar-aux-loss: 95.36819530
Epoch: [30 ] train-acc: 0.93125000, dom-acc: 0.59732143, val-acc: 0.92750000, val_loss: 0.27099735
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 293.58583951, sen-loss: 21.48182971, dom-loss: 78.39488536, src-aux-loss: 98.92441916, tar-aux-loss: 94.78470421
Epoch: [31 ] train-acc: 0.93142857, dom-acc: 0.60017857, val-acc: 0.92750000, val_loss: 0.27177191
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 293.96083689, sen-loss: 21.19774275, dom-loss: 78.48610824, src-aux-loss: 98.34750146, tar-aux-loss: 95.92948478
Epoch: [32 ] train-acc: 0.93196429, dom-acc: 0.60276786, val-acc: 0.93000000, val_loss: 0.26941997
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 291.72994328, sen-loss: 20.95397514, dom-loss: 78.51088852, src-aux-loss: 98.03101432, tar-aux-loss: 94.23406613
Epoch: [33 ] train-acc: 0.93517857, dom-acc: 0.57053571, val-acc: 0.92500000, val_loss: 0.27341422
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 291.35239363, sen-loss: 20.82208738, dom-loss: 78.53360337, src-aux-loss: 97.63857555, tar-aux-loss: 94.35812789
Epoch: [34 ] train-acc: 0.93571429, dom-acc: 0.58508929, val-acc: 0.92250000, val_loss: 0.27365410
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 291.63386226, sen-loss: 20.59845831, dom-loss: 78.53727937, src-aux-loss: 96.95400786, tar-aux-loss: 95.54411721
Epoch: [35 ] train-acc: 0.93589286, dom-acc: 0.61517857, val-acc: 0.92000000, val_loss: 0.27228606
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 291.18201709, sen-loss: 20.33276927, dom-loss: 78.36079931, src-aux-loss: 96.58620828, tar-aux-loss: 95.90224016
Epoch: [36 ] train-acc: 0.93589286, dom-acc: 0.60607143, val-acc: 0.92500000, val_loss: 0.26142240
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 289.19281363, sen-loss: 20.29083938, dom-loss: 78.50332510, src-aux-loss: 96.28012651, tar-aux-loss: 94.11852312
Epoch: [37 ] train-acc: 0.93714286, dom-acc: 0.57642857, val-acc: 0.92500000, val_loss: 0.27077192
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 289.61748958, sen-loss: 20.08610741, dom-loss: 78.48659837, src-aux-loss: 95.92768902, tar-aux-loss: 95.11709487
Epoch: [38 ] train-acc: 0.93714286, dom-acc: 0.61160714, val-acc: 0.92750000, val_loss: 0.26747400
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 288.46850348, sen-loss: 19.72292065, dom-loss: 78.39414608, src-aux-loss: 95.28420472, tar-aux-loss: 95.06723201
Epoch: [39 ] train-acc: 0.94017857, dom-acc: 0.60410714, val-acc: 0.92750000, val_loss: 0.26004708
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 287.29587150, sen-loss: 19.56934730, dom-loss: 78.36397266, src-aux-loss: 94.74660724, tar-aux-loss: 94.61594546
Epoch: [40 ] train-acc: 0.94089286, dom-acc: 0.60785714, val-acc: 0.92500000, val_loss: 0.26488897
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 286.02561307, sen-loss: 19.39515322, dom-loss: 78.36298370, src-aux-loss: 94.21501416, tar-aux-loss: 94.05246067
Epoch: [41 ] train-acc: 0.94214286, dom-acc: 0.62107143, val-acc: 0.92500000, val_loss: 0.26620767
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 286.31717348, sen-loss: 19.07893999, dom-loss: 78.33407962, src-aux-loss: 93.98381048, tar-aux-loss: 94.92034370
Epoch: [42 ] train-acc: 0.94053571, dom-acc: 0.61794643, val-acc: 0.92500000, val_loss: 0.25841987
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 283.70377803, sen-loss: 19.03925989, dom-loss: 78.18329263, src-aux-loss: 93.54049009, tar-aux-loss: 92.94073600
Epoch: [43 ] train-acc: 0.94214286, dom-acc: 0.58955357, val-acc: 0.92750000, val_loss: 0.25803655
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 283.37696242, sen-loss: 18.77545214, dom-loss: 78.19934583, src-aux-loss: 92.68414503, tar-aux-loss: 93.71801823
Epoch: [44 ] train-acc: 0.94357143, dom-acc: 0.61955357, val-acc: 0.92750000, val_loss: 0.26176602
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 284.40926862, sen-loss: 18.69313444, dom-loss: 78.12226439, src-aux-loss: 92.15574384, tar-aux-loss: 95.43812501
Epoch: [45 ] train-acc: 0.94446429, dom-acc: 0.62785714, val-acc: 0.91750000, val_loss: 0.27088514
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 282.70711279, sen-loss: 18.39514711, dom-loss: 78.05382663, src-aux-loss: 91.65695530, tar-aux-loss: 94.60118389
Epoch: [46 ] train-acc: 0.94464286, dom-acc: 0.61312500, val-acc: 0.91750000, val_loss: 0.27105081
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 280.31581044, sen-loss: 18.27379314, dom-loss: 77.98096496, src-aux-loss: 91.29656148, tar-aux-loss: 92.76449102
Epoch: [47 ] train-acc: 0.94571429, dom-acc: 0.61526786, val-acc: 0.92750000, val_loss: 0.26163867
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 281.77075696, sen-loss: 17.95780220, dom-loss: 77.97253150, src-aux-loss: 90.89550722, tar-aux-loss: 94.94491613
Epoch: [48 ] train-acc: 0.94500000, dom-acc: 0.63669643, val-acc: 0.93000000, val_loss: 0.25935158
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_electronics_HATN.ckpt
Best Epoch: [ 43] best val accuracy: 0.00000000 best val loss: 0.25803655
Testing accuracy: 0.89450000
./work/attentions/kitchen_electronics_train_HATN.txt
./work/attentions/kitchen_electronics_test_HATN.txt
loading data...
source domain:  kitchen target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 30180
vocab-size:  78115
['great', 'love', 'easy', 'good', 'best', 'excellent', 'perfect', 'happy', 'pleased', 'wonderful', 'loves', 'fantastic', 'highly', 'amazing', 'awesome', 'satisfied', 'fabulous', 'nice', 'attractive', 'well', 'terrific', 'favorite', 'incredible', 'fast', 'quick', 'solid', 'simple', 'outstanding', 'useful', 'elegant', 'tough', 'easier', 'perfectly', 'decent', 'surprised', 'durable', 'far', 'impressed']
['disappointed', 'poor', 'disappointing', 'cheap', 'terrible', 'worst', 'useless', 'broke', 'horrible', 'poorly', 'bad', 'save', 'broken', 'defective', 'returned', 'flimsy', 'expensive', 'awful', 'impossible', 'wrong', 'dissapointed', 'dangerous', 'failed', 'overpriced', 'ruined', 'stopped', 'difficult', 'worthless', 'hard', 'frustrating', 'misleading', 'unhappy', 'better', 'quit', 'disapointed', 'flawed', 'worse', 'sad', 'return', 'worked', 'messy', 'weak', 'lousy', 'stuck', 'shattered', 'embarrassed', 'fell', 'overrated', 'shocked', 'ridiculous', 'waiting', 'unacceptable', 'leaky']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
78115
5600 400 6000 19856 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 419.95726991, sen-loss: 76.82949811, dom-loss: 76.90171754, src-aux-loss: 142.26981461, tar-aux-loss: 123.95623940
Epoch: [1  ] train-acc: 0.76428571, dom-acc: 0.83633929, val-acc: 0.76500000, val_loss: 0.63840973
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 389.71054697, sen-loss: 66.76754397, dom-loss: 73.56797361, src-aux-loss: 133.21587157, tar-aux-loss: 116.15915966
Epoch: [2  ] train-acc: 0.76982143, dom-acc: 0.81821429, val-acc: 0.79000000, val_loss: 0.53957069
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 367.52579093, sen-loss: 54.99456128, dom-loss: 72.36491489, src-aux-loss: 127.05186784, tar-aux-loss: 113.11444771
Epoch: [3  ] train-acc: 0.83839286, dom-acc: 0.71258929, val-acc: 0.84000000, val_loss: 0.42343575
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 347.78188491, sen-loss: 43.90845433, dom-loss: 72.58846492, src-aux-loss: 122.87112695, tar-aux-loss: 108.41383880
Epoch: [4  ] train-acc: 0.85928571, dom-acc: 0.61107143, val-acc: 0.86500000, val_loss: 0.35609385
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 341.08629441, sen-loss: 38.97030522, dom-loss: 73.87983078, src-aux-loss: 119.79363829, tar-aux-loss: 108.44252050
Epoch: [5  ] train-acc: 0.87875000, dom-acc: 0.59383929, val-acc: 0.88250000, val_loss: 0.33185923
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 334.18532658, sen-loss: 35.94863755, dom-loss: 74.61494917, src-aux-loss: 117.46690017, tar-aux-loss: 106.15483856
Epoch: [6  ] train-acc: 0.88482143, dom-acc: 0.56357143, val-acc: 0.88750000, val_loss: 0.31489497
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 331.18345213, sen-loss: 33.80189353, dom-loss: 76.17885578, src-aux-loss: 115.93538076, tar-aux-loss: 105.26732171
Epoch: [7  ] train-acc: 0.89017857, dom-acc: 0.52142857, val-acc: 0.90500000, val_loss: 0.30631146
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 328.61458802, sen-loss: 32.18069431, dom-loss: 77.04499918, src-aux-loss: 114.77664363, tar-aux-loss: 104.61225128
Epoch: [8  ] train-acc: 0.89607143, dom-acc: 0.47401786, val-acc: 0.89750000, val_loss: 0.30169505
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 326.15523982, sen-loss: 30.41896030, dom-loss: 78.22410607, src-aux-loss: 112.72961426, tar-aux-loss: 104.78255999
Epoch: [9  ] train-acc: 0.90125000, dom-acc: 0.43875000, val-acc: 0.90000000, val_loss: 0.29859546
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 322.34467077, sen-loss: 29.53247467, dom-loss: 78.41373444, src-aux-loss: 111.90679663, tar-aux-loss: 102.49166226
Epoch: [10 ] train-acc: 0.90232143, dom-acc: 0.42812500, val-acc: 0.91000000, val_loss: 0.29245445
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 319.76281714, sen-loss: 28.51706931, dom-loss: 79.08762676, src-aux-loss: 111.01374191, tar-aux-loss: 101.14437926
Epoch: [11 ] train-acc: 0.90500000, dom-acc: 0.40133929, val-acc: 0.90750000, val_loss: 0.28847250
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 319.10323262, sen-loss: 27.84124137, dom-loss: 79.05671799, src-aux-loss: 109.75778204, tar-aux-loss: 102.44749314
Epoch: [12 ] train-acc: 0.90500000, dom-acc: 0.41732143, val-acc: 0.91250000, val_loss: 0.28578651
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 314.85170293, sen-loss: 27.40479682, dom-loss: 79.00867230, src-aux-loss: 108.97652799, tar-aux-loss: 99.46170539
Epoch: [13 ] train-acc: 0.91000000, dom-acc: 0.41017857, val-acc: 0.89750000, val_loss: 0.30593663
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 314.41401196, sen-loss: 26.62134091, dom-loss: 78.86642671, src-aux-loss: 108.01283854, tar-aux-loss: 100.91340530
Epoch: [14 ] train-acc: 0.91107143, dom-acc: 0.42151786, val-acc: 0.90250000, val_loss: 0.29625875
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 313.28291917, sen-loss: 26.40999081, dom-loss: 78.64379382, src-aux-loss: 107.43760234, tar-aux-loss: 100.79153085
Epoch: [15 ] train-acc: 0.91196429, dom-acc: 0.43669643, val-acc: 0.91500000, val_loss: 0.28444275
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 310.82748556, sen-loss: 25.67308632, dom-loss: 78.45314687, src-aux-loss: 106.47654969, tar-aux-loss: 100.22470206
Epoch: [16 ] train-acc: 0.91589286, dom-acc: 0.45580357, val-acc: 0.91750000, val_loss: 0.28014672
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 308.47825027, sen-loss: 25.18896037, dom-loss: 78.12120128, src-aux-loss: 105.90172660, tar-aux-loss: 99.26636297
Epoch: [17 ] train-acc: 0.91625000, dom-acc: 0.47178571, val-acc: 0.90500000, val_loss: 0.28457320
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 307.99457455, sen-loss: 24.82392428, dom-loss: 77.75947016, src-aux-loss: 105.59303975, tar-aux-loss: 99.81814045
Epoch: [18 ] train-acc: 0.91714286, dom-acc: 0.49687500, val-acc: 0.91000000, val_loss: 0.28072593
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 304.98475599, sen-loss: 24.66039690, dom-loss: 77.62940818, src-aux-loss: 104.74163949, tar-aux-loss: 97.95331013
Epoch: [19 ] train-acc: 0.91892857, dom-acc: 0.49151786, val-acc: 0.91000000, val_loss: 0.27922171
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 304.65150881, sen-loss: 24.18506150, dom-loss: 77.35775745, src-aux-loss: 104.15219462, tar-aux-loss: 98.95649421
Epoch: [20 ] train-acc: 0.92107143, dom-acc: 0.50946429, val-acc: 0.91250000, val_loss: 0.27595401
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 303.33766270, sen-loss: 23.96984440, dom-loss: 77.18142939, src-aux-loss: 103.55474257, tar-aux-loss: 98.63164520
Epoch: [21 ] train-acc: 0.92250000, dom-acc: 0.50821429, val-acc: 0.91750000, val_loss: 0.27300319
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 301.94011998, sen-loss: 23.73956770, dom-loss: 77.01748997, src-aux-loss: 102.71631163, tar-aux-loss: 98.46674955
Epoch: [22 ] train-acc: 0.92357143, dom-acc: 0.50589286, val-acc: 0.91750000, val_loss: 0.27497876
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 299.53649902, sen-loss: 23.20958787, dom-loss: 76.90462273, src-aux-loss: 102.36869889, tar-aux-loss: 97.05359089
Epoch: [23 ] train-acc: 0.92535714, dom-acc: 0.51392857, val-acc: 0.91500000, val_loss: 0.27276945
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 301.96472430, sen-loss: 23.13275900, dom-loss: 77.03529590, src-aux-loss: 102.05305123, tar-aux-loss: 99.74361825
Epoch: [24 ] train-acc: 0.92589286, dom-acc: 0.51535714, val-acc: 0.92000000, val_loss: 0.27525806
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 298.64830923, sen-loss: 22.74686000, dom-loss: 77.11579823, src-aux-loss: 101.09059912, tar-aux-loss: 97.69505000
Epoch: [25 ] train-acc: 0.92535714, dom-acc: 0.52080357, val-acc: 0.92000000, val_loss: 0.27289596
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 297.73760939, sen-loss: 22.51623897, dom-loss: 76.93049657, src-aux-loss: 100.64589280, tar-aux-loss: 97.64498192
Epoch: [26 ] train-acc: 0.92821429, dom-acc: 0.49500000, val-acc: 0.92250000, val_loss: 0.27679300
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 295.21309662, sen-loss: 22.36895083, dom-loss: 77.21161395, src-aux-loss: 100.24424505, tar-aux-loss: 95.38828653
Epoch: [27 ] train-acc: 0.92839286, dom-acc: 0.50642857, val-acc: 0.91500000, val_loss: 0.27081564
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 296.26458740, sen-loss: 22.28648867, dom-loss: 76.93645275, src-aux-loss: 99.77686948, tar-aux-loss: 97.26477635
Epoch: [28 ] train-acc: 0.92946429, dom-acc: 0.49008929, val-acc: 0.92500000, val_loss: 0.27304950
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 294.57698226, sen-loss: 21.89892490, dom-loss: 77.38764852, src-aux-loss: 99.20427036, tar-aux-loss: 96.08613926
Epoch: [29 ] train-acc: 0.92964286, dom-acc: 0.48285714, val-acc: 0.92250000, val_loss: 0.26928961
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 293.54693413, sen-loss: 21.64370321, dom-loss: 77.58614582, src-aux-loss: 98.58456272, tar-aux-loss: 95.73252326
Epoch: [30 ] train-acc: 0.93125000, dom-acc: 0.48437500, val-acc: 0.92250000, val_loss: 0.27371734
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 293.23836970, sen-loss: 21.49162940, dom-loss: 77.56957000, src-aux-loss: 98.05381900, tar-aux-loss: 96.12335050
Epoch: [31 ] train-acc: 0.93196429, dom-acc: 0.47973214, val-acc: 0.92000000, val_loss: 0.27353355
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 293.88415241, sen-loss: 21.21517491, dom-loss: 77.71201545, src-aux-loss: 97.37495959, tar-aux-loss: 97.58200210
Epoch: [32 ] train-acc: 0.93339286, dom-acc: 0.47357143, val-acc: 0.92250000, val_loss: 0.27536508
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 291.33293319, sen-loss: 20.98004232, dom-loss: 77.66895157, src-aux-loss: 97.00091726, tar-aux-loss: 95.68302232
Epoch: [33 ] train-acc: 0.93285714, dom-acc: 0.45375000, val-acc: 0.92250000, val_loss: 0.27567527
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 290.69149518, sen-loss: 20.93150947, dom-loss: 77.87431008, src-aux-loss: 96.77490371, tar-aux-loss: 95.11077279
Epoch: [34 ] train-acc: 0.93357143, dom-acc: 0.47062500, val-acc: 0.92250000, val_loss: 0.27361903
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_video_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26928961
Testing accuracy: 0.84700000
./work/attentions/kitchen_video_train_HATN.txt
./work/attentions/kitchen_video_test_HATN.txt
loading data...
source domain:  video target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 9750
vocab-size:  98084
['best', 'great', 'funny', 'good', 'enjoyable', 'excellent', 'classic', 'amazing', 'better', 'favorite', 'love', 'greatest', 'brilliant', 'superb', 'perfect', 'hard', 'awesome', 'fantastic', 'beautifully', 'underrated', 'informative', 'loved', 'nice', 'hilarious', 'easy', 'poignant', 'recommended', 'funniest', 'solid', 'clever', 'terrific', 'rather', 'epic', 'immortal', 'beautiful', 'recommend', 'slow', 'believable', 'revisited', 'entertaining', 'sad', 'brilliantly', 'wonderful', 'incredible', 'simple', 'jokes', 'liked', 'riveting', 'fabulous', 'old', 'leavens', 'watchable', 'masterful', 'fine', 'heartbreaking', 'realistic', 'pleasant', 'scary', 'emotional', 'sweet', 'romantic', 'uplifting', 'funnier']
['worst', 'horrible', 'terrible', 'bad', 'poor', 'boring', 'awful', 'worse', 'disappointing', 'garbled', 'dissapointing', 'bother', 'harmful', 'wrong', 'wasted', 'incomplete', 'frustrating', 'dull', 'disappointed', 'ruined', 'unfunny', 'cuts', 'annoying', 'forgettable', 'unconvincing', 'low', 'dreadful', 'poorly', 'hated', 'uninspired', 'weak', 'unwatchable', 'needless', 'atrocious', 'ok', 'save', 'skip', 'contrived', 'uncooked', 'lousy', 'predictable', 'cheesy', 'pointless', 'mediocre', 'lackluster', 'waste', 'laughable', 'amusing', 'cut', 'sappy', 'unmemorable', 'pretentious', 'disgusting', 'vapid', 'befuddled', 'unbearable']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
98084
5600 400 6000 36180 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.70426679, sen-loss: 77.48764104, dom-loss: 79.45749104, src-aux-loss: 135.67110205, tar-aux-loss: 112.08803022
Epoch: [1  ] train-acc: 0.67982143, dom-acc: 0.59857143, val-acc: 0.69000000, val_loss: 0.65502280
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 377.60130715, sen-loss: 70.23136312, dom-loss: 77.15655577, src-aux-loss: 125.32553542, tar-aux-loss: 104.88785130
Epoch: [2  ] train-acc: 0.73321429, dom-acc: 0.75178571, val-acc: 0.70000000, val_loss: 0.59214520
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 351.72306228, sen-loss: 62.01503438, dom-loss: 76.06328017, src-aux-loss: 115.73521435, tar-aux-loss: 97.90953487
Epoch: [3  ] train-acc: 0.78571429, dom-acc: 0.78473214, val-acc: 0.80500000, val_loss: 0.51516086
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 330.73317647, sen-loss: 53.46102846, dom-loss: 75.50830668, src-aux-loss: 107.33865577, tar-aux-loss: 94.42518491
Epoch: [4  ] train-acc: 0.81589286, dom-acc: 0.79964286, val-acc: 0.80000000, val_loss: 0.44009715
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 316.60326004, sen-loss: 47.10938236, dom-loss: 75.17518657, src-aux-loss: 100.90394223, tar-aux-loss: 93.41475075
Epoch: [5  ] train-acc: 0.84142857, dom-acc: 0.79642857, val-acc: 0.83000000, val_loss: 0.40025011
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 306.76332712, sen-loss: 42.49477975, dom-loss: 74.92921829, src-aux-loss: 96.56629735, tar-aux-loss: 92.77303201
Epoch: [6  ] train-acc: 0.86196429, dom-acc: 0.78973214, val-acc: 0.85750000, val_loss: 0.36464855
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 295.97671509, sen-loss: 39.16337089, dom-loss: 74.94657880, src-aux-loss: 93.06991011, tar-aux-loss: 88.79685485
Epoch: [7  ] train-acc: 0.86803571, dom-acc: 0.78455357, val-acc: 0.86750000, val_loss: 0.35461369
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 292.56347251, sen-loss: 37.08402880, dom-loss: 74.87345862, src-aux-loss: 90.62557399, tar-aux-loss: 89.98041046
Epoch: [8  ] train-acc: 0.88446429, dom-acc: 0.77892857, val-acc: 0.87750000, val_loss: 0.32416764
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 285.97936487, sen-loss: 34.68682969, dom-loss: 75.18435234, src-aux-loss: 87.65592307, tar-aux-loss: 88.45225906
Epoch: [9  ] train-acc: 0.88803571, dom-acc: 0.77535714, val-acc: 0.88000000, val_loss: 0.31314504
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 281.04705048, sen-loss: 33.07509249, dom-loss: 75.23289132, src-aux-loss: 85.77368850, tar-aux-loss: 86.96537924
Epoch: [10 ] train-acc: 0.89321429, dom-acc: 0.76357143, val-acc: 0.89250000, val_loss: 0.29488361
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 279.21870494, sen-loss: 31.72454864, dom-loss: 75.39556026, src-aux-loss: 84.05615610, tar-aux-loss: 88.04243851
Epoch: [11 ] train-acc: 0.89982143, dom-acc: 0.74910714, val-acc: 0.90750000, val_loss: 0.28385597
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 276.73891616, sen-loss: 30.58206955, dom-loss: 75.63556999, src-aux-loss: 82.51495793, tar-aux-loss: 88.00631917
Epoch: [12 ] train-acc: 0.89464286, dom-acc: 0.72901786, val-acc: 0.87250000, val_loss: 0.30764782
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 275.02093387, sen-loss: 29.76743677, dom-loss: 75.81393194, src-aux-loss: 81.38876849, tar-aux-loss: 88.05079752
Epoch: [13 ] train-acc: 0.90482143, dom-acc: 0.71732143, val-acc: 0.90000000, val_loss: 0.29247758
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 273.31717348, sen-loss: 29.09880608, dom-loss: 76.17230320, src-aux-loss: 79.88847739, tar-aux-loss: 88.15758681
Epoch: [14 ] train-acc: 0.90732143, dom-acc: 0.69160714, val-acc: 0.91000000, val_loss: 0.27779028
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 269.31366158, sen-loss: 28.39294943, dom-loss: 76.15641409, src-aux-loss: 78.87948552, tar-aux-loss: 85.88481283
Epoch: [15 ] train-acc: 0.90750000, dom-acc: 0.69178571, val-acc: 0.89750000, val_loss: 0.29158479
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 268.09713697, sen-loss: 27.91052919, dom-loss: 76.34474069, src-aux-loss: 77.65191188, tar-aux-loss: 86.18995476
Epoch: [16 ] train-acc: 0.91285714, dom-acc: 0.68071429, val-acc: 0.90500000, val_loss: 0.27644971
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 264.64282453, sen-loss: 27.27021582, dom-loss: 76.57291746, src-aux-loss: 76.39483249, tar-aux-loss: 84.40485907
Epoch: [17 ] train-acc: 0.91500000, dom-acc: 0.67562500, val-acc: 0.91750000, val_loss: 0.27294999
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 264.09415078, sen-loss: 26.75942589, dom-loss: 76.86332542, src-aux-loss: 75.35938561, tar-aux-loss: 85.11201280
Epoch: [18 ] train-acc: 0.91946429, dom-acc: 0.66464286, val-acc: 0.91750000, val_loss: 0.27277291
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 263.15803838, sen-loss: 26.37947728, dom-loss: 77.00925988, src-aux-loss: 74.33284947, tar-aux-loss: 85.43645108
Epoch: [19 ] train-acc: 0.92178571, dom-acc: 0.65973214, val-acc: 0.91750000, val_loss: 0.27164438
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 260.47718453, sen-loss: 25.99171730, dom-loss: 77.17925125, src-aux-loss: 73.64368114, tar-aux-loss: 83.66253561
Epoch: [20 ] train-acc: 0.91625000, dom-acc: 0.65580357, val-acc: 0.90000000, val_loss: 0.27260351
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 260.72095811, sen-loss: 25.55998240, dom-loss: 77.13389993, src-aux-loss: 72.82093370, tar-aux-loss: 85.20614243
Epoch: [21 ] train-acc: 0.92392857, dom-acc: 0.64178571, val-acc: 0.91500000, val_loss: 0.27072141
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 260.13574898, sen-loss: 24.99752152, dom-loss: 77.38827217, src-aux-loss: 71.96064931, tar-aux-loss: 85.78930640
Epoch: [22 ] train-acc: 0.92642857, dom-acc: 0.62910714, val-acc: 0.90250000, val_loss: 0.27065071
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 256.61404252, sen-loss: 24.73035994, dom-loss: 77.64775127, src-aux-loss: 70.75654832, tar-aux-loss: 83.47938305
Epoch: [23 ] train-acc: 0.92607143, dom-acc: 0.62803571, val-acc: 0.91250000, val_loss: 0.27195215
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 254.80111647, sen-loss: 24.27822208, dom-loss: 77.72988975, src-aux-loss: 69.85718364, tar-aux-loss: 82.93582079
Epoch: [24 ] train-acc: 0.92678571, dom-acc: 0.61991071, val-acc: 0.91250000, val_loss: 0.27441487
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 255.25901365, sen-loss: 23.89920102, dom-loss: 77.66503930, src-aux-loss: 69.32538974, tar-aux-loss: 84.36938381
Epoch: [25 ] train-acc: 0.92910714, dom-acc: 0.61732143, val-acc: 0.91250000, val_loss: 0.26934499
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 255.02624965, sen-loss: 23.70750420, dom-loss: 77.95134151, src-aux-loss: 68.44634789, tar-aux-loss: 84.92105609
Epoch: [26 ] train-acc: 0.93000000, dom-acc: 0.61803571, val-acc: 0.91000000, val_loss: 0.26865262
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 252.13165021, sen-loss: 23.29293813, dom-loss: 77.92545789, src-aux-loss: 67.61220935, tar-aux-loss: 83.30104482
Epoch: [27 ] train-acc: 0.93089286, dom-acc: 0.60892857, val-acc: 0.90750000, val_loss: 0.26974776
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 252.70787275, sen-loss: 23.06315035, dom-loss: 77.95251167, src-aux-loss: 67.05414844, tar-aux-loss: 84.63806188
Epoch: [28 ] train-acc: 0.93285714, dom-acc: 0.61401786, val-acc: 0.90750000, val_loss: 0.27199373
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 249.74809396, sen-loss: 22.70635922, dom-loss: 77.98718196, src-aux-loss: 66.35718232, tar-aux-loss: 82.69737089
Epoch: [29 ] train-acc: 0.93446429, dom-acc: 0.60517857, val-acc: 0.90500000, val_loss: 0.26950788
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 247.68825650, sen-loss: 22.42441579, dom-loss: 77.81792676, src-aux-loss: 65.60418281, tar-aux-loss: 81.84173042
Epoch: [30 ] train-acc: 0.93696429, dom-acc: 0.58116071, val-acc: 0.90750000, val_loss: 0.27071869
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 248.04041338, sen-loss: 22.07501661, dom-loss: 78.12608320, src-aux-loss: 65.00661907, tar-aux-loss: 82.83269560
Epoch: [31 ] train-acc: 0.93696429, dom-acc: 0.60151786, val-acc: 0.90750000, val_loss: 0.26987150
---------------------------------------------------

Successfully load model from save path: ./work/models/video_books_HATN.ckpt
Best Epoch: [ 26] best val accuracy: 0.00000000 best val loss: 0.26865262
Testing accuracy: 0.86883333
./work/attentions/video_books_train_HATN.txt
./work/attentions/video_books_test_HATN.txt
loading data...
source domain:  video target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 11843
vocab-size:  91852
['best', 'funny', 'great', 'good', 'classic', 'enjoyable', 'love', 'amazing', 'favorite', 'excellent', 'awesome', 'better', 'loved', 'underrated', 'superb', 'brilliant', 'greatest', 'hard', 'perfect', 'beautifully', 'funniest', 'fantastic', 'hilarious', 'poignant', 'entertaining', 'easy', 'nice', 'informative', 'watching', 'terrific', 'solid', 'epic', 'clever', 'believable', 'brilliantly', 'beautiful', 'performed', 'riveting', 'wonderful', 'jokes', 'watchable', 'romantic', 'incredible', 'scary', 'fabulous', 'liked', 'fine', 'masterful', 'emotional', 'magnificent', 'uplifting', 'realistic', 'funnier', 'old']
['worst', 'horrible', 'terrible', 'bad', 'poor', 'boring', 'disappointing', 'worse', 'awful', 'garbled', 'bother', 'dissapointing', 'harmful', 'average', 'wrong', 'wasted', 'dull', 'ruined', 'unfunny', 'hated', 'cuts', 'annoying', 'disappointed', 'forgettable', 'unwatchable', 'low', 'poorly', 'laughable', 'dreadful', 'needless', 'skip', 'uninspired', 'unconvincing', 'atrocious', 'ok', 'contrived', 'uncooked', 'lousy', 'dissapointed', 'pointless', 'predictable', 'weak', 'cheesy', 'mediocre', 'lackluster', 'amusing', 'cut', 'sappy', 'unmemorable', 'pretentious', 'disgusting', 'vapid', 'lame', 'unbearable', 'save']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
91852
5600 400 6000 36180 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 405.20481348, sen-loss: 77.52114165, dom-loss: 79.33434397, src-aux-loss: 135.73999673, tar-aux-loss: 112.60932994
Epoch: [1  ] train-acc: 0.67303571, dom-acc: 0.44303571, val-acc: 0.68750000, val_loss: 0.65548235
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 378.50778580, sen-loss: 70.30499387, dom-loss: 78.76440752, src-aux-loss: 124.46401972, tar-aux-loss: 104.97436535
Epoch: [2  ] train-acc: 0.74482143, dom-acc: 0.42714286, val-acc: 0.71750000, val_loss: 0.59105211
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 354.94822049, sen-loss: 62.11670035, dom-loss: 78.62171823, src-aux-loss: 115.57660079, tar-aux-loss: 98.63320130
Epoch: [3  ] train-acc: 0.77732143, dom-acc: 0.41598214, val-acc: 0.80000000, val_loss: 0.51685452
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 335.38626719, sen-loss: 53.29200274, dom-loss: 78.63474751, src-aux-loss: 107.15301645, tar-aux-loss: 96.30650127
Epoch: [4  ] train-acc: 0.82089286, dom-acc: 0.40982143, val-acc: 0.79250000, val_loss: 0.43515942
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 319.42114401, sen-loss: 46.72166336, dom-loss: 78.59730005, src-aux-loss: 101.17265999, tar-aux-loss: 92.92951959
Epoch: [5  ] train-acc: 0.84178571, dom-acc: 0.43535714, val-acc: 0.84000000, val_loss: 0.39536780
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 310.23817420, sen-loss: 42.03518245, dom-loss: 78.48902476, src-aux-loss: 97.26383203, tar-aux-loss: 92.45013440
Epoch: [6  ] train-acc: 0.86214286, dom-acc: 0.44241071, val-acc: 0.85500000, val_loss: 0.35535640
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 302.20629215, sen-loss: 38.91028564, dom-loss: 78.49053127, src-aux-loss: 94.30921489, tar-aux-loss: 90.49626243
Epoch: [7  ] train-acc: 0.86821429, dom-acc: 0.46000000, val-acc: 0.86250000, val_loss: 0.35018989
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 296.52000284, sen-loss: 36.76744318, dom-loss: 78.42615068, src-aux-loss: 91.89199132, tar-aux-loss: 89.43441892
Epoch: [8  ] train-acc: 0.88250000, dom-acc: 0.49250000, val-acc: 0.88500000, val_loss: 0.31671494
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 291.48369479, sen-loss: 34.62235850, dom-loss: 78.33792168, src-aux-loss: 89.04969680, tar-aux-loss: 89.47371787
Epoch: [9  ] train-acc: 0.88714286, dom-acc: 0.54857143, val-acc: 0.88750000, val_loss: 0.30288377
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 286.52074146, sen-loss: 33.01228771, dom-loss: 78.39658386, src-aux-loss: 87.29925045, tar-aux-loss: 87.81261945
Epoch: [10 ] train-acc: 0.89232143, dom-acc: 0.52857143, val-acc: 0.89750000, val_loss: 0.29039392
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 283.63489580, sen-loss: 31.87873220, dom-loss: 78.33144838, src-aux-loss: 85.67281008, tar-aux-loss: 87.75190377
Epoch: [11 ] train-acc: 0.89964286, dom-acc: 0.53544643, val-acc: 0.89000000, val_loss: 0.28603148
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 280.69545150, sen-loss: 30.67337783, dom-loss: 78.20817274, src-aux-loss: 84.25633222, tar-aux-loss: 87.55756819
Epoch: [12 ] train-acc: 0.89214286, dom-acc: 0.54875000, val-acc: 0.87750000, val_loss: 0.30818641
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 278.77626324, sen-loss: 29.84380194, dom-loss: 78.17779374, src-aux-loss: 82.94047457, tar-aux-loss: 87.81419283
Epoch: [13 ] train-acc: 0.90071429, dom-acc: 0.55955357, val-acc: 0.88500000, val_loss: 0.29478621
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 276.81651545, sen-loss: 29.20676335, dom-loss: 78.19374627, src-aux-loss: 81.59693640, tar-aux-loss: 87.81906807
Epoch: [14 ] train-acc: 0.90892857, dom-acc: 0.48910714, val-acc: 0.90000000, val_loss: 0.28191999
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 273.96773100, sen-loss: 28.55986010, dom-loss: 78.14735937, src-aux-loss: 80.35521561, tar-aux-loss: 86.90529728
Epoch: [15 ] train-acc: 0.90839286, dom-acc: 0.58517857, val-acc: 0.89750000, val_loss: 0.28894264
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 271.44967175, sen-loss: 28.09344712, dom-loss: 78.07793200, src-aux-loss: 79.08307821, tar-aux-loss: 86.19521362
Epoch: [16 ] train-acc: 0.91125000, dom-acc: 0.61133929, val-acc: 0.90000000, val_loss: 0.27667072
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 268.16546512, sen-loss: 27.40688363, dom-loss: 78.06186509, src-aux-loss: 77.82614115, tar-aux-loss: 84.87057716
Epoch: [17 ] train-acc: 0.91410714, dom-acc: 0.61000000, val-acc: 0.90500000, val_loss: 0.27492198
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 265.84788918, sen-loss: 26.95923056, dom-loss: 78.06502908, src-aux-loss: 76.79234555, tar-aux-loss: 84.03128397
Epoch: [18 ] train-acc: 0.91500000, dom-acc: 0.61250000, val-acc: 0.91250000, val_loss: 0.27429965
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 265.51918125, sen-loss: 26.50102820, dom-loss: 78.00214356, src-aux-loss: 75.84598371, tar-aux-loss: 85.17002589
Epoch: [19 ] train-acc: 0.91964286, dom-acc: 0.57035714, val-acc: 0.90250000, val_loss: 0.27147129
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 263.10594988, sen-loss: 26.21090008, dom-loss: 77.97493994, src-aux-loss: 74.94156682, tar-aux-loss: 83.97854137
Epoch: [20 ] train-acc: 0.91375000, dom-acc: 0.54125000, val-acc: 0.89500000, val_loss: 0.27582026
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 262.70474625, sen-loss: 25.71494266, dom-loss: 77.89278150, src-aux-loss: 74.05673164, tar-aux-loss: 85.04029113
Epoch: [21 ] train-acc: 0.92035714, dom-acc: 0.64375000, val-acc: 0.90750000, val_loss: 0.27503151
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 261.41657615, sen-loss: 25.15797507, dom-loss: 77.90682954, src-aux-loss: 73.50604707, tar-aux-loss: 84.84572476
Epoch: [22 ] train-acc: 0.92125000, dom-acc: 0.59705357, val-acc: 0.90250000, val_loss: 0.27137122
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 258.44197822, sen-loss: 24.91954431, dom-loss: 77.93532151, src-aux-loss: 72.28429586, tar-aux-loss: 83.30281711
Epoch: [23 ] train-acc: 0.92500000, dom-acc: 0.63723214, val-acc: 0.91500000, val_loss: 0.27211651
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 256.19830394, sen-loss: 24.49151520, dom-loss: 77.89171427, src-aux-loss: 71.27207428, tar-aux-loss: 82.54299957
Epoch: [24 ] train-acc: 0.92428571, dom-acc: 0.62473214, val-acc: 0.91750000, val_loss: 0.27282435
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 256.81912303, sen-loss: 24.07848697, dom-loss: 77.88746834, src-aux-loss: 70.68007356, tar-aux-loss: 84.17309427
Epoch: [25 ] train-acc: 0.92607143, dom-acc: 0.58785714, val-acc: 0.91500000, val_loss: 0.27044192
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 256.05957353, sen-loss: 23.94187035, dom-loss: 77.85318696, src-aux-loss: 69.68304139, tar-aux-loss: 84.58147544
Epoch: [26 ] train-acc: 0.92767857, dom-acc: 0.59535714, val-acc: 0.91500000, val_loss: 0.27372339
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 253.55505848, sen-loss: 23.58943360, dom-loss: 77.83303535, src-aux-loss: 68.86644721, tar-aux-loss: 83.26614308
Epoch: [27 ] train-acc: 0.92910714, dom-acc: 0.61562500, val-acc: 0.91000000, val_loss: 0.26986039
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 253.01150465, sen-loss: 23.27711249, dom-loss: 77.79067999, src-aux-loss: 68.28094277, tar-aux-loss: 83.66277009
Epoch: [28 ] train-acc: 0.93125000, dom-acc: 0.62928571, val-acc: 0.91000000, val_loss: 0.27350855
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 249.72304308, sen-loss: 22.88827663, dom-loss: 77.84516633, src-aux-loss: 67.50430647, tar-aux-loss: 81.48529398
Epoch: [29 ] train-acc: 0.93303571, dom-acc: 0.59223214, val-acc: 0.90750000, val_loss: 0.27118438
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 248.43624580, sen-loss: 22.56181803, dom-loss: 77.77169025, src-aux-loss: 66.58935252, tar-aux-loss: 81.51338530
Epoch: [30 ] train-acc: 0.93500000, dom-acc: 0.54339286, val-acc: 0.90750000, val_loss: 0.27257589
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 248.87573195, sen-loss: 22.20686821, dom-loss: 77.79219723, src-aux-loss: 65.96056774, tar-aux-loss: 82.91609710
Epoch: [31 ] train-acc: 0.93464286, dom-acc: 0.59937500, val-acc: 0.91000000, val_loss: 0.27400392
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 247.20900786, sen-loss: 21.82667207, dom-loss: 77.73724276, src-aux-loss: 65.22755444, tar-aux-loss: 82.41753680
Epoch: [32 ] train-acc: 0.93571429, dom-acc: 0.64142857, val-acc: 0.90750000, val_loss: 0.27253208
---------------------------------------------------

Successfully load model from save path: ./work/models/video_dvd_HATN.ckpt
Best Epoch: [ 27] best val accuracy: 0.00000000 best val loss: 0.26986039
Testing accuracy: 0.87650000
./work/attentions/video_dvd_train_HATN.txt
./work/attentions/video_dvd_test_HATN.txt
loading data...
source domain:  video target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 17009
vocab-size:  83059
['best', 'great', 'funny', 'good', 'enjoyable', 'classic', 'excellent', 'better', 'amazing', 'awesome', 'love', 'favorite', 'nice', 'perfect', 'superb', 'fantastic', 'greatest', 'underrated', 'loved', 'brilliant', 'beautifully', 'hilarious', 'funniest', 'solid', 'terrific', 'easy', 'clever', 'watching', 'epic', 'fine', 'un', 'beautiful', 'decent', 'performed', 'poignant', 'entertaining', 'brilliantly', 'pretty', 'wonderful', 'incredible', 'sad', 'jokes', 'liked', 'believable', 'perfectly', 'scary', 'fabulous', 'riveting', 'recommend', 'watchable', 'masterful', 'old', 'sweet', 'romantic', 'realistic', 'simple', 'funnier']
['worst', 'horrible', 'terrible', 'bad', 'boring', 'poor', 'disappointing', 'awful', 'worse', 'garbled', 'dissapointing', 'bother', 'hard', 'average', 'wrong', 'wasted', 'ruined', 'dull', 'unfunny', 'annoying', 'cuts', 'disappointed', 'hated', 'forgettable', 'unwatchable', 'poorly', 'low', 'dreadful', 'weak', 'lousy', 'needless', 'uninspired', 'atrocious', 'ok', 'save', 'laughable', 'skip', 'contrived', 'uncooked', 'predictable', 'cheesy', 'mediocre', 'lackluster', 'amusing', 'dissapointed', 'cut', 'sappy', 'unmemorable', 'unconvincing', 'dreary', 'disgusting', 'vapid', 'lame']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
83059
5600 400 6000 36180 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.74113131, sen-loss: 77.50185400, dom-loss: 78.81961083, src-aux-loss: 134.83975935, tar-aux-loss: 113.57990628
Epoch: [1  ] train-acc: 0.67928571, dom-acc: 0.75214286, val-acc: 0.68750000, val_loss: 0.65534192
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 374.91718006, sen-loss: 70.49303216, dom-loss: 74.59964937, src-aux-loss: 123.65585333, tar-aux-loss: 106.16864455
Epoch: [2  ] train-acc: 0.74053571, dom-acc: 0.80696429, val-acc: 0.70750000, val_loss: 0.59543532
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 351.19917846, sen-loss: 62.63266802, dom-loss: 72.34382230, src-aux-loss: 114.97425967, tar-aux-loss: 101.24842960
Epoch: [3  ] train-acc: 0.78089286, dom-acc: 0.75696429, val-acc: 0.79250000, val_loss: 0.51958227
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 331.41745448, sen-loss: 53.98516977, dom-loss: 72.22434354, src-aux-loss: 107.45525384, tar-aux-loss: 97.75268912
Epoch: [4  ] train-acc: 0.82089286, dom-acc: 0.65803571, val-acc: 0.80750000, val_loss: 0.44207218
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 319.07482767, sen-loss: 47.42107004, dom-loss: 72.83525664, src-aux-loss: 102.46470052, tar-aux-loss: 96.35380125
Epoch: [5  ] train-acc: 0.84357143, dom-acc: 0.59348214, val-acc: 0.83500000, val_loss: 0.40361115
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 311.99965954, sen-loss: 42.70037061, dom-loss: 74.17258936, src-aux-loss: 98.96057475, tar-aux-loss: 96.16612512
Epoch: [6  ] train-acc: 0.86732143, dom-acc: 0.60866071, val-acc: 0.85250000, val_loss: 0.36662444
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 304.61892772, sen-loss: 38.88673611, dom-loss: 75.10942066, src-aux-loss: 95.84480560, tar-aux-loss: 94.77796566
Epoch: [7  ] train-acc: 0.86982143, dom-acc: 0.57330357, val-acc: 0.86500000, val_loss: 0.35378620
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 299.95512676, sen-loss: 36.37698367, dom-loss: 76.54633629, src-aux-loss: 93.53211683, tar-aux-loss: 93.49968886
Epoch: [8  ] train-acc: 0.88696429, dom-acc: 0.54821429, val-acc: 0.88000000, val_loss: 0.32183838
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 296.38500094, sen-loss: 34.07973161, dom-loss: 77.93630815, src-aux-loss: 91.58084244, tar-aux-loss: 92.78812027
Epoch: [9  ] train-acc: 0.89053571, dom-acc: 0.48598214, val-acc: 0.88500000, val_loss: 0.31213486
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 293.55894065, sen-loss: 32.46944693, dom-loss: 78.91935742, src-aux-loss: 89.95178872, tar-aux-loss: 92.21834630
Epoch: [10 ] train-acc: 0.89428571, dom-acc: 0.42303571, val-acc: 0.88750000, val_loss: 0.29713699
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 290.62927413, sen-loss: 31.37748389, dom-loss: 79.61750603, src-aux-loss: 88.15069890, tar-aux-loss: 91.48358428
Epoch: [11 ] train-acc: 0.89642857, dom-acc: 0.42946429, val-acc: 0.88250000, val_loss: 0.29046860
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 288.29090834, sen-loss: 30.37766067, dom-loss: 79.80664104, src-aux-loss: 86.34320289, tar-aux-loss: 91.76340330
Epoch: [12 ] train-acc: 0.90071429, dom-acc: 0.42428571, val-acc: 0.88000000, val_loss: 0.30737633
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 286.35988379, sen-loss: 29.50503305, dom-loss: 79.71097219, src-aux-loss: 85.35702801, tar-aux-loss: 91.78685075
Epoch: [13 ] train-acc: 0.90428571, dom-acc: 0.43205357, val-acc: 0.88250000, val_loss: 0.30041921
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 283.95277905, sen-loss: 28.91675847, dom-loss: 79.26928252, src-aux-loss: 83.49228597, tar-aux-loss: 92.27445322
Epoch: [14 ] train-acc: 0.91035714, dom-acc: 0.43848214, val-acc: 0.89750000, val_loss: 0.28121322
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 280.41303539, sen-loss: 28.26317102, dom-loss: 78.70206767, src-aux-loss: 82.86397889, tar-aux-loss: 90.58381706
Epoch: [15 ] train-acc: 0.91035714, dom-acc: 0.47714286, val-acc: 0.88750000, val_loss: 0.29334891
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 276.70971990, sen-loss: 27.80895703, dom-loss: 78.33819801, src-aux-loss: 81.22002220, tar-aux-loss: 89.34254110
Epoch: [16 ] train-acc: 0.91357143, dom-acc: 0.49705357, val-acc: 0.90750000, val_loss: 0.27947557
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 275.84154034, sen-loss: 27.27259932, dom-loss: 78.01388013, src-aux-loss: 80.08967954, tar-aux-loss: 90.46538121
Epoch: [17 ] train-acc: 0.91517857, dom-acc: 0.51285714, val-acc: 0.90250000, val_loss: 0.28223938
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 272.62171030, sen-loss: 26.75390105, dom-loss: 77.78260511, src-aux-loss: 78.97198862, tar-aux-loss: 89.11321509
Epoch: [18 ] train-acc: 0.91928571, dom-acc: 0.52285714, val-acc: 0.91500000, val_loss: 0.27383360
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 272.15960217, sen-loss: 26.23939829, dom-loss: 77.48312819, src-aux-loss: 78.23088980, tar-aux-loss: 90.20618683
Epoch: [19 ] train-acc: 0.92160714, dom-acc: 0.51607143, val-acc: 0.91250000, val_loss: 0.27404553
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 269.71012759, sen-loss: 25.94586266, dom-loss: 77.38465512, src-aux-loss: 77.28070617, tar-aux-loss: 89.09890270
Epoch: [20 ] train-acc: 0.92142857, dom-acc: 0.52714286, val-acc: 0.89750000, val_loss: 0.27237058
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 267.51056910, sen-loss: 25.47423172, dom-loss: 77.46625793, src-aux-loss: 76.20820856, tar-aux-loss: 88.36187065
Epoch: [21 ] train-acc: 0.92089286, dom-acc: 0.51107143, val-acc: 0.91250000, val_loss: 0.27750686
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 267.73510480, sen-loss: 24.92711989, dom-loss: 77.42861468, src-aux-loss: 75.55292755, tar-aux-loss: 89.82644176
Epoch: [22 ] train-acc: 0.92660714, dom-acc: 0.48473214, val-acc: 0.89750000, val_loss: 0.27177593
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 265.12053490, sen-loss: 24.69019083, dom-loss: 77.78597778, src-aux-loss: 74.51901218, tar-aux-loss: 88.12535435
Epoch: [23 ] train-acc: 0.92428571, dom-acc: 0.49633929, val-acc: 0.91250000, val_loss: 0.27346632
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 263.15536404, sen-loss: 24.28726699, dom-loss: 77.77509534, src-aux-loss: 73.51182213, tar-aux-loss: 87.58118027
Epoch: [24 ] train-acc: 0.92553571, dom-acc: 0.46857143, val-acc: 0.91500000, val_loss: 0.27375236
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 262.34948063, sen-loss: 23.94175389, dom-loss: 77.84805983, src-aux-loss: 72.98490936, tar-aux-loss: 87.57475764
Epoch: [25 ] train-acc: 0.93017857, dom-acc: 0.45392857, val-acc: 0.91000000, val_loss: 0.26831797
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 263.21266651, sen-loss: 23.69365789, dom-loss: 77.92143440, src-aux-loss: 72.12494695, tar-aux-loss: 89.47262758
Epoch: [26 ] train-acc: 0.93071429, dom-acc: 0.44044643, val-acc: 0.91500000, val_loss: 0.26920569
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 261.64155960, sen-loss: 23.34414285, dom-loss: 77.97983092, src-aux-loss: 71.36285049, tar-aux-loss: 88.95473659
Epoch: [27 ] train-acc: 0.93053571, dom-acc: 0.43151786, val-acc: 0.91000000, val_loss: 0.26990831
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 260.30851948, sen-loss: 23.09154822, dom-loss: 78.18278658, src-aux-loss: 70.73173431, tar-aux-loss: 88.30245191
Epoch: [28 ] train-acc: 0.92875000, dom-acc: 0.43410714, val-acc: 0.90750000, val_loss: 0.27818015
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 257.92000544, sen-loss: 22.72106203, dom-loss: 78.02241635, src-aux-loss: 70.01164690, tar-aux-loss: 87.16487962
Epoch: [29 ] train-acc: 0.93250000, dom-acc: 0.42982143, val-acc: 0.90500000, val_loss: 0.26939338
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 254.65942192, sen-loss: 22.43913336, dom-loss: 77.80851281, src-aux-loss: 69.12408897, tar-aux-loss: 85.28768760
Epoch: [30 ] train-acc: 0.93464286, dom-acc: 0.42455357, val-acc: 0.91250000, val_loss: 0.27223280
---------------------------------------------------

Successfully load model from save path: ./work/models/video_electronics_HATN.ckpt
Best Epoch: [ 25] best val accuracy: 0.00000000 best val loss: 0.26831797
Testing accuracy: 0.84933333
./work/attentions/video_electronics_train_HATN.txt
./work/attentions/video_electronics_test_HATN.txt
loading data...
source domain:  video target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 13856
vocab-size:  78115
['best', 'funny', 'great', 'good', 'classic', 'enjoyable', 'love', 'excellent', 'amazing', 'favorite', 'better', 'nice', 'awesome', 'superb', 'perfect', 'underrated', 'greatest', 'loved', 'brilliant', 'hard', 'beautifully', 'fantastic', 'hilarious', 'funniest', 'terrific', 'easy', 'watching', 'solid', 'entertaining', 'epic', 'beautiful', 'poignant', 'fine', 'un', 'brilliantly', 'clever', 'performed', 'believable', 'wonderful', 'sad', 'jokes', 'liked', 'riveting', 'incredible', 'scary', 'fabulous', 'watchable', 'masterful', 'sweet', 'recommend', 'romantic', 'perfectly', 'heartbreaking', 'simple', 'emotional', 'funnier', 'old']
['worst', 'horrible', 'terrible', 'bad', 'poor', 'boring', 'disappointing', 'worse', 'awful', 'garbled', 'dissapointing', 'bother', 'harmful', 'average', 'wrong', 'wasted', 'dull', 'ruined', 'unfunny', 'annoying', 'cuts', 'hated', 'disappointed', 'forgettable', 'unwatchable', 'dreadful', 'poorly', 'low', 'laughable', 'uninspired', 'weak', 'lousy', 'needless', 'skip', 'unconvincing', 'atrocious', 'ok', 'lame', 'contrived', 'uncooked', 'predictable', 'cheesy', 'mediocre', 'lackluster', 'amusing', 'dissapointed', 'cut', 'sappy', 'unmemorable', 'disgusting', 'vapid', 'save']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
78115
5600 400 6000 36180 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 399.67354417, sen-loss: 77.50730872, dom-loss: 78.66528368, src-aux-loss: 136.30385768, tar-aux-loss: 107.19709325
Epoch: [1  ] train-acc: 0.67803571, dom-acc: 0.74008929, val-acc: 0.68750000, val_loss: 0.65540051
---------------------------------------------------

adapt 0.049958374957880025 lr 0.004655062223111114
Epoch: [2  ] loss: 370.64998865, sen-loss: 70.45638800, dom-loss: 75.01942706, src-aux-loss: 125.15244383, tar-aux-loss: 100.02172995
Epoch: [2  ] train-acc: 0.73750000, dom-acc: 0.82857143, val-acc: 0.70500000, val_loss: 0.59445357
---------------------------------------------------

adapt 0.0996679946249559 lr 0.0043609797474671065
Epoch: [3  ] loss: 345.06876612, sen-loss: 62.40772486, dom-loss: 73.44256186, src-aux-loss: 115.90785390, tar-aux-loss: 93.31062526
Epoch: [3  ] train-acc: 0.78464286, dom-acc: 0.80419643, val-acc: 0.80250000, val_loss: 0.51795030
---------------------------------------------------

adapt 0.1 lr 0.004106884509124773
Epoch: [4  ] loss: 325.67177677, sen-loss: 53.66730586, dom-loss: 73.51246619, src-aux-loss: 107.65944010, tar-aux-loss: 90.83256304
Epoch: [4  ] train-acc: 0.82267857, dom-acc: 0.77267857, val-acc: 0.80000000, val_loss: 0.44074991
---------------------------------------------------

adapt 0.1 lr 0.003884847521204562
Epoch: [5  ] loss: 311.96345592, sen-loss: 46.94966707, dom-loss: 74.18054390, src-aux-loss: 102.07122415, tar-aux-loss: 88.76202071
Epoch: [5  ] train-acc: 0.84589286, dom-acc: 0.76723214, val-acc: 0.84250000, val_loss: 0.39331272
---------------------------------------------------

adapt 0.1 lr 0.0036889397323344054
Epoch: [6  ] loss: 304.59660029, sen-loss: 42.23650482, dom-loss: 74.81360924, src-aux-loss: 98.10491860, tar-aux-loss: 89.44156939
Epoch: [6  ] train-acc: 0.86035714, dom-acc: 0.75357143, val-acc: 0.85750000, val_loss: 0.35838476
---------------------------------------------------

adapt 0.1 lr 0.0035146332824396815
Epoch: [7  ] loss: 295.61389732, sen-loss: 38.76933692, dom-loss: 75.96665663, src-aux-loss: 94.98609430, tar-aux-loss: 85.89180720
Epoch: [7  ] train-acc: 0.86785714, dom-acc: 0.70589286, val-acc: 0.86000000, val_loss: 0.34716275
---------------------------------------------------

adapt 0.1 lr 0.0033584068983394896
Epoch: [8  ] loss: 293.33744812, sen-loss: 36.50224417, dom-loss: 77.01486695, src-aux-loss: 92.78361422, tar-aux-loss: 87.03672320
Epoch: [8  ] train-acc: 0.88392857, dom-acc: 0.65785714, val-acc: 0.87750000, val_loss: 0.32047135
---------------------------------------------------

adapt 0.1 lr 0.003217478292467414
Epoch: [9  ] loss: 286.30289435, sen-loss: 34.32663737, dom-loss: 77.92507404, src-aux-loss: 89.90080047, tar-aux-loss: 84.15038353
Epoch: [9  ] train-acc: 0.88892857, dom-acc: 0.62580357, val-acc: 0.88000000, val_loss: 0.30687296
---------------------------------------------------

adapt 0.1 lr 0.003089618120905312
Epoch: [10 ] loss: 284.54911733, sen-loss: 32.72158523, dom-loss: 78.92299670, src-aux-loss: 88.40960848, tar-aux-loss: 84.49492627
Epoch: [10 ] train-acc: 0.89428571, dom-acc: 0.59821429, val-acc: 0.88500000, val_loss: 0.29481819
---------------------------------------------------

adapt 0.1 lr 0.002973017787506803
Epoch: [11 ] loss: 281.94301891, sen-loss: 31.58266926, dom-loss: 79.17301488, src-aux-loss: 86.59340769, tar-aux-loss: 84.59392589
Epoch: [11 ] train-acc: 0.89964286, dom-acc: 0.57991071, val-acc: 0.88500000, val_loss: 0.28510195
---------------------------------------------------

adapt 0.1 lr 0.0028661936750064665
Epoch: [12 ] loss: 278.93558359, sen-loss: 30.52073116, dom-loss: 79.13760966, src-aux-loss: 84.84476632, tar-aux-loss: 84.43247855
Epoch: [12 ] train-acc: 0.89464286, dom-acc: 0.59294643, val-acc: 0.87750000, val_loss: 0.30855510
---------------------------------------------------

adapt 0.1 lr 0.0027679165582520605
Epoch: [13 ] loss: 276.60277152, sen-loss: 29.72857864, dom-loss: 79.04812551, src-aux-loss: 83.59696382, tar-aux-loss: 84.22910219
Epoch: [13 ] train-acc: 0.90714286, dom-acc: 0.58616071, val-acc: 0.88750000, val_loss: 0.28819060
---------------------------------------------------

adapt 0.1 lr 0.002677158766052148
Epoch: [14 ] loss: 273.73449683, sen-loss: 29.04503284, dom-loss: 78.79893589, src-aux-loss: 82.08483696, tar-aux-loss: 83.80569041
Epoch: [14 ] train-acc: 0.90946429, dom-acc: 0.59339286, val-acc: 0.90000000, val_loss: 0.27916619
---------------------------------------------------

adapt 0.1 lr 0.002593054072035326
Epoch: [15 ] loss: 272.07936835, sen-loss: 28.42818138, dom-loss: 78.14030248, src-aux-loss: 81.37226024, tar-aux-loss: 84.13862538
Epoch: [15 ] train-acc: 0.91160714, dom-acc: 0.62366071, val-acc: 0.89500000, val_loss: 0.28481573
---------------------------------------------------

adapt 0.1 lr 0.002514866859365871
Epoch: [16 ] loss: 268.12600803, sen-loss: 28.00655092, dom-loss: 77.99244601, src-aux-loss: 79.82365897, tar-aux-loss: 82.30335248
Epoch: [16 ] train-acc: 0.91517857, dom-acc: 0.62044643, val-acc: 0.90250000, val_loss: 0.27966607
---------------------------------------------------

adapt 0.1 lr 0.0024419681393728185
Epoch: [17 ] loss: 265.39085233, sen-loss: 27.25566915, dom-loss: 77.74299079, src-aux-loss: 78.69846669, tar-aux-loss: 81.69372278
Epoch: [17 ] train-acc: 0.91607143, dom-acc: 0.63848214, val-acc: 0.90500000, val_loss: 0.27541253
---------------------------------------------------

adapt 0.1 lr 0.0023738167022013005
Epoch: [18 ] loss: 262.31983972, sen-loss: 26.84181633, dom-loss: 77.68407279, src-aux-loss: 77.43060401, tar-aux-loss: 80.36334670
Epoch: [18 ] train-acc: 0.91892857, dom-acc: 0.63196429, val-acc: 0.90750000, val_loss: 0.27182013
---------------------------------------------------

adapt 0.1 lr 0.0023099441564585744
Epoch: [19 ] loss: 262.20304894, sen-loss: 26.42834727, dom-loss: 77.62152606, src-aux-loss: 76.58549827, tar-aux-loss: 81.56767786
Epoch: [19 ] train-acc: 0.92089286, dom-acc: 0.62669643, val-acc: 0.91500000, val_loss: 0.27175349
---------------------------------------------------

adapt 0.1 lr 0.0022499429485385797
Epoch: [20 ] loss: 262.43141985, sen-loss: 26.10068912, dom-loss: 77.56531847, src-aux-loss: 75.76221353, tar-aux-loss: 83.00319636
Epoch: [20 ] train-acc: 0.91857143, dom-acc: 0.61973214, val-acc: 0.89750000, val_loss: 0.27175090
---------------------------------------------------

adapt 0.1 lr 0.002193456688254154
Epoch: [21 ] loss: 260.01881623, sen-loss: 25.71261499, dom-loss: 77.82319516, src-aux-loss: 74.94266072, tar-aux-loss: 81.54034474
Epoch: [21 ] train-acc: 0.92339286, dom-acc: 0.61562500, val-acc: 0.92250000, val_loss: 0.27360967
---------------------------------------------------

adapt 0.1 lr 0.0021401722764675278
Epoch: [22 ] loss: 258.35528374, sen-loss: 25.07987977, dom-loss: 78.02665937, src-aux-loss: 74.41306251, tar-aux-loss: 80.83568299
Epoch: [22 ] train-acc: 0.92321429, dom-acc: 0.58946429, val-acc: 0.90250000, val_loss: 0.26937783
---------------------------------------------------

adapt 0.1 lr 0.0020898134530513185
Epoch: [23 ] loss: 255.15006971, sen-loss: 24.88424224, dom-loss: 78.28148502, src-aux-loss: 72.99504498, tar-aux-loss: 78.98929721
Epoch: [23 ] train-acc: 0.92464286, dom-acc: 0.58883929, val-acc: 0.91500000, val_loss: 0.27437112
---------------------------------------------------

adapt 0.1 lr 0.002042135473504465
Epoch: [24 ] loss: 256.82417476, sen-loss: 24.48993357, dom-loss: 78.48273045, src-aux-loss: 72.11146277, tar-aux-loss: 81.74004784
Epoch: [24 ] train-acc: 0.92642857, dom-acc: 0.55776786, val-acc: 0.92000000, val_loss: 0.27046582
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 254.07014585, sen-loss: 24.09875604, dom-loss: 78.56755781, src-aux-loss: 71.56999838, tar-aux-loss: 79.83383429
Epoch: [25 ] train-acc: 0.92857143, dom-acc: 0.55616071, val-acc: 0.91250000, val_loss: 0.26765642
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 255.87852633, sen-loss: 23.86515531, dom-loss: 78.61348820, src-aux-loss: 70.75277308, tar-aux-loss: 82.64711034
Epoch: [26 ] train-acc: 0.92964286, dom-acc: 0.54294643, val-acc: 0.91750000, val_loss: 0.26688242
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 251.89170885, sen-loss: 23.49754471, dom-loss: 78.71950889, src-aux-loss: 69.76751235, tar-aux-loss: 79.90714276
Epoch: [27 ] train-acc: 0.93071429, dom-acc: 0.52776786, val-acc: 0.90500000, val_loss: 0.26724434
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 251.75521374, sen-loss: 23.28223477, dom-loss: 78.56011307, src-aux-loss: 69.06926891, tar-aux-loss: 80.84359527
Epoch: [28 ] train-acc: 0.92875000, dom-acc: 0.53392857, val-acc: 0.91500000, val_loss: 0.27635798
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 248.41085625, sen-loss: 22.86208969, dom-loss: 78.42834491, src-aux-loss: 68.39084837, tar-aux-loss: 78.72957349
Epoch: [29 ] train-acc: 0.93267857, dom-acc: 0.50848214, val-acc: 0.91500000, val_loss: 0.26783219
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 247.05830061, sen-loss: 22.59281898, dom-loss: 78.09874892, src-aux-loss: 67.47354561, tar-aux-loss: 78.89318579
Epoch: [30 ] train-acc: 0.93500000, dom-acc: 0.48794643, val-acc: 0.91500000, val_loss: 0.27016294
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 247.07079029, sen-loss: 22.25881647, dom-loss: 77.75824755, src-aux-loss: 66.84044829, tar-aux-loss: 80.21327752
Epoch: [31 ] train-acc: 0.93517857, dom-acc: 0.56125000, val-acc: 0.91250000, val_loss: 0.27015641
---------------------------------------------------

Successfully load model from save path: ./work/models/video_kitchen_HATN.ckpt
Best Epoch: [ 26] best val accuracy: 0.00000000 best val loss: 0.26688242
Testing accuracy: 0.86050000
./work/attentions/video_kitchen_train_HATN.txt
./work/attentions/video_kitchen_test_HATN.txt
